\subsection{Noise}

% (TODO): Write about main motivation for doing this: what if we have a model
% e.g. in software that we are now implementing physically: how does the model
% then respond to the noise?

The results are shown in Fig. \ref{input_noise_snr}, illustrating a slight
performance degradation when the ratio of signal power to noise power drops
below 20 dB. The reservoir performance drops more drastically when reaching 10
dB. Similar performance degradation was seen in \cite{dambre_information_2012},
where the same SNR measure was used to evaluate the signal reconstruction
capacity from the state of a dynamical system.

\begin{figure}
  \centering
  \includegraphics[width=2.5in]{img/input_noise_snr.png}
  \caption{
    Noise causing a decrease in performance for an ESN reservoir on the NARMA10
task ($N = 200$ nodes). When the signal to noise ratio drops below 20dB, the
performance of the reservoir degrades in an exponential manner.
  }
  \label{input_noise_snr}
\end{figure}

A comparison can be drawn to commonly recommended SNRs in the IEEE 802.11
standard for Wi-Fi communications, ranging from 15db to 25dB. Below this
threshold, wireless communication will quickly become unusable.

In summary, our results indicate a robustness to the presence of noise in the
input stream. This is a promising result when considering physical substrates as
reservoir mediums, providing lower bounds on the ratio of signal power to noise
power that is necessary.

\subsection{Measurement equipment accuracy}

In Fig. \ref{adc_quantization} we see a performance degradation from 12 to 10
quantization bits. As discrete output states move beneath a quantization of 10
bits, the performance quickly deteriorates. However, even with just 4 bits to
discretely separate outputs, reservoirs are still able to replicate the input
sequence to some degree. Furthermore, reservoirs with 400 hidden nodes and just
64 output bits consistently provide the same performance as that of a reservoir
with 50 hidden nodes and no output quantization.

\begin{figure}[H]
  \centering
  \includegraphics[width=2.5in]{img/adc_quantization.png}
  \caption{
    Performance effect of ADC quantization on four reservoirs of different
sizes. $\tanh$ is used as activation function for the experiment, dividing its
range of (1-, 1) into $n$ discrete output bins.
  }
  \label{adc_quantization}
\end{figure}

A 12-bit ADC has $2^{12}$, or 4096 output codes, which in accordance with our
experiments would impose little increase in the prediction error. 16-bit ADCs,
outputting $2^{16}$, or 65536 discrete states, would influence our ESN
simulations even less.

Previously, a major factor limiting the performance of a nonlinear analog
electric circuit implementation has been found to be quantization noise
\cite{soriano_delay-based_2015}. Low error rates on the Mackey-Glass task were
achieved with 8 bits or more in the ADC. This result differs from the
performance measures seen in our ESN implementation, where NRMSE starts leveling
out around 10 bits of accuracy.

We attribute this discrepancy to the nature of the $\tanh$ activation. In our
experiment, $\tanh$ is divided linearly into discrete bins, although we may not
use the complete range. As the NARMA10 task relies heavily upon memory, and less
so upon nonlinearity, the activation of a high number of nodes will lie in the
highly linear part of $\tanh$ in the interval [-0.4, 0.4], with a few nodes
outside this range. Essentially, this may in the worst cases account for halving
the available output bins

Our results demonstrate that the reservoir methodology provides an inherent
resilience to output quantization, specifically so when using ADCs with 12-bits
of resolution or higher. When designing physical RC systems, this may be applied
in preliminary analysis of the resolution and representation of output
activations.

\subsection{Partially observable reservoir state}

\begin{figure*}[htbp]
  \centering
  \begin{subfigure}{.3\textwidth}
    \centering
    \includegraphics[width=\linewidth]{img/input_density_all.png}
    \caption{}
  \end{subfigure}
  \begin{subfigure}{.3\textwidth}
    \centering
    \includegraphics[width=\linewidth]{img/output_density_all.png}
    \caption{}
  \end{subfigure}
  \begin{subfigure}{.3\textwidth}
    \centering
    \includegraphics[width=\linewidth]{img/partial_visibility.png}
    \caption{}
  \end{subfigure}
  \caption{
    These figures show the influence of reservoirs that are only partially
visible on the performance for the NARMA10 task. The density is a measurement
for the fraction of elements in the input and output matrices containing
non-zero elements. The heat map was generated using a reservoir size of 200
hidden nodes.
  }
  \label{partial_visibility}
\end{figure*}

When attaching input units to the network by weights, experience has shown that
choosing $\mathbf{W}_{in}$ can be done freely, as long as
$\rho(\mathbf{W}_{res}) < 1$ is satisfied \cite{jaeger_echo_2001}. Experimental
results from adjusting the sparsity of $\mathbf{W}_{in}$ is shown in
Fig. \ref{partial_visibility}a, in which all experiments were done with a
uniform input weight distribution in the interval [-0.5, 0.5].

Across all three reservoir sizes, we see a small increase in the signal
prediction error when increasing input connectivity from an initial density of
0.1. While finding the optimal reservoir perturbance clearly depends on the
memory capacity and computational dynamics required for the task at hand, this
benchmark task gives a clear indication of the influence of input sparsity in
general. The relatively low sensitivity to this parameter suggests that physical
reservoirs may achieve excellent performance even when only perturbing selected
parts of the system. An input stream parameter found to be of greater relevance
is input scaling, i.e. the amplitude of the input signal that the reservoir sees
\cite{alippi_quantification_2009}. The necessary scaling depends on the
nonlinearity needed, as inputs far from 0 drive $\tanh$ neurons towards
saturation. In a physical reservoir design setting, this parameter is often
possible to adjust after-the-fact, as opposed to changing the physical hardware
layout of the system.

Next, a general trend seen when adjusting output connectivity, is a slight,
linear decrease in performance from a completely dense output matrix to a
density of 0.4 (Fig. \ref{partial_visibility}b). Furthermore, for the 50 and 100
node reservoirs, there is a noticeable transition from a linear to an
exponential NRMSE growth.

We may understand these results to indicate that there is a critical point in
the output density space where there the readout layer starts seeing enough of
the reservoir dynamics to reconstruct the input NARMA10 signal. Additionally,
when the output density exceeds this threshold, the performance will continue to
grow linearly with the density, achieving the best performance at full
connectivity.

Another interesting result illustrated in Fig. \ref{partial_visibility}b is that
increasing the reservoir size without increasing the amount of output nodes has
little effect on the network performance. Changing the x-axis of the previous
plot the actual amount of connected nodes instead of the output density reveals
this conclusively, as seen in Fig. \ref{output_nodes}.

\begin{figure}[H]
  \centering
  \includegraphics[width=2.5in]{img/output_nodes.png}
  \caption{
    Output nodes.
  }
  \label{output_nodes}
\end{figure}

Our results suggest that in the case of output connectivity, the performance of
reservoir systems only correlates with the absolute amount of nodes seen, and
not with the internal reservoir size. Hence, physical reservoir implementations
must carefully consider this property, making it imperative that a partial
visibility of the reservoir dynamics is sufficient for the task of which it is
designed. In essence, the gathered information that is present in clusters of
neurons on microelectrode grids may be completely lost if the layout is too
sparse.

% (TODO): Maybe don't use lines here? Scatter?

% (TODO): Also plot output connectivity as amount of output nodes. Perhaps we see
% that the size of the underlying reservoir does not matter at all?

% (TODO): Discuss how this relates to the maximum information capacity of
% individual nodes, and include Shannon information and how complexity is defined
% as the amount of information required to describe something.

\begin{figure}[H]
  \centering
  \includegraphics[width=2.5in]{img/input_scaling_distrib.png}
  \caption{
    Effect of input scaling on three input weight distributions. With the fixed
distribution every input weight is set to 1. The best performance of all three
distributions lie around NRMSE $\approx 0.25$.
  }
  \label{input_scaling_distrib}
\end{figure}

Fig. \ref{input_scaling_distrib} explores the performance of ESNs with three
different input weight distributions: uniform, Gaussian and fixed. Despite the
performance disparity with increasing input scaling, the best performance of all
three classes hover around the same NRMSE of around 0.25. This demonstration of
scaling differently distributed inputs such that they all achieve acceptable
performance bodes well for physical RC. In particular, we observe a promising
result when considering RC with physical substrates in which every node is
forced to receive the same input. Exploring different $\mathbf{W}_{in}$
distributions in ESNs may give an intuition for its importance as a reservoir
parameter, which may provide valuable in reservoir design, as one may thus
resort to the cheapest and simplest option.

% (TODO): Rewrite last sentence above.

% (TODO): Ensure that the subfigure figure isn't pushed _into_ the references.

\subsection{Topology}

Topology section. Probably a literature review, leading into discussions about
future work.

\begin{figure}[H]
  \centering
  \includegraphics[width=2.5in]{img/reservoir_density_distrib.png}
  \caption{
    Reservoir density versus for two weight distributions. Fixed weight will not
work for internal nodes, unless special topology is employed, e.g. ring
(elaborate). This shows that sparse internal weight matrices work fine.
  }
  \label{reservoir_density_distrib}
\end{figure}

Something about internal reservoir density as an introductory exploration. EA/GA
for looking at morphology that is equivalent to random connectivity, but easier
to generate, and thus relevant for physical substrates?

% (TODO): Reason for reservoir sparsity is in the original Jaeger paper,
% e.g. multiple creating separate networks with rich dynamics.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
