\subsection{Noise}

Physical, real world systems are affected by noise. By extension, designers of
reservoirs that use material substrates must be aware of the effects the noise
that is present may have on computational power.

It is well known in the field of traditional artificial neural networks that an
addition of noise to training data can lead to generalization improvements
similar to that of Tikhonov Regularization \cite{bishop_training_1995}. This has
been verified to hold for the RC paradigm \cite{jaeger_echo_2001,
kurkova_stable_2008}, where an additional noise term $\mathbf{v}(t)$ is added to
the reservoir. The noise is either added to all reservoir nodes, or to an output
\textit{feedback} of $\mathbf{y}(t)$ back into the reservoir nodes. A more
pragmatic approach is thus simply using ridge regression, as to avoid the
nondeterminism present with dynamic noise injection.

In the noise section we will investigate the impact of adding noise to just the
input signal, and hence exploring reservoir robustness to noisy environments.

Additive white Gaussian noise (AWGN) is a common noise model that mimics the
noise patterns of many random processes in nature. The noise is additive,
meaning the AWGN output is the sum of the input $u_{i}$ and the noise values
$v_{i}$. $v_{i}$ is i.i.d and drawn from a Gaussian distribution with zero-mean,
and a variance $\sigma^{2}$.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
