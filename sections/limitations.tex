\subsection{Noise}

Physical, real world systems are affected by noise. By extension, designers of
reservoirs that use material substrates must be aware of the effects the noise
that is present may have on computational power.

It is well known in the field of traditional artificial neural networks that an
addition of noise to training data can lead to generalization improvements
similar to that of Tikhonov Regularization \cite{bishop_training_1995}. This has
been verified to hold for the RC paradigm \cite{jaeger_echo_2001,
kurkova_stable_2008}, where an additional noise term $\mathbf{v}(t)$ is added to
the reservoir. The noise is either added to all reservoir nodes, or to an output
\textit{feedback} $\mathbf{y}(t)$ that is fed back into the reservoir
nodes. This work cites simply using ridge regression to be a more pragmatic
approach, as to avoid the nondeterminism present with dynamic noise injection,
but has additionally identified the presence of noise within reservoirs to be
advantageous.

Whether this internal noise resilience of ESNs translates to a more general
robustness to noisy inputs is unclear. A common approach in research is to first
design a model of the system, and it is crucial to know how how the model will
translate into a physical medium. Previous work has shown that combining the
traditional ESN with a state machine framework, using Viterbi training with
multiple readout filters in a manner similar to hidden Markov models, leads to
noise-robust classification \cite{skowronski_noise-robust_2007}. However, our
motivation is an investigation of the effect of moving an existing model to a
physical substrate that may exhibit unknown noise patterns. Hence, we use the
traditional ESN approach, focusing on the impact of adding noise to just the
input signal of the test set, without changing the internal reservoir dynamics
or readout methodology.

This has previously been investigated in LSMs, using reservoirs containing 1232
leaky integrate integrate-and-fire neurons
\cite{verstraeten_isolated_2005}. Here, adding three types of noise from the
NOISEX database: speech babble, white-noise, and car interior noise, to speech
recognition tasks saw the error rate consistently staying above 80\% with an SNR
of 10dB. We further this work using the dynamics of ESNs, with the motivation of
exploring the their general robustness to noisy inputs. This provides insight
into whether we can expect a similar behavior regardless of the reservoir medium
that is used to implement rich, complex dynamics.

Modeling random processes in nature is commonly done with the Additive white
Gaussian noise (AWGN) model. The noise is additive, meaning the AWGN output is
the sum of the input $\mathbf{u}(t)$ and the noise values
$\mathbf{v}(t)$. $\mathbf{v}(t)$ is i.i.d and drawn from a Gaussian distribution
with zero-mean, and a variance $\sigma^{2}$.

\subsection{Measurement equipment accuracy}

When conducting experiments using physical reservoirs, one will inevitably have
to interact with substrates with measurement instruments. Whether it be
transforming digital representations of reservoir perturbations to analog
signals that cause the excitation, or the reverse mapping of the analog state of
the reservoir into a digital representation, the accuracy of equipment used for
such conversions is of crucial importance.

Equipment sensor anomalies, noise, and amplification gain may all impact
performance. In this section we conduct a case study for a typical conversion of
physical information present in a system into a digital representation with an
ADC.

Common ADC errors include gain error, which causes the slope of the transfer
function to deviate from the ideal slope, and offset error, in which the slope
of the transfer function is offset by some constant amount. In both of these
cases, the use of regression methods will cause no performance penalty as a
result of the error.

A more general problem present in ADCs, which is also manifested similarly in
virtually all measurement methods, is the resolution of the output signal that
is produced. ADCs generally have an output quantization range from 6 bits to 24
bits, and this quantization is of interest when considering physical reservoir
systems. A resolution that is too low may cause two different internal states in
the reservoir to be interpreted equally. Naturally, when the amount of discrete
output bins becomes too scarce for the readout to interpret the high dimensional
dynamics of the system, the performance will suffer.

\subsection{Partially observable reservoir state}

Consider a physical reservoir system using microelectrode arrays (MEAs) as its
computational substrate, a common approach when using biological, \textit{in
vitro} components \cite{aaser_towards_2017}. The goal of such MEAs is to serve
as an interface that connects biological neuronal activity to electronic
circuitry, and it does so by having an organization of microelectrodes on a
two-dimensional grid. Obtaining neural signals is done only through the
electrode interface by means of a two-way transduction from voltage drop in the
biological environment to a an electric current and vice versa.

When seeding MEAs with solutions containing neuronal cultures, one is by no
means guaranteed a neural network that fits the MEA layout. In fact, with common
grid layouts ranging from 64 to 256 electrodes, each electrode will examine its
surrounding area, not individual cells. Thus, in this section we intend to
provide an insight into the performance effect of having reservoirs that are
only partially observable.

In this section we experiment with the sparsity of $\mathbf{W}_{in}$ and
$\mathbf{W}_{out}$. In both cases, we now generate the connection matrices such
that a wanted density, given as the fraction of connected nodes, is
achieved. Input and output is adjusted separately. Fig. \ref{partial_visibility}
shows the results of our simulation runs. Additionally, we investigate the
effect of fixing the input weight distribution completely, hence only allowing a
constant scaling of the input stream.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
