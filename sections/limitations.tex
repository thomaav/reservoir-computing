\subsection{Noise}

Physical, real world systems are affected by noise. By extension, designers of
reservoirs that use material substrates must be aware of the effects the noise
that is present may have on computational power.

It is well known in the field of traditional artificial neural networks that an
addition of noise to training data can lead to generalization improvements
similar to that of Tikhonov Regularization \cite{bishop_training_1995}. This has
been verified to hold for the RC paradigm \cite{jaeger_echo_2001,
kurkova_stable_2008}, where an additional noise term $\mathbf{v}(t)$ is added to
the reservoir. The noise is either added to all reservoir nodes, or to an output
\textit{feedback} of $\mathbf{y}(t)$ back into the reservoir nodes. A more
pragmatic approach is thus simply using ridge regression, as to avoid the
nondeterminism present with dynamic noise injection.

Previous work has shown that combining the traditional ESN with a state machine
framework, using Viterbi training with multiple readout filters in a manner
similar to hidden Markov models, leads to noise-robust classification
\cite{skowronski_noise-robust_2007}. However, this does not necessarily tell us
anything about the noise-robustness we may expect from realizing physical
reservoirs. As a common approach is to first design a model of the system, it is
therefore crucial to know how how the model will translate into a physical
medium, particularly when considering noise. Thus, we will in this section
investigate the impact of adding noise to the input signal without changing the
internal reservoir dynamics.

This has previously been investigated in LSMs, using reservoirs containing 1232
leaky integrate integrate-and-fire neurons
\cite{\cite{verstraeten_isolated_2005}}. Here, adding three types of noise from
the NOISEX database to word recognition tasks: speech babble, white-noise, and
car interior noise, saw the error rate consistently staying above 80\% with an
SNR of 10dB. We further this investigation using the dynamics of ESNs with the
motivation of exploring the general noise robustness of any reservoir system.

% (TODO): The last sentence is complete garbage.

Additive white Gaussian noise (AWGN) is a common noise model that mimics the
noise patterns of many random processes in nature. The noise is additive,
meaning the AWGN output is the sum of the input $u_{i}$ and the noise values
$v_{i}$. $v_{i}$ is i.i.d and drawn from a Gaussian distribution with zero-mean,
and a variance $\sigma^{2}$.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
