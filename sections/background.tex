\subsection{Echo State Networks}

Jaeger proposed the \textit{Echo State Network} \cite{jaeger_echo_2001} as an
alternative approach to RNNs, in which only the weights of the output units are
modified. In RNNs, the activation state $\mathbf{x}(t)$ is determined as a
function of the sequence of inputs that is fed to the network $\mathbf{u}(t),
\mathbf{u}(t-1), \ldots$ making its activation an ``echo'' of the input history.

% (TODO): only in some cases does it have the "echo" state property^.

\begin{figure}[H]
  \centering
  \includegraphics[width=2.5in]{img/esn.png}
  \caption{
    Basic architecture of ESN reservoir systems. The reservoir acts as a
high-dimensional kernel, transforming the temporal input sequence into a spatial
representation. The readout is trained with supervision, providing a least
squares optimum.
  }
  \label{esn}
\end{figure}

In this paper, we consider discrete-time RNNs with $N$ internal network nodes, a
single input, and a single output node. Fig. \ref{esn} illustrates the basic
architecture of such ESN reservoirs. The reservoir state is updated according to

\begin{equation}
  \mathbf{x}(t + 1) =
    \tanh(\mathbf{W}^{res}\mathbf{x}(t)
        + \mathbf{W}^{in}\mathbf{u}(t)),
  \label{xt}
\end{equation}

\noindent using $\tanh$ as the nonlinear transfer function for internal
reservoir nodes. The output of the reservoir is given by

\begin{equation}
  \mathbf{y}(t + 1) =
    \mathbf{W}^{out}\mathbf{x}(t).
  \label{yt}
\end{equation}

To train an ESN model of size N in a supervised and offline mode, it is run to
completion on a training set. The reservoir states are represented by an Nth
order column vector $\mathbf{X}$, and the one-dimensional output by a first
order column vector $\mathbf{Y}$. The linear readout layer is then trained to
minimize the squared output error $E = \norm{\mathbf{Y} - \mathbf{\hat{Y}}}$
where $\mathbf{\hat{Y}}$ is the target output, which amounts to solving a system
of linear equations

\begin{equation}
  \mathbf{\hat{Y}} = \mathbf{W}^{out}\mathbf{X}.
  \label{hatreg}
\end{equation}

Well-known methods of solving (\ref{hatreg}) include ridge regression, often
called Tikhonov regularization, and the Moore-Penrose pseudo-inverse.

When the network is adapted to $\mathbf{W}^{out}$, the ESN is fully trained,
thus illustrating the apparent simplicity and low algorithmic complexity of the
method. Gauging the performance of a trained network is done by running a test
set. We use the normalized root mean square error (NRMSE) for this evaluation,
given a predicted signal $\mathbf{y}(t)$ and a target signal
$\mathbf{\hat{y}}(t)$

\begin{equation}
  E(\mathbf{y}, \mathbf{\hat{y}}) = \sqrt{\frac{
      \mean{\norm{\mathbf{y}(t) - \mathbf{\hat{y}}(t)}^{2}}
    }{
      \mean{\norm{\mathbf{\hat{y}}(t) - \mean{\mathbf{\hat{y}}(t)}}^{2}}
    }
  }
  .
  \label{nrmse}
\end{equation}

% I use NRMSE from Reservoir Computing Approaches to Recurrent Neural Network
% Training by Jaeger.

\subsection{Applications}

Applications of RC lie primarily in the domain of time series prediction and
classification. The paradigm has been successfully applied to predict myriads of
time series, where benchmark tasks range from from the simple H\'enon Map
\cite{goudarzi_comparative_2014}, to time series requiring more computation and
memory, such as the mildly chaotic Mackey-Glass time series
\cite{alippi_quantification_2009}, and the Santa Fe chaotic timeseries
\cite{rodan_minimum_2011}. Real world approaches include equalizing a wireless
communication channel \cite{jaeger_harnessing_2004}, and short-term traffic,
electric load, and stock price forecasting \cite{an_short-term_2011,
song_hourly_2011, lin_short-term_2009}.

Use of reservoir systems for pattern classification tends to involve sequential
input such as speech \cite{verstraeten_reservoir-based_2006}, but has also seen
static pattern recognition tasks like detecting invasive plant species
\cite{wootton_optimizing_2017}. More exotic applications of RC include robot
control \cite{aislan_antonelo_learning_2015}, reinforcement learning
\cite{bush_modeling_2005}, and learning grammatical structure
\cite{tong_learning_2007}.

\subsection{Reservoir generation}

As with virtually every machine learning technique, the application of ESNs
requires some experience. Although a conceptually simple idea, generating
adequate reservoir networks is influenced by multiple global
parameters. Recommendations to achieve sufficient performance are presented in
\cite{montavon_practical_2012, jaeger_tutorial_nodate}, suggesting parameters
such as the scaling of the input weight matrix, the spectral radius of the
reservoir connection matrix, the leaking rate of reservoir nodes, and the model
size parameters to be of high importance. However, in practice the evaluation of
a reservoir is an endeavor often conducted by training the output and measuring
the error, sometimes requiring extensive parameter sweeps.

% (TODO): Write about how this relates to physical reservoirs.

\subsection{Assessing the quality of a reservoir}

In the RC methodology, computation and memory retainment are intertwined, in
stark contrast to traditional architectures which use separate memory storage
units. Attributing task-solving performance to either memory capacity or
computation is therefore quite difficult. A major challenge is finding suitable
performance metrics that are substrate-independent, especially given the wide
variety of architectures of which the parameter spaces may differ completely.

Attempts have been made to unify quality metrics for reservoir substrates,
e.g. the ability to separate different inputs \cite{legenstein_edge_2007}, the
ability to generalize \cite{legenstein_edge_2007}, and linear short-term memory
\cite{jaeger_short_2002}. However, as we use extensively researched ESNs, we
have chosen the NARMA10 time series to be the main evaluation criteria for
network performance. Its wide use enables comparisons to previous work, which
makes it a good candidate for evaluating memory capacity and computational power
with a single metric.

\subsection{NARMA}

Nonlinear autoregressive moving average (NARMA) \cite{atiya_new_2000} is a class
of time series models widely used to benchmark the performance of RC models. We
use the NARMA10 task to evaluate the emulation performance of an ESN, which is a
temporal task with a time-lag of ten time steps, given by

% (TODO): Cite any NARMA users? Many listed in Kubota 2019^.

\begin{equation}
  y_{t+1} = \alpha y_{t} +
  \beta y_{t} \sum_{i=0}^{9}y_{t-i} +
  \gamma u_{t}u_{t-9} +
  \delta,
  \label{narma}
\end{equation}

\noindent with the default constant parameters $\alpha = 0.3$, $\beta = 0.05$,
$\gamma = 1.5$ and $\delta = 0.1$. The input $u_{t}$ is an i.i.d. stream drawn
from the interval [0, 0.5]. The task presents a challenge of both memory and
nonlinearity, uncovered to be a universal trade-off in in dynamical systems used
in reservoir settings \cite{dambre_information_2012, verstraeten_memory_2010},
therefore making it a well-suited task. The dynamical anatomy of the NARMA10
task was further investigated in \cite{kubota_dynamical_2019}.

% (TODO): Introduce the problem statements individually for each
% limitation. ''Natural computational systems must have specific topologies, and
% the uniform random connectivity is not appropriate.'' Topology discussion,
% Manevitz et. al.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
