In this paper, we consider discrete-time ESNs with $N$ internal network nodes, a
single input, and a single output node.

The default reservoir size used is 200 hidden nodes. $\mathbf{W}^{res}$ and
$\mathbf{W}^{in}$ are both generated as random matrices with i.i.d. entries in
the interval [-0.5, 0.5]. Both matrices are fully connected, and the reservoir
weight matrix was rescaled such that $\rho(\mathbf{W}^{res}) = 0.9$. This method
for instantiating $\mathbf{W}^{res}$ and $\mathbf{W}^{in}$ is common practice in
RC \cite{montavon_practical_2012}. Input scaling is set to $\iota = 1.0$, as the
parameter was found to have little impact on performance when initializing
$\mathbf{W}^{in}$ from a uniform distribution. $\mathbf{W}^{out}$ is adapted
with the Moore-Penrose pseudo-inverse using singular value decomposition, as
this was found to lead to the best results.

The first 200 states of each run are discarded to provide a washout of the
initial reservoir state. For all experiments, the generated input was split into
a training and test set, with $L_{train} = 2000$ and $L_{test} = 3000$. Reported
performances are the mean across ten randomizations of each model
representative. A few experiments were run with more randomizations to achieve
statistical significance, which is reported in each such case.

All reservoirs were constructed with the parameters from this baseline,
differing only in the parameters given for each specific experiment. The Python
software library implementation is available
online\footnote{https://github.com/thomaav/esn-physical-limits}.

\subsection{Noise}

Modeling random processes in nature is commonly done with the Additive white
Gaussian noise (AWGN) model. The noise is additive, meaning the AWGN output is
the sum of the input $\mathbf{u}(t)$ and the noise values
$\mathbf{v}(t)$. $\mathbf{v}(t)$ is i.i.d and drawn from a Gaussian distribution
with zero-mean, and a variance $\sigma^{2}$.

We model AWGN by extending the ESN model to take the sum of two individual
inputs, $\mathbf{u}(t)$ and $\mathbf{v}(t)$, which represent the signal and the
noise. The goal of the reservoir remains a computation on the signal
$\mathbf{u}(t)$, a task now hindered by the unwanted noise.

We vary the signal to noise ratio of the injected noise when running the test
dataset. The signal to noise ratio is measured in dB, and is calculated as $SNR
= 10\log_{10}(\frac{var(u)}{var(v)})$.

\subsection{Measurement equipment accuracy}

To emulate the behavior of an ADC, we extend our ESN model to allow for
quantization of reservoir output before it is passed to the readout layer. This
quantization effectively divides the range of the nonlinear activation function,
i.e. $\tanh$ with a range (-1, 1), of each hidden node into a discrete set of
fixed output bins.

We run experiments with four different reservoir sizes: 50, 100, 200 and 400
hidden nodes, to see whether it is possible to compensate for lower resolutions
by increasing the size of the reservoir. For this experiment each model
representative is run 20 times.

\subsection{Partially visible reservoir state}

We begin by experimenting with the sparsity of $\mathbf{W}^{in}$ and
$\mathbf{W}^{out}$. In both cases, we now generate the connection matrices such
that a wanted density, given as the fraction of connected nodes, is
achieved. Input and output is adjusted separately. For the input density
experiment each model representative is run 50 times to determine that there is
a definitive trend.

Additionally, we examine three different input weight distributions: uniform,
Gaussian and fixed. All inputs are sampled as i.i.d streams with distributions
commonly used in the literature. The uniform distribution is sampled in the
interval [-0.5, 0.5], the Gaussian distribution is sampled with a zero mean and
standard deviation $\sigma = 1.0$, and the fixed distribution has every input
weight set to 1. Moreover, we explore the parameter space of the input scaling
in the interval [0.1, 1.0].

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
