Training Recurrent Neural Networks is an inherently difficult task. To combat
the high algorithmic complexity of previous training methods, Liquid State
Machines (LSMs) \cite{maass_real-time_2002}, Echo State Networks (ESNs)
\cite{jaeger_echo_2001} and Backpropagation Decorrelation (BPDC)
\cite{steil_backpropagation-decorrelation:_2004} independently present
alternative supervised learning methods that don't adapt the internal weights of
the network. Instead, the output is generated using a simple, memoryless
classifier or regressor, making the function of the RNN resemble that of a
kernel in kernel method algorithms, which seek features and general relations in
datasets to increase separability. Thus, by projecting into a high-dimensional
space, the temporal information of time series input may be incorporated in the
instantaneous readout. This methodology has been unified into the research
subfield of Reservoir Computing (RC) \cite{schrauwen_overview_2007}, of which
the focus is on separating the randomly generated reservoir from the trained
readout layer.

Interestingly, there is no need for the reservoir to be an artificial neural
network -- any high-dimensional, driven system exhibiting complex dynamic
behavior can be used \cite{schrauwen_overview_2007}. Reservoirs are thus
designed as to harness the dynamics of the \textit{substrate} that implements
it. A multitude of substrates have shown promise as reservoirs: dynamical
systems models such as Cellular Automata \cite{nichele_deep_2017} and the more
general Random Boolean Network \cite{snyder_computational_2013}, electronic
reservoirs using memristor circuits \cite{kulkarni_memristor-based_2012},
photonic reservoirs \cite{vandoorne_experimental_2014}, and more biologically
oriented reservoirs such as gene regulation networks \cite{jones_is_2007} or the
cat primary visual cortex \cite{scholkopf_temporal_2007}. Consult
\cite{tanaka_recent_2018} for an overview of recent advances in physical RC.

As implementations of reservoir systems increasingly tend toward physical
substrates, the computational performance may be affected by intrinsic physical
limitations. In this paper we seek to investigate several fundamental properties
related to physical reservoirs: how does noise affect performance? How does the
accuracy of our measurement equipment affect reservoir quality? What if we are
only able to partly observe the reservoir? Does there exist network topologies
that provide more capable reservoirs? We find that ... (TODO): What did we find?

% (TODO): make sure to only specify things that are actually investigated^.

% (TODO): Physical substrates present difficulties, no longer possible to just
% tune the spectral radius (keys from practical guide: spectral radius, input
% scaling, leakiness).

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End: