Training Recurrent Neural Networks (RNNs) is an inherently difficult task
\cite{bengio_learning_1994}. To combat the high algorithmic complexity of
previous training methods, Echo State Networks (ESNs) \cite{jaeger_echo_2001}
present an alternative supervised learning technique that does not adapt the
internal weights of the network. Instead, the output is generated using a
simple, memoryless classifier or regressor, making the function of the internal
RNN resemble that of kernel methods. Thus, by projecting the input sequence into
a high-dimensional space, the temporal information of a time series may be
incorporated in the instantaneous readout. This methodology, concerned with
exploiting the underlying dynamics of a \textit{reservoir}, is unified in the
research subfield of Reservoir Computing (RC).

Interestingly, there is no need for the reservoir to be an artificial neural
network -- any high-dimensional, driven system exhibiting complex dynamic
behavior can be used \cite{schrauwen_overview_2007}. Reservoirs are thus
designed such that we are able to harness the dynamics that governs the
\textit{substrate} that implements it. A multitude of substrates have shown
promise as reservoirs: electronic memristor circuits
\cite{kulkarni_memristor-based_2012}, photonic systems
\cite{vandoorne_experimental_2014}, mechanical springs
\cite{hauser_towards_2011} and more biologically oriented reservoirs such as
gene regulation networks \cite{jones_is_2007} and the cat primary visual cortex
\cite{scholkopf_temporal_2007}. Consult \cite{tanaka_recent_2018} for an
overview of recent advances in physical RC.

Commonly, preliminary studies are conducted by simulating the proposed dynamical
system numerically to gauge its applicability in a physical reservoir
setting. Such models may not entirely encapsulate the uncertainties and
limitations that will exist in its corresponding physical setting, thus leading
to a divergence between the performances observed
\cite{vandoorne_experimental_2014, katumba_neuromorphic_2018,
jensen_reservoir_2017}. The extent of performance degradation caused by physical
limitations is not readily understood, and presents a knowledge gap in the field
of physical RC.

Hence, as implementations of reservoir systems increasingly tend toward physical
substrates, the computational performance may be affected by intrinsic physical
limitations. In this paper we seek to investigate fundamental properties related
to physical reservoirs.

Reservoir robustness to noise is a primary concern that has been demonstrated
both numerically and experimentally in an optoeletronic setting, where
pre-processing techniques have been shown to reduce performance degradations
\cite{soriano_optoelectronic_2013}. It is well-established that ESNs are
resilient to internal noise \cite{jaeger_echo_2001}, but it is uncertain whether
this translates to intrinsically noise inputs. ESNs provide a natural context
for studying the general significance of white noise.

Another relevant noise characteristic is that of equipment
accuracy. Quantization noise, i.e. the resolution of the interface equipment, is
usually determined by DAC and ADC instruments. Employing equipment with
sufficient resolution has been found to be important when using physical
reservoirs \cite{soriano_delay-based_2015}. The quantized ESN model, qESN,
exhibits similar dynamics to that of random Boolean networks
\cite{busing_connectivity_2010}.

Physical substrates will also differ in their process for both input
perturbation and state observation. The impact of input and output density,
i.e. the amount of reservoir nodes that are visible to an observer, will impact
performance.

Lastly, being able to vary the input that each node sees freely is desirable,
especially when considering substrates with a restrictive topology. This is a
property that has previously been suggested to hold for ESNs
\cite{jaeger_echo_2001}.

The rest of this paper is structured as follows: first we provide relevant
background theory, including the motivation behind the exploration of each
physical limitation. Next we present methods and the simulation setup, followed
by a section on results and discussion. Finally we draw conclusions and suggest
future work.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End: