\begin{abstract}
  Reservoir Computing (RC) emerged as an alternative framework to the traditional gradient descent methods for training Recurrent Neural Networks (RNNs). Key to this methodology is a randomly generated reservoir, commonly an RNN that remains untrained, and a linear readout layer that is trained using simple one-shot learning methods. Interestingly, there is no need for the reservoir to be an artificial neural network -- any high-dimensional, driven system exhibiting complex dynamic behavior can be used. A wide range of physical reservoirs have been realized, ranging from optical laser circuits and nanomagnetic assemblies to biological neural networks. The computational performance of such physical substrates is closely related to common physical limitations, e.g. noise, measurement accuracy, partially visible reservoir state, and physical morphology. Here we investigate these fundamental properties of physical reservoirs, offering insights into impairments that may be present with such limitations, which in the wider context help improve the design of future substrates (TODO: concrete examples).
\end{abstract}

\begin{IEEEkeywords}
  Reservoir computing, unconventional computing, echo state networks, time series computing (TODO: finish).
\end{IEEEkeywords}

% Thoughts
% * Perhaps using the word «physical» too much?
% * In the future: add concrete findings to the end.
% * Finish IEEEkeywords.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End: