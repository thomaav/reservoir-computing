\chapter{Background}

Related work and the state of the art were reviewed, and identification of
relevant background material was carried out in the project preceding this
thesis \cite{aven_exploring_2019}. This background is herein amended with deeper
insights into the paradigm of physical reservoir computing, and adapted to fit a
thesis rather than an article. Specifically, the main focus is transferred from
physical limitations in general, to the narrower scope of spatial limitations
relating to physical morphology and topology.

\textcolor{red}{
  The chapter covers important material used throughout the text, as well as a
short historical background for context. Talk about sections.
}

\section{Complexity: Order, Chaos and Criticality}

In deeming a physical system able to \textit{compute}, one implies information
storage, retrieval and modification. We are as humans intimately familiar with
the continuous, yet spontaneous computation present in our brains -- our
consciousness. We are less acquainted, however, with the conditions that
\textit{caused} the emergence of such a system.

Spanning a wide range of topics and disciplines, the field of \textit{complexity
theory} seeks answers to this conundrum. An exact definition of ``complexity''
is perhaps ever so elusive, but at its core lies an emergence of behavior
greater than the sum of its parts. Simple, local interactions give rise to
intricate, global patterns. This spontaneous emergence of complex behavior is
ubiquitous in nature. Ranging from convection cells in physics, to swarm and
flock behavior in biology, there is an abundance of interesting phenomena to
study \cite{heylighen_science_1999-1}.

Langton investigated the emergence of computation in cellular automata (CA)
\cite{langton_computation_1990}. His findings indicate a \textit{criticality} as
a condition to support computation. In essence, in between \textit{ordered} and
\textit{chaotic} dynamics, we find a critical phase transition. It is these
systems, intertwining order and chaos, that are of interest.

In systems that are too static, perturbation will fade too quickly. Chaotic
systems, on the other hand, are wildly unpredictable, making them excessively
sensitive. This \textit{edge of chaos} is a recurring theme in the investigation
of the computational capabilities of physical systems. In fact, the edge of
chaos has been found to be of significant in predicting the computational
performance of for neural microcircuit models, consisting of spiking neurons and
dynamic synapses \cite{legenstein_edge_2007}.

Biologically inspired models, most famously the artificial neural network (ANN),
are valuable scientific tools. Oftentimes, finding a suitable set of parameters
for a model will amount to much the same as finding the critical phase
transition between order and chaos. Reservoir computing (RC), a niche framework
within the field of machine learning, is concerned with observing the inherent
dynamics of a ``reservoir'' of local interconnections. Often employing random
neural networks, RC exploits the intrinsic computation emerging from these local
interconnections to solve practical tasks.

\textcolor{red}{
  Should we include any figures? There is much beauty to behold.
}

\section{Reservoir Computing Fundamentals}

Training recurrent neural networks (RNN) is an inherently difficult
task. Gradient descent methods that incorporate loss information become
increasingly inefficient on problems with long-range temporal dependencies. This
inefficiency makes the backpropagation algorithm used with feed-forward
structures less attractive. Specifically, a continuous search in the parameter
space of recurrent networks may cause bifurcations points in the dynamics of the
system, causing non-convergent training \cite{doya_bifurcations_nodate}. To
circumvent this complexity, alternative methods that leave internal weights
untrained have been proposed.

Liquid State Machines (LSMs) \cite{maass_real-time_2002} and Echo State Networks
(ESNs) \cite{jaeger_echo_2001} independently present supervised learning methods
that do not adapt the internal weights of the RNN. Instead, the output is
generated using a simple, memoryless classifier or regressor. This makes the RNN
function much like a kernel in kernel method algorithms, which seeks features
and general relations in datasets to increase separability.

Thus, by projecting into a high-dimensional space, temporal information of an
input may be incorporated in an instantaneous readout. This methodology has been
unified into the research subfield of RC \cite{schrauwen_overview_2007}, in
which the focus is on separating the randomly generated \textit{reservoir} from
the trained readout layer.

Interestingly, there is no need for the reservoir to be an artificial neural
network -- any high-dimensional, driven system exhibiting complex dynamic
behavior can be used \cite{schrauwen_overview_2007}. As long as the dynamics of
the \textit{substrate} can respond suitably to input, it can in theory be used
as a reservoir.

% (TODO): Correct section reference.
A multitude of substrates have shown promise as reservoirs: dynamical systems
models such as CA \cite{yilmaz_reservoir_2014}, and the more general random
Boolean network (RBN) \cite{snyder_computational_2013}, provide a discrete
alternative to the analogue ESN. Furthermore, a recent surge in physical
reservoir implementations has reinvigorated the field, which is further
introduced in Section \textcolor{red}{ref}.

\section{Echo State Networks}

\subsection{Echo State Network Internals}

% (TODO): Ask about re-using figures and text. Should be fine as it is cited and
% specified. Rephrasing is a stupid waste of time.

\begin{figure}[t!]
  \centering
  \includegraphics[width=3.0in]{figures/esn.png}
  \caption{
    Fig. 1: Basic architecture of ESN reservoir systems. The reservoir acts as a
high-dimensional kernel, transforming the temporal input sequence into a spatial
representation. The readout is trained with supervised linear regression,
providing a least squares optimum.
  }
  \label{fig:narma10}
\end{figure}

\textcolor{red}{
  Perhaps the most widely used and known reservoir. A section that specifically
explains how the default Echo State Networks function.
}

\textcolor{red}{
  Common tunings, such as input scaling, spectral radius, leakage and
sparsity. Extensions to base model. Bias, output feedback, teacher forced
etc. Also deep ESNs now (most recent, actually quite recent).
}

\textcolor{red}{
  In practice, we really just see a big difference from input scaling and
scaling the spectral radius, others are marginal. Practicality and empiricism.
}

\textcolor{red}{
  Some minor details about work that has been done to improve upon the model,
such as intrinsic plasticity, leaking neurons, lateral inhibitions, and so
on. Can/should be adapted from pre-thesis.
}

\textcolor{red}{
  Echo State Property and short-term memory.
}

\subsection{Training}

\textcolor{red}{
  Ridge Regression with friends.
}

\subsection{Real World Applications}

The ESN methodology has been applied somewhat successfully to real world
tasks. Approaches include equalizing a wireless communication channel
\cite{jaeger_harnessing_2004}, and short-term traffic \cite{an_short-term_2011},
electric load \cite{song_hourly_2011}, and stock price forecasting
\cite{lin_short-term_2009}. Robot control is also a popular area of research for
RC applications, particularly for motor control and event detection
\cite{aislan_antonelo_learning_2015, harding_evolution_2005,
hutchison_movement_2004}. Perhaps less conventionally, RC has also been applied
in the context of reinforcement learning \cite{bush_modeling_2005}.

However, as the practicality of the paradigm resides primarily in chaotic time
series prediction and classification, this is also its main focus. Furthermore,
recent years have seen an increase in the realization of physical reservoirs to
accompany existing software simulations. An example is a silicon photonics chip
capable of 5-bit header recognition up to 12.5 Gbits$^{-1}$, and is scalable to
even higher bitrates \cite{vandoorne_experimental_2014}. This surge of optimism
has breathed new life into the field of RC, as physical reservoirs pave the way
for new types of integrated chips.

\subsection{Comparison to State of the Art}

Few definitive comparisons between the ESN and similar flavors of the RNN have
been carried out. The long short-term memory (LSTM) \cite{hochreiter_long_1997},
as well as its more recent descendant, the gated recurrent unit (GRU)
\cite{cho_learning_2014}, are mainstays in sequence processing, particularly in
natural language processing. Early experiments demonstrated the ESN methodology
to outperform previous LSTM methods on learning a chaotic attractor by orders of
magnitude \cite{jaeger_echo_2001}, but ESNs have largely remained a secondary
tool outside of chaotic time series prediction and classification.

A recent study compares the promising deep echo state network architecture
(DeepESN) to that of gated RNNs, where an experimental comparison between
recurrent models on multivariate time-series prediction tasks is made
\cite{gallicchio_comparison_2019}. It is established that the DeepESN
methodology outperforms other RNN approaches in their prediction ability on
challenging, real world datasets. The computation time is also lessened about
one order of magnitude compared to fully-trained RNN approaches. Thus, the
adoption of LSTM and GRU in practice may not necessarily be based on performance
suitability, but rather software availability and popularity.

\section{Physical Reservoir Computing}

\textcolor{red}{
  Often used quite interchangeably with ``Evolution in Materio''.
}

\textcolor{red}{
  Anything exhibiting complex dynamic behavior may be used. Explain this, and
why this is, and what implications this carries.
}

\textcolor{red}{
  The emergence of physical substrates as a medium for the reservoir computing
paradigm. Illustrate some of the main topics, like photonics, optoeletronics,
mechanical (coupled oscillators), biologically inspired (ecoli, invitro),
magnetism. Use the survey paper by Tanaka et al.
}

\textcolor{red}{
  It is importance to stress that we are considering alternatives to the
traditional reservoirs in this section. Drive home the point that we are
exploring physical mediums that are wildly different from traditional computers,
where memory and computation and fundamentally separate.
}

\textcolor{blue}{
  ``There are many physical properties and considerations that require
discussion when talking about using any novel material for computation. Here we
focus on four key factors that possess some relevance to substrate-based
reservoirs: (i) a means by which to observe network connectivity and activity,
(ii) assuring non-linearity is present in the system, (iii) methods for
modelling activity and behaviour, and (iv) the impact of environmental
factors.'', in-materio paper, Dale et al.
}

\textcolor{blue}{
  Something similar in ``Recent advances..'' as well, with (i-iv) points about
what is necessary for something to be a physical reservoir.
}

In practice, physical reservoirs have been realized with a broad range of
substrates: photonic systems \cite{vandoorne_experimental_2014}, electronic
memristor circuits \cite{kulkarni_memristor-based_2012}, mechanical springs
\cite{hauser_towards_2011}, and more biologically oriented reservoirs such as
gene regulation networks \cite{jones_is_2007}, and the cat primary visual cortex
\cite{scholkopf_temporal_2007}. Consult \cite{tanaka_recent_2018} for a thorough
review of investigated physical substrates, their applications, and the general
trends in physical RC.

\subsection{Topology and Spatial Networks}

\textcolor{red}{
  Use a previous section on physical reservoir computing to lead into background
on networks that are manifested into a physical space. Talk about previous work,
such as the Minimum Complexity Echo State Network, which has research goals that
very closely resemble that of this thesis.
}

\textcolor{red}{
  Discuss existing theory on spatial networks. Relevant topics may include the
Erdős–Rényi model as a basis for Echo State Networks, then the Waxman model with
its physical restraints, and then a dive further into completely regular
structures such as lattices. Watts-Strogatz.
}

\textcolor{blue}{
  Previous work on topology. For example ``Reservoir Topology in Deep Echo State
Networks'' by Gallicchio et al., ``Optimal modularity and memory capacity of
neural reservoirs'', Rodriguez et al., ``Role of Structure..'', Dale et al.,
``Stability and Topology in Reservoir Computing'', Manevitz et al., and lastly
all of Rodan's thesis on CRJ and deterministically constructed networks.
}

\textcolor{blue}{
  ``Graph Echo State Networks'' and ``Tree Echo State Networks'' by
Gallicchio. \cite{gallicchio_tree_2013}. \cite{gallicchio_graph_2010}. The
possibility of extending neural networks methodologies for learning in domains
of highly structured data representations.
}

\section{Assessing Reservoir Quality}

Designing \textit{good} reservoirs, possessing some set of desired properties,
naturally requires some metric by which we can evaluate and compare. Parameter
sweeps, i.e. our trial and error methods, must be accompanied by sufficient
methods of assessing computational performance.

Evaluation of reservoir quality is split into two different
approaches. Intuitively, measuring the performance of the model on a given
benchmark task is a simple, direct way of assessment. However, to gain an
intuition for a more general, expected performance across multiple benchmarks,
one may measure independent properties of the system, e.g. the spectral radius
of the internal weight matrix. The two approaches are often used in conjunction,
combined to propose an overall quality.

\subsection{Independent Metrics}

\subsubsection{Kernel Quality and Generalization}

Within the RC paradigm we are concerned with producing a complex mapping from
the input stream to some spatial, internal representation, such that a
memory-less, linear readout map may be employed for classification.

The \textit{linear separation property}, or \textit{kernel quality}, measures
ability to separate different inputs \cite{legenstein_edge_2007}. It is an
empirical measure of this complex mapping, denoting the potential diversity of
nonlinear operations carried out by a reservoir. Kernel quality is evaluated by
presenting a reservoir of size $n$ with $m$ different input sequences, and
computing the rank of the resulting $m\times n$ matrix consisting of the
reservoir states resulting at some time step $t$ for the input stream
\cite{busing_connectivity_2010}.

Another metric accompanying the kernel quality is the \textit{generalization
capability} of the reservoir \cite{legenstein_edge_2007}. This metric addresses
ability to generalize already learned functions or filters to new, unseen input,
and is used as an estimation of the VC-dimension of the
reservoir. Generalization capability is evaluated with the same method as kernel
quality, but instead requires input streams that are similar, or belong to the
same class \cite{busing_connectivity_2010}.

A reservoir in the ordered regime will naturally exhibit low values on both
metrics, while both metrics will be high in a network in the chaotic
regime. Thus, in general, reservoirs exhibiting a high kernel quality and a low
generalization rank are desirable, and the difference between the two is
sometimes used as its own metric \cite{busing_connectivity_2010}.

\subsubsection{Short-Term Memory}

Short-term memory capacity was introduced as a quantitative measurement of
linear memory capacity in reservoirs \cite{jaeger_short_2002}. It is a way to
examine the fading memory present in the system, and is measured by attaching
output units to the reservoir, which each are trained to recall some time
delayed version of the input sequence. By measuring how much of the input each
output unit can recover, we can estimate the memory capacity $MC$ by summing
over all time delays. Jaeger defined this as

\begin{equation}
  MC =
  \sum_{k}^{\infty}MC_{k} =
  \frac
  {cov^2(u(t-k), y_k(t))}
  {\sigma^{2}(u(t))\sigma^{2}(y_{k}(t))}
  ,
  \label{stm-eq}
\end{equation}

where $MC$ in general is limited by the reservoir size $N$, such that $MC \leq
N$. High input retention is a desirable property, but an increase in memory
capacity through parameter tuning is often met with a decrease in complex
information processing, due to a universal trade-off between memory and
nonlinearity \cite{dambre_information_2012}.

% (TODO): Not entirely orthogonal from generalization (see Burkow).

\subsubsection{Memory-Nonlinearity Trade-off}

Experimentation with a wide range of reservoirs has indicated a crucial
interplay between the memory and nonlinearity properties in reservoir operation
\cite{verstraeten_memory_2010}. In fact, the interplay has been uncovered to be
a universal trade-off between depth of memory and nonlinear computation
performed by a dynamical system \cite{dambre_information_2012}.

Thus, analyzing the boundary between an ordered, static regime that provides
memory, and a chaotic, dynamic regime that provides processing, is of vital
importance in the design of reservoirs. Determining the required nonlinearity
for a task is not a simple task, and often benefits from intuition about
nonlinear dynamics. Empirically, it has been shown that the input scaling,
determining the nonlinearity of reservoir responses, and the spectral radius,
scaling the importance of previous states, are the main parameters for
optimization in ESNs, illustrating the significance of the trade-off
\cite{montavon_practical_2012}.

Further formalization of the trade-off has been conducted, accompanied by a
proposition of a mixture reservoir combining both linear and nonlinear
dynamics. Adding a ``pinch of linearity'' is cited to improve performance
considerably \cite{inubushi_reservoir_2017}.

\subsubsection{Further Metrics}

A handful of other methods to assess quality and criticality of reservoirs have
been adapted, including the Lyapunov exponent
\cite{verstraeten_experimental_2007}, the Jacobian of the reservoir
\cite{alippi_quantification_2009}, Fisher information
\cite{livi_determination_2018}, and a separation ratio
\cite{gibbons_unifying_2010}.

In summary, given the vast amount of methods for evaluation, choosing a set of
suitable metrics is a surmountable task. This is especially so, given that few
metrics are entirely orthogonal, and are can be found to strongly correlate in
the prediction of performance \cite{chrol-cannon_correlation_2014}.

\subsection{Benchmarks}

Employing benchmarks to measure the performance of reservoirs is a means to
directly capture performance on specific tasks. Myriads of benchmarks exist
within the field of time series prediction, generation, and classification. The
benchmark spectrum range from simple tasks, to complicated, highly dynamic and
autoregressive time series.

Simpler tasks include the XOR problem of which is not linearly separable
\cite{goos_pattern_2003}, and n-bit temporal density and parity
\cite{bertschinger_real-time_2004}. More complex tasks may range from
recognizing isolated digits in speech \cite{verstraeten_isolated_2005}, to
predicting time series, of which the most popular are the Mackey-Glass time
delay differential equation \cite{mackey_oscillation_1977}, and the nonlinear
autoregressive moving average, NARMA \cite{atiya_new_2000}. Further datasets,
such as the Santa Fe Laser, Hénon Map, IPIX Radar, and Sunspot series datasets
have also been used \cite{rodan_minimum_2011}.

\subsubsection{NARMA - Nonlinear Autoregressive Moving Average}

\begin{figure}[t!]
  \centering
  \includegraphics[width=3.0in]{figures/NARMA10.png}
  \caption{
    Example output generated by a 10th-order NARMA system. The autoregressive
moving average nature of the time series is clearly visible.
  }
  \label{fig:narma10}
\end{figure}

The class of time series provided by a nonlinear autoregressive moving average,
most often simply referred to as \textit{NARMA}, is a model commonly used to
benchmark recurrent networks \cite{atiya_new_2000}. Its widespread use yields
baseline performances for well established models, as well as more novel
approaches \cite{verstraeten_experimental_2007, appeltant_information_2011}.

NARMA provides discrete-time temporal tasks, introducing a time-lag of $n$ time
steps, and is given by

\begin{equation}
  y_{t} = \alpha y_{t-1} +
  \beta y_{t-1} \sum_{i=1}^{n}y_{t-i} +
  \gamma u_{t-1}u_{t-n} +
  \delta
  .
  \label{eq:narma}
\end{equation}

Here, $n$ is the order of the system, and common constant parameters are $\alpha
= 0.3$, $\beta = 0.05$, $\gamma = 1.5$ and $\delta = 0.1$. The input $u_{t}$ is
an i.i.d. stream drawn uniformly from the interval [0, 0.5]. The time series is
unstable, and tasks with higher than a 10th-order time lag introduce a
saturation function to produce a bounded sequence:

\begin{equation}
  y_{t} =
  \tanh(
  \alpha y_{t-1} +
  \beta y_{t-1} \sum_{i=1}^{n}y_{t-i} +
  \gamma u_{t-1}u_{t-n} +
  \delta
  )
  .
  \label{eq:narma-tanh}
\end{equation}

\begin{figure}[t!]
  \centering
  \includegraphics[width=3.0in]{figures/NARMA-nonlinearity.png}
  \caption{
    Nonlinear mapping of the product $u_{t-1}u_{t-n}$ of inputs in the NARMA
time series in Equation \protect\ref{eq:narma}.
  }
  \label{fig:narma-nonlinearity}
\end{figure}

Predicting a NARMA time series, given an input sequence $u$, presents a
challenge of both memory and nonlinearity. This makes NARMA well-suited for
evaluating both the memory capacity and computational power of reservoirs with a
single metric. Reservoirs must necessarily remember input sequences of length
$n$, and should preferably adhere to suitable dynamics on top of this. A
10th-order system is depicted in Figure \ref{fig:narma10}, and a nonlinear
product on the input sequence is shown in Figure \ref{fig:narma-nonlinearity}.

Evaluation of ESN performance on the NARMA10 system is a thoroughly explored
area in the field of RC. Reported NRMSE performances for traditional ESN
reservoirs of size $N = 200$ lie in the range [0.20, 0.25]
\cite{verstraeten_experimental_2007, rodan_minimum_2011,
goudarzi_comparative_2014, jaeger_adaptive_2003}. For some context, using a
shift register containing the input as a reservoir will achieve a minimal NRMSE
of 0.4. To achieve NRMSE values below this threshold it is necessary to
introduce nonlinearity in the reservoir.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End: