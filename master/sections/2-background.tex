\chapter{Background}

Related work and the state of the art were reviewed, and identification of
relevant background material was carried out in the project preceding this
thesis \cite{aven_exploring_2019}. This background is herein amended with deeper
insights into the paradigm of physical reservoir computing. Specifically, the
main focus is transferred from physical limitations in general, to the narrower
scope of spatial limitations relating to physical morphology and topology.

\textcolor{red}{
  The chapter covers important material used throughout the text, as well as a
short historical background for context. Talk about sections.
}

\section{Complexity: Order, Chaos and Criticality}

In deeming a physical system able to \textit{compute}, one implies information
storage, retrieval and modification. We are as humans intimately familiar with
the continuous, yet spontaneous computation present in our brains -- our
consciousness. We are less acquainted, however, with the conditions that
\textit{caused} the emergence of such a system.

Spanning a wide range of topics and disciplines, the field of \textit{complexity
theory} seeks answers to this conundrum. An exact definition of ``complexity''
is perhaps ever so elusive, but at its core lies an emergence of behavior
greater than the sum of its parts. Simple, local interactions give rise to
intricate, global patterns. This spontaneous emergence of complex behavior is
ubiquitous in nature. Ranging from convection cells in physics, to swarm and
flock behavior in biology, there is an abundance of interesting phenomena to
study \cite{heylighen_science_1999-1}.

Langton investigated the emergence of computation in cellular automata (CA)
\cite{langton_computation_1990}. His findings indicate a \textit{criticality} as
a condition to support computation. In essence, in between \textit{ordered} and
\textit{chaotic} dynamics, we find a critical phase transition. It is these
systems, intertwining order and chaos, that are of interest.

In systems that are too static, perturbation will fade too quickly. Chaotic
systems, on the other hand, are wildly unpredictable, making them excessively
sensitive. This \textit{edge of chaos} is a recurring theme in the investigation
of the computational capabilities of physical systems. In fact, the edge of
chaos has been found to be of significant in predicting the computational
performance of for neural microcircuit models, consisting of spiking neurons and
dynamic synapses \cite{legenstein_edge_2007}.

Biologically inspired models, most famously the artificial neural network (ANN),
are valuable scientific tools. Oftentimes, finding a suitable set of parameters
for a model will amount to much the same as finding the critical phase
transition between order and chaos. Reservoir computing (RC), a niche framework
within the field of machine learning, is concerned with observing the inherent
dynamics of a ``reservoir'' of local interconnections. Often employing random
neural networks, RC exploits the intrinsic computation emerging from these local
interconnections to solve practical tasks.

\textcolor{red}{
  Should we include any figures? There is much beauty to behold.
}

\section{Reservoir Computing}

\textcolor{blue}{
  ``Instead it proposes that the trajectory of internal states of a recurrent
neural circuit provides a raw, unbiased, and universal source of temporally
integrated information, from which specific readout elements can extract
specific information about past inputs for their individual task.'', from Maass
et al. paper.
}

\textcolor{blue}{
  I have some in the abstract I wrote for SPRAK3501.
}

\section{Echo State Networks}

\subsection{Recurrent Neural Networks}

\textcolor{red}{
  Their use, and why backward edges makes it notoriously difficult to train such
networks.
}

\textcolor{blue}{
  Problems with RNN and LSTM: page 5 of ``Short Term Memory in ESNs'', by Jaeger
has some info on LSTM problem with chaos, and RNN training problem on long
temporal dependencies.
}

\subsection{Echo State Networks}

\textcolor{red}{
  The inception of reservoir computing as a methodology with Jaeger's Echo State
Network and Maass's Liquid State Machine.
}

\textcolor{red}{
  A section that specifically explains how the default Echo State Networks
function. Some minor details about work that has been done to improve upon the
model, such as intrinsic plasticity, leaking neurons, lateral inhibitions, and
so on. Can/should be adapted from pre-thesis.
}

\textcolor{red}{
  Common tunings, such as input scaling, spectral radius, leakage and
sparsity. Extensions to base model. Bias, output feedback, teacher forced
etc. Also deep ESNs now (most recent, actually quite recent).
}

\textcolor{red}{
  Echo State Property and short-term memory.
}

\subsection{Real World Applications}

The ESN methodology has been applied somewhat successfully to real world
tasks. Approaches include equalizing a wireless communication channel
\cite{jaeger_harnessing_2004}, and short-term traffic \cite{an_short-term_2011},
electric load \cite{song_hourly_2011}, and stock price forecasting
\cite{lin_short-term_2009}. Robot control is also a popular area of research for
RC applications, particularly for motor control and event detection
\cite{aislan_antonelo_learning_2015, harding_evolution_2005,
hutchison_movement_2004}. Perhaps less conventionally, RC has also been applied
in the context of reinforcement learning \cite{bush_modeling_2005}.

However, as the practicality of the paradigm resides primarily in chaotic time
series prediction and classification, this is also its main focus. Furthermore,
recent years have seen an increase in the realization of physical reservoirs to
accompany existing software simulations. An example is a silicon photonics chip
capable of 5-bit header recognition up to 12.5 Gbits$^{-1}$, and is scalable to
even higher bitrates \cite{vandoorne_experimental_2014}. This surge of optimism
has breathed new life into the field of RC, as physical reservoirs pave the way
for new types of integrated chips.

\subsection{Comparison to State of the Art}

\textcolor{red}{
  Perhaps a short comparison to RNN training methods, LSTM, and GRU. If not a
complete subsection, at least introduce this in some of the other subsections,
as to mention that there are other areas of research concerned with the same
types of tasks. I don't know if this is necessary.
}

\textcolor{red}{
  sota: gru, lstm, bi-lstm are some current ones that are used a
lot. ``Comparison between DeepESNs and gatedRNNs on multivariate time-series
prediction'' is a good one to actually get some insight. This is very lacking in
literature!
}

\section{Physical Reservoir Computing}

\textcolor{red}{
  Anything exhibiting complex dynamic behavior may be used. Explain this, and
why this is, and what implications this carries.
}

\textcolor{red}{
  The emergence of physical substrates as a medium for the reservoir computing
paradigm. Illustrate some of the main topics, like photonics, optoeletronics,
mechanical (coupled oscillators), biologically inspired (ecoli, invitro),
magnetism. Use the survey paper by Tanaka et al.
}

\textcolor{red}{
  It is importance to stress that we are considering alternatives to the
traditional reservoirs in this section. Drive home the point that we are
exploring physical mediums that are wildly different from traditional computers,
where memory and computation and fundamentally separate.
}

\textcolor{blue}{
  ``There are many physical properties and considerations that require
discussion when talking about using any novel material for computation. Here we
focus on four key factors that possess some relevance to substrate-based
reservoirs: (i) a means by which to observe network connectivity and activity,
(ii) assuring non-linearity is present in the system, (iii) methods for
modelling activity and behaviour, and (iv) the impact of environmental
factors.'', in-materio paper, Dale et al.
}

\subsection{Spatial Networks}

\textcolor{red}{
  Use a previous section on physical reservoir computing to lead into background
on networks that are manifested into a physical space. Talk about previous work,
such as the Minimum Complexity Echo State Network, which has research goals that
very closely resemble that of this thesis.
}

\textcolor{red}{
  Discuss existing theory on spatial networks. Relevant topics may include the
Erdős–Rényi model as a basis for Echo State Networks, then the Waxman model with
its physical restraints, and then a dive further into completely regular
structures such as lattices. Watts-Strogatz.
}

\textcolor{blue}{
  Previous work on topology. For example ``Reservoir Topology in Deep Echo State
Networks'' by Gallicchio et al., ``Optimal modularity and memory capacity of
neural reservoirs'', Rodriguez et al., ``Role of Structure..'', Dale et al.,
``Stability and Topology in Reservoir Computing'', Manevitz et al., and lastly
all of Rodan's thesis on CRJ and deterministically constructed networks.
}

\section{Assessing Reservoir Quality}

Designing \textit{good} reservoirs, possessing some set of desired properties,
naturally requires some metric by which we can evaluate and compare. Parameter
sweeps, i.e. our trial and error methods, must be accompanied by sufficient
methods of assessing computational performance.

Evaluation of reservoir quality is split into two different
approaches. Intuitively, measuring the performance of the model on a given
benchmark task is a simple, direct way of assessment. However, to gain an
intuition for a more general, expected performance across multiple benchmarks,
one may measure independent properties of the system, e.g. the spectral radius
of the internal weight matrix. The two approaches are often used in conjunction,
combined to propose an overall quality.

\subsection{Independent Metrics}

\subsubsection{Kernel Quality and Generalization}

Within the RC paradigm we are concerned with producing a complex mapping from
the input stream to some spatial, internal representation, such that a
memory-less, linear readout map may be employed for classification.

The \textit{linear separation property}, or \textit{kernel quality}, measures
ability to separate different inputs \cite{legenstein_edge_2007}. It is an
empirical measure of this complex mapping, denoting the potential diversity of
nonlinear operations carried out by a reservoir. Kernel quality is evaluated by
presenting a reservoir of size $n$ with $m$ different input sequences, and
computing the rank of the resulting $m\times n$ matrix consisting of the
reservoir states resulting at some time step $t$ for the input stream
\cite{busing_connectivity_2010}.

Another metric accompanying the kernel quality is the \textit{generalization
capability} of the reservoir \cite{legenstein_edge_2007}. This metric addresses
ability to generalize already learned functions or filters to new, unseen input,
and is used as an estimation of the VC-dimension of the
reservoir. Generalization capability is evaluated with the same method as kernel
quality, but instead requires input streams that are similar, or belong to the
same class \cite{busing_connectivity_2010}.

A reservoir in the ordered regime will naturally exhibit low values on both
metrics, while both metrics will be high in a network in the chaotic
regime. Thus, in general, reservoirs exhibiting a high kernel quality and a low
generalization rank are desirable, and the difference between the two is
sometimes used as its own metric \cite{busing_connectivity_2010}.

\subsubsection{Short-Term Memory}

Short-term memory capacity was introduced as a quantitative measurement of
linear memory capacity in reservoirs \cite{jaeger_short_2002}. It is a way to
examine the fading memory present in the system, and is measured by attaching
output units to the reservoir, which each are trained to recall some time
delayed version of the input sequence. By measuring how much of the input each
output unit can recover, we can estimate the memory capacity $MC$ by summing
over all time delays. Jaeger defined this as

\begin{equation}
  MC =
  \sum_{k}^{\infty}MC_{k} =
  \frac
  {cov^2(u(t-k), y_k(t))}
  {\sigma^{2}(u(t))\sigma^{2}(y_{k}(t))}
  ,
  \label{stm-eq}
\end{equation}

where $MC$ in general is limited by the reservoir size $N$, such that $MC \leq
N$. High input retention is a desirable property, but an increase in memory
capacity through parameter tuning is often met with a decrease in complex
information processing, due to a universal trade-off between memory and
nonlinearity \cite{dambre_information_2012}.

% (TODO): Not entirely orthogonal from generalization (see Burkow).

% (TODO): Cite "Computational analysis of memory capacity in echo state
% networks" in the methodology section of this specifically for ESNs.

\subsubsection{Memory-Nonlinearity Trade-off}

Experimentation with a wide range of reservoirs has indicated a crucial
interplay between the memory and nonlinearity properties in reservoir operation
\cite{verstraeten_memory_2010}. In fact, the interplay has been uncovered to be
a universal trade-off between depth of memory and nonlinear computation
performed by a dynamical system \cite{dambre_information_2012}.

Thus, analyzing the boundary between an ordered, static regime that provides
memory, and a chaotic, dynamic regime that provides processing, is of vital
importance in the design of reservoirs. Determining the required nonlinearity
for a task is not a simple task, and often benefits from intuition about
nonlinear dynamics. Empirically, it has been shown that the input scaling,
determining the nonlinearity of reservoir responses, and the spectral radius,
scaling the importance of previous states, are the main parameters for
optimization in ESNs, illustrating the significance of the trade-off
\cite{montavon_practical_2012}.

Further formalization of the trade-off has been conducted, accompanied by a
proposition of a mixture reservoir combining both linear and nonlinear
dynamics. Adding a ``pinch of linearity'' is cited to improve performance
considerably \cite{inubushi_reservoir_2017}.

\subsubsection{Further Metrics}

A handful of other methods to assess quality and criticality of reservoirs have
been adapted, including the Lyapunov exponent
\cite{verstraeten_experimental_2007}, the Jacobian of the reservoir
\cite{alippi_quantification_2009}, Fisher information
\cite{livi_determination_2018}, and a separation ratio
\cite{gibbons_unifying_2010}.

In summary, given the vast amount of methods for evaluation, choosing a set of
suitable metrics is a surmountable task. This is especially so, given that few
metrics are entirely orthogonal, and are can be found to strongly correlate in
the prediction of performance \cite{chrol-cannon_correlation_2014}.

\subsection{Benchmarks}

Employing benchmarks to measure the performance of reservoirs is a means to
directly capture performance on specific tasks. Myriads of benchmarks exist
within the field of time series prediction, generation, and classification. The
benchmark spectrum range from simple tasks, to complicated, highly dynamic and
autoregressive time series.

Simpler tasks include the XOR problem of which is not linearly separable
\cite{goos_pattern_2003}, and n-bit temporal density and parity
\cite{bertschinger_real-time_2004}. More complex tasks may range from
recognizing isolated digits in speech \cite{verstraeten_isolated_2005}, to
predicting time series, of which the most popular are the Mackey-Glass time
delay differential equation \cite{mackey_oscillation_1977}, and the nonlinear
autoregressive moving average, NARMA \cite{atiya_new_2000}. Further datasets,
such as the Santa Fe Laser, Hénon Map, IPIX Radar, and Sunspot series datasets
have also been used \cite{rodan_minimum_2011}.

\subsubsection{NARMA - Nonlinear Autoregressive Moving Average}

\begin{figure}[t!]
  \centering
  \includegraphics[width=3.0in]{figures/NARMA10.png}
  \caption{
    Example output generated by a 10th-order NARMA system. The autoregressive
moving average nature of the time series is clearly visible.
  }
  \label{fig:narma10}
\end{figure}

The class of time series provided by a nonlinear autoregressive moving average,
most often simply referred to as \textit{NARMA}, is a model commonly used to
benchmark recurrent networks \cite{atiya_new_2000}. Its widespread use yields
baseline performances for well established models, as well as more novel
approaches \cite{verstraeten_experimental_2007, appeltant_information_2011}.

NARMA provides discrete-time temporal tasks, introducing a time-lag of $n$ time
steps, and is given by

\begin{equation}
  y_{t} = \alpha y_{t-1} +
  \beta y_{t-1} \sum_{i=1}^{n}y_{t-i} +
  \gamma u_{t-1}u_{t-n} +
  \delta
  .
  \label{eq:narma}
\end{equation}

Here, $n$ is the order of the system, and common constant parameters are $\alpha
= 0.3$, $\beta = 0.05$, $\gamma = 1.5$ and $\delta = 0.1$. The input $u_{t}$ is
an i.i.d. stream drawn uniformly from the interval [0, 0.5]. The time series is
unstable, and tasks with higher than a 10th-order time lag introduce a
saturation function to produce a bounded sequence:

\begin{equation}
  y_{t} =
  \tanh(
  \alpha y_{t-1} +
  \beta y_{t-1} \sum_{i=1}^{n}y_{t-i} +
  \gamma u_{t-1}u_{t-n} +
  \delta
  )
  .
  \label{eq:narma-tanh}
\end{equation}

\begin{figure}[t!]
  \centering
  \includegraphics[width=3.0in]{figures/NARMA-nonlinearity.png}
  \caption{
    Nonlinear mapping of the product $u_{t-1}u_{t-n}$ of inputs in the NARMA
time series in Equation \protect\ref{eq:narma}.
  }
  \label{fig:narma-nonlinearity}
\end{figure}

Predicting a NARMA time series, given an input sequence $u$, presents a
challenge of both memory and nonlinearity. This makes NARMA well-suited for
evaluating both the memory capacity and computational power of reservoirs with a
single metric. Reservoirs must necessarily remember input sequences of length
$n$, and should preferably adhere to suitable dynamics on top of this. A
10th-order system is depicted in Figure \ref{fig:narma10}, and a nonlinear
product on the input sequence is shown in Figure \ref{fig:narma-nonlinearity}.

Evaluation of ESN performance on the NARMA10 system is a thoroughly explored
area in the field of RC. Reported NRMSE performances for traditional ESN
reservoirs of size $N = 200$ lie in the range [0.20, 0.25]
\cite{verstraeten_experimental_2007, rodan_minimum_2011,
goudarzi_comparative_2014, jaeger_adaptive_2003}. For some context, using a
shift register containing the input as a reservoir will achieve a minimal NRMSE
of 0.4. To achieve NRMSE values below this threshold it is necessary to
introduce nonlinearity in the reservoir.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End: