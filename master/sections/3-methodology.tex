\chapter{Methodology}

\section{ESN Parameters and Sample Sizes}

In this section we present the baseline ESN used during experiments. For each
experiment described in this thesis, the default setup will be as described in
this section, unless otherwise specified. All ESNs are generated according to
the architecture presented in Figure \ref{fig:esn}.

We consider discrete-time ESNs with $N$ internal network nodes, a single input,
and a single output node. $\mathbf{W}^{in}$ is generated as a random matrix with
i.i.d. entries in the interval [-0.5, 0.5], and is fully connected. Experiments
with default ESNs, i.e. experiments where the internal network
$\mathbf{W}^{res}$ is not replaced, the weights are generated from the same
distribution as $\mathbf{W}^{in}$, but with a 10\% node connectivity. This
method for instantiating $\mathbf{W}^{res}$ and $\mathbf{W}^{in}$ is common
practice in RC \cite{montavon_practical_2012}.

In all experiments where relevant, the reservoir weight matrix was rescaled such
that its spectral radius $\rho(\mathbf{W}^{res}) = 0.9$. The default input
scaling used is $\iota = 1.0$. Both parameters could be tuned to provide
marginally better results in most cases, but these values were found to give a
good baseline for comparisons between models.

$\mathbf{W}^{out}$ is adapted with ridge regression, using single value
decomposition for computational routines. This was found to lead to the most
stable and precise results.

% (TODO): Perhaps re-run everything with exactly 20 runs. We need the
% std. dev. anyway for tables in appendix.
For all experiment runs, the first 200 states of each run are discarded to
provide a washout of the initial reservoir state. Reported performances are the
mean across 20 randomizations of each model representative. This sample size was
found to be appropriate to pinpoint definite trends in the results. Standard
deviations for all experiments are thus provided in the appendix.

The Python software library implementation is available online, including a
Jupyter Notebook reproducing each
experiment\footnote{\textcolor{red}{deadlink}}.

\section{Benchmarks and Metrics}

\textcolor{red}{
  We use the NRMSE. Comparability to the state of the art.
}

\textcolor{red}{
  NARMA-10. Implementation of KQ/G and MC as well here. Show memory curve thing,
how much per delay is remembered.
}

% (TODO): Cite "Computational analysis of memory capacity in echo state
% networks" in the methodology section of this specifically for ESNs. For MC.

\section{Experiments}

\textcolor{red}{
  We use standard ESN for everything, for experiments what really differs is the
wres.
}

\textcolor{red}{
  Anyway: for each experiment, describe the experimental setup that was used
such that it may \textit{easily be reproduced}. It has been very helpful when
papers actually provide every relevant parameter such that I could run the
experiments myself.
}

\textcolor{red}{
  For Chapter so-and-so, and chapter so-and-so. Chapter \ref{ch:rgg} and Chapter
\ref{ch:regular-tilings}.
}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End: