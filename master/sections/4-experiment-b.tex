\chapter{Experiments: Lattices}
\label{ch:regular-tilings}

\begin{figure}[t]
  \centering
  \begin{subfigure}{.32\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/square.png}
    \label{fig:rt-square}
  \end{subfigure}
  \begin{subfigure}{.32\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/hex.png}
    \label{fig:rt-hex}
  \end{subfigure}
  \begin{subfigure}{.32\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/triangular.png}
    \label{fig:rt-tri}
  \end{subfigure}
  \caption{
    Types of lattices investigated for their quality as reservoir
topologies. Investigated tilings include square (a), hexagonal (b), and
triangular (c).
  }
  \label{fig:regular-tilings}
\end{figure}

Lattice models are common in computational physics
\cite{lavis_equilibrium_2015}. Understanding important models of computational
physics in reservoir contexts is thus crucial to advance physical RC
methodology. For example, the Ising model with dipole moments of atomic spins
\cite{jensen_computation_2018}, spin-torque oscillator arrays
\cite{tsunegi_physical_2019}, and the Ginzburg-Landau equation
\cite{opala_neuromorphic_2019}, describe systems that are employed on a
two-dimensional lattice, and have been used in reservoir settings. In this
chapter, we therefore investigate lattice networks as more realistic models of
physical reservoirs.

We explore the properties that lattice graphs exhibit as reservoirs by
structuring internal nodes in this manner. Lattice graphs may be embedded in
Euclidean space to form regular tilings, of which there three in two-dimensional
space: square, hexagonal and triangular, which are all depicted in Figure
\ref{fig:regular-tilings}. Other, more complicated tiling schemes
exist. Semiregular, often called uniform, tilings are created using two or more
faces. However, complicated grids are left outside the scope of this thesis, as
our primary focus is the fundamental applicability of lattice layouts, not
comparing the performance between them.

Reservoirs are created by replacing the reservoirs of ESN models with the
adjacency matrix generated for lattice models. For each experiment,
$\mathbf{W}^{res}$ is then scaled to a spectral radius of 0.9, as this is
equivalent to scaling the coupling, or spacing, between nodes in a physical
system. Beyond this, our reservoir model remains the same as that of the ESN.

\section{Reservoir Quality of Lattices}
\label{sec:lattice-quality}

\subsection{Synopsis}

First, we evaluate the default quality of lattice reservoirs with the NARMA-10
benchmark. Reservoirs are generated by embedding internal nodes in a metric
space, much like in Figure \ref{fig:regular-tilings}, and connecting neighboring
pairs with an edge of unit length. Nodes along the edges are not connected to
the opposite side of the lattice, making the lattice aperiodic. Lastly, unit
weight of all edges are scaled by the appropriate scalar to allow a spectral
radius of 0.9.

\subsection{Results and Discussion}

\begin{figure}[htb]
  \centering
  \includegraphics[width=3.5in]{figures/regular-tilings-performance.png}
  \caption{
    NARMA-10 NRMSE of square, hexagonal, and triangular regular tilings as
reservoir topologies.
  }
  \label{fig:rt-performance}
\end{figure}

Figure \ref{fig:rt-performance} shows how reservoir error scales with reservoir
size, depicting NARMA-10 benchmark NRMSE as a function of the amount of hidden
nodes $N$. We see that restricting reservoir topologies to lattice structures
results in a significant performance penalty. Additionally, little difference is
seen between the three types of tilings.

\begin{figure}[htb]
  \centering
  \begin{subfigure}{.32\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/regular-tilings-performance-is-sq.png}
    \caption{}
    \label{fig:rt-is-square}
  \end{subfigure}
  \begin{subfigure}{.32\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/regular-tilings-performance-is-hex.png}
    \caption{}
    \label{fig:rt-is-hex}
  \end{subfigure}
  \begin{subfigure}{.32\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/regular-tilings-performance-is-tri.png}
    \caption{}
    \label{fi:rt-is-tri}
  \end{subfigure}
  \caption{
    Regular tilings investigated for their quality as reservoir topologies, here
as a function of reservoir size and input scaling. Investigated topologies
include square (a), hexagonal (b), and triangular (c) regular tilings.
  }
  \label{fig:rt-performance-is}
\end{figure}

In Section \ref{sec:dist-func}, it was discovered that reservoirs modeling
random geometric graphs exhibited low memory retention. The symptoms are similar
here: the lattice reservoirs perform worse than a delay line would, and only
perform marginally better with an increasing size. Figure
\ref{fig:rt-performance-is} illustrates the effect of scaling the magnitude of
the input. Again, we see clearly see reservoirs favoring low input scaling
values.

We interpret these results to indicate that the structure imposed by an
undirected lattice shifts the point of criticality described in
\ref{sec:criticality}. When input scaling is lessened such that the required
memory capacity for the benchmark task is reached, the error diminishes rapidly,
and the existing reservoir dynamics work as intended.

Curiously, the NRMSE differs only slightly between the three types of
lattice. It seems that it is the overall lattice structure that is important,
not the type of tiling implementing it. We therefore argue that the different
tilings, which in practice dictate the amount of incident edges per vertex, work
mostly as minor tuning parameters. The idea that overall structure is important
is in accordance with our findings in \ref{sec:restore}, concluding that
\textit{how information flows} in the network is vital.

Input scaling decreased the benchmark error of lattice reservoirs, but the best
performing networks of Figure \ref{fig:rt-performance-is}, i.e. the biggest
reservoirs with the lowest input scaling, are not quite comparable to the
ESN. For example, square grid reservoirs of size 200 benchmark a mean NRMSE of
around 0.35, while corresponding ESNs average around 0.25.

Overall, it is interesting that undirected lattice reservoirs perform as well as
they do. On the other hand, the distribution of the input weights are drawn from
a uniform distribution in the interval [-0.5, 0.5], letting internal nodes see
varying representations of the input signal. As physical substrates may differ
in the input schemes they offer, the input scheme will be further investigated
in the next section.

To summarize, we have in this section found undirected lattice reservoirs to
provide promising results. A key discovery of Chapter \ref{ch:rgg} is the
importance of a directed flow of information, and whether directedness also
improves lattice models is the topic of the next section.

\section{Lattices with Directed Edges}
\label{sec:lat-dir-edge}

\subsection{Synopsis}

\begin{figure}[htb]
  \centering
  \begin{subfigure}{.40\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/dir_lattice_025.png}
    \caption{}
    \label{fig:dir-lattice-a}
  \end{subfigure}
  \hspace{25pt}
  \begin{subfigure}{.40\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/dir_lattice_075.png}
    \caption{}
    \label{fig:dir-lattice-b}
  \end{subfigure}
  \caption{
    Example square grids where 25\% (a) and 75\% (b) of the undirected edges are
made directed.
  }
  \label{fig:dir-lattice}
\end{figure}

One of the key discoveries of Chapter \ref{ch:rgg} is that directed edges
improve performance significantly in random geometric graph reservoirs. It is of
interest to repeat this experiment with lattice reservoirs, especially since
information flow is so clearly visible in a lattice structure. We modify the
generated lattice graphs generated in previous sections of this chapter to have
a fraction of directed edges. Figure \ref{fig:dir-lattice} illustrates the
concept for square lattices, where 25\% (Figure \ref{fig:dir-lattice-a}) and
75\% (Figure \ref{fig:dir-lattice-b}) of the edges have been directed. The
directed reservoir edges have a 50\% chance of going in either direction.

\subsection{Results and Discussion}

\begin{figure}[htb]
  \centering
  \begin{subfigure}{.32\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/rt-dir-perf-sq.png}
    \caption{}
    \label{fig:rt-dir-perf-trisurf-sq}
  \end{subfigure}
  \begin{subfigure}{.32\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/rt-dir-perf-hex.png}
    \caption{}
    \label{fig:rt-dir-perf-trisurf-hex}
  \end{subfigure}
  \begin{subfigure}{.32\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/rt-dir-perf-tri.png}
    \caption{}
    \label{fig:rt-dir-perf-trisurf-tri}
  \end{subfigure}
  \caption{
    Benchmark error as a function of reservoir size and directedness. The
fraction of directed edges determines the amount of edges that are left
bidirectional, explained by Figure \protect\ref{fig:dir-lattice}. Shown are
results for square (a), hexagonal (b) and triangular (c) lattices.
  }
  \label{fig:rt-dir-perf-trisurf}
\end{figure}

\begin{figure}[htb]
  \centering
  \includegraphics[width=3.5in]{figures/rt-dir-perf.png}
  \caption{
    Benchmark error as a function of size for directed lattice reservoirs. All
reservoirs are fully directed, and the direction of each edge is decided by an
unbiased coin toss.
  }
  \label{fig:rt-dir-perf}
\end{figure}

Figure \ref{fig:rt-dir-perf-trisurf} presents the results of introducing a
fraction of directed edges to lattice reservoirs. Most reservoirs show reduced
error rates with increasing fractions of directed edges.

A small exception is visible for very small square and hexagonal reservoirs,
where a fully directed reservoir may degrade if the edges align in an
insufficient manner. Nodes in triangular reservoirs have six incident edges, as
opposed to the three and four of square and hexagonal, thus giving a smaller
chance of insufficient reservoirs appearing. Note that this problem of
generating insufficient directions disappears with bigger reservoirs. This
compelling result indicates that our method of generating directed edges
guarantees good reservoirs as their sizes increase -- one does not need to ``get
lucky'' with directions.

Convincing improvements are exhibited once reservoirs become fully directed and
sufficiently big, which is especially visible at the sudden drops at the closest
points of the surface areas. The drops are only sudden when compared to
reservoirs of a lower fraction of directed edges. We plot the cross section of
the surface areas at full directedness in Figure \ref{fig:rt-dir-perf}, showing
that the error rates keep decreasing with increased reservoir size. This is
again in stark contrast to their undirected counterparts, which in Section
\ref{sec:lattice-quality}, and also previous in Section \ref{sec:restore} only
perform marginally better as reservoir size is increased.

\begin{figure}
  \centering
  \includegraphics[width=3.5in]{figures/rt-performance-big.png}
  \caption{
    NRMSE of square lattice reservoirs with global, fixed input schemes compared
to standard ESNs. With the sparse input scheme only 50\% of the hidden nodes see
the input.
  }
  \label{fig:rt-performance-big}
\end{figure}

Thus, twice we have found directed edges to improve reservoir performance
significantly, once in random geometric graph reservoirs in Section
\ref{sec:restore}, and now again in lattices. Despite this, lattice reservoirs
still perform slightly worse than ESN.

In the project preceding this thesis, it was found that a global input scheme,
i.e. an input scheme where every input weight is set to 1 and then scaled, works
with ESNs given appropriate scaling \cite{aven_exploring_2019}. This simple
input scheme is relevant to physical RC, given physical substrates in which
every node is forced to see the same input. Results from using this input scheme
with square grids are shown in Figure \ref{fig:rt-performance-big}.

It is abundantly clear that the global input scheme works well with the square
lattice. In fact, it scales even better with reservoir size than the regular
ESNs used in our experiments. Additionally, we have also included error rates
for a square grid with a sparser input scheme in which only 50\% of the hidden
nodes see the input. These networks marginally improve even further upon the
performance of square lattices.

\begin{table}[htb]
  \centering
  \begin{center}
    \caption{
      Simplicity of the weighting scheme of square grids. Square grid reservoirs
contain a single unique magnitude for input and reservoir weights. Displayed
values are given as an average across 20 experiment runs (std. dev.).
    }
    \label{tab:sq-global-input}
    \begin{tabular}{c c c c c}
      \hline
      \thead{Reservoir type} & \thead{Hidden \\ nodes} & \thead{Unique \\ input weights} & \thead{Unique \\ reservoir weights} & \thead{NARMA-10 \\ NRMSE} \\
      \hline
      \rule{0pt}{2.5ex}Square grid & 100 & 1 & 1 & 0.346 (0.019) \\
      Square grid & 225 & 1 & 1 & 0.245 (0.022) \\
      Square grid & 400 & 1 & 1 & 0.168 (0.009) \\
      \rule{0pt}{3ex}ESN & 100 & 100 & 997 (26) & 0.388 (0.019) \\
      ESN & 225 & 225 & 5098 (74) & 0.282 (0.019) \\
      ESN & 400 & 400 & 16070 (113) & 0.215 (0.021)\rule[-1ex]{0pt}{0pt} \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

Table \ref{tab:sq-global-input} shows the simplicity of the square grids used in
Figure \ref{fig:rt-performance-big}. The input of the entire reservoir is
decided by a single scalar. Furthermore, there is only a single unique magnitude
used for reservoir weights in $\mathbf{W}^{res}$, which is determined by the
spacing of nodes. ESNs shown in Table \ref{tab:sq-global-input} contain a large
amount of unique reservoir weights, but achieve a worse NRMSE on the NARMA-10
benchmark.

Simple cyclic reservoirs (SCR) use topology in a similar manner to use a
deterministic weighting scheme \cite{rodan_minimum_2011}. Units in SCRs are
organized in a cycle, with a single unique weight magnitude. All input
connections have the same absolute weight value, but the sign is determined by
means of an unbiased coin. The intention with SCR is to remove stochasticity in
reservoir generation. With square lattices a similar strategy is
employed. However, we do not use a stochastic scheme of generating signed input
values, but instead generate the flow of information in this manner. This leaves
an opportunity to study the structures that are generated, allowing us to peer
into the apparent black box of the ESN, as we shall see in Section
\ref{sec:shrink-grow}.

The time evolution of internal node activations proves to be of interest. Figure
\ref{fig:activations} depicts an example run, comparing the standard ESN to
square grids with a fixed, global input scheme. It is clearly visible that nodes
in Figure \ref{fig:activations-b} receive the same input, while uniform
distribution of the ESN in \ref{fig:activations-a} seems more sporadic due to
its uniform input distribution in the interval [-0.5, 0.5].

\begin{figure}[htb]
  \centering
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/esn-activations.png}
    \caption{}
    \label{fig:activations-a}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/sq-activations.png}
    \caption{}
    \label{fig:activations-b}
  \end{subfigure}
  \caption{
    Evolution of internal node activations for ESN (a) and square lattice with
global input (b). Reservoirs contain $N = 144$ nodes, but only a subset of 10 is
shown to avoid clutter.
  }
  \label{fig:activations}
\end{figure}

Note also that the activations of nodes in a square lattice reservoir are
strictly positive, as the NARMA input sequence is strictly positive. This may
warrant a change of activation to other sigmoid functions, as half of $tanh$
remains unused, but has not been investigated further.

In summary, our experimental results demonstrate the potential of designing
reservoirs in a non-stochastic manner. By introducing directed edges to
spatially constrained lattice reservoirs, we have stumbled upon reservoirs that
perform exceptionally well on the NARMA-10 benchmark. These results suggest
that, in a physical RC context, physical substrates will show degraded
performance unless there is a directed flow of information. Additionally, we
propose directed lattice reservoirs as a means to explore the impact of
information flow further.

\section{Nonlinear Dynamics in Square Grids}

\subsection{Synopsis}

In this section we look at the potential diversity of nonlinear operations in
square lattice reservoirs. Although the NARMA-10 presents a task of nonlinear
operation, we herein conduct experiments to determine the kernel quality and
generalization capabilities of the square lattice reservoirs, and run benchmarks
with the Mackey-Glass benchmark to generate a mildly chaotic attractor ($\tau =
17$).

\subsection{Results and Discussion}

\begin{figure}[htb]
  \centering
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/esn-rank.png}
    \caption{}
    \label{fig:rank-a}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/sq-rank.png}
    \caption{}
    \label{fig:rank-b}
  \end{subfigure}
  \caption{
    Kernel quality and generalization capability as a function of reservoir size
for ESN (a) and square lattice reservoirs (b).
  }
  \label{fig:rank}
\end{figure}

Consider Figure \ref{fig:rank}, comparing the kernel quality of the default ESN
to square lattice reservoirs. The parameters of the reservoirs remain the same
as in Section \ref{sec:lat-dir-edge}, except a scaling of the ESN spectral
radius to 0.7, as to avoid complete saturation of the respective ranks. We
discover that square lattice reservoirs attain lower kernel
qualities. Additionally the difference between kernel quality and generalization
is also higher for ESN. This difference is often used as a metric of reservoir
quality, as a high kernel quality and low generalization rank is desirable.

If we examine kernel quality (KQ) and generalization rank (G) as a behavior
space $KQ:G$, sweeping parameter spaces will reveal the flexibility of reservoir
types. For all reservoirs, $KQ < N$ and $G < N$, given reservoir size $N$. It is
known that the standard, fully-connected ESN may be tuned to have almost any
$KQ:G$, while lattice reservoirs cover a smaller area of the $KQ:G$ behavior
space, as they are unable to achieve a high $KQ$ \cite{mcquillan_role_2019},
which is a conclusion that is partly reproduced in Figure \ref{fig:rank}.

In Section \ref{ssec:topology-and-spatial-networks}, we noted a discrepancy
between performance predicted by kernel quality and benchmarks used by Rodan and
Tiňo in their work on ring topologies. Ring topologies cover a smaller area of
the $KQ:G$ behavior space than fully-connected ESNs. Nonetheless, cyclic
reservoirs with regular jumps (CRJ) consistently outperform ESNs across multiple
benchmarks \cite{rodan_simple_2012}. We see a similar outcome in our experiments
with square lattices, where reservoirs perform comparably on the benchmark, but
exhibit a lower kernel quality. This strengthens a conclusion that
deterministically constructed reservoirs can perform well, given appropriate
tasks.

\begin{figure}[htb]
  \centering
  \includegraphics[width=3.5in]{figures/mg17.png}
  \caption{
    NRMSE as a function of reservoir size. Note that this benchmark is the
Mackey-Glass chaotic attractor with $\tau = 17$.
  }
  \label{fig:mg17}
\end{figure}

Figure \ref{fig:mg17} shows NRMSE achieved for square lattice reservoirs on the
Mackey-Glass delay differential equation with mild chaos, $\tau = 17$. Again,
see a comparable performance, although in this case the ESN model scales
slightly better. We would like to stress that previous work commonly compares
reservoirs up to a size of $N = 200$, in which case both Figure
\ref{fig:rt-performance-big} and Figure \ref{fig:mg17} show ESN and square
lattice reservoirs to perform equivalently. Thus, from these benchmarks we may
gather that there is potential in lattice reservoirs as simulation models, most
pressingly as a tool for theoretical analysis rather than an ESN competitor.

\section{Shrinking and Growing Square Grids}
\label{sec:shrink-grow}

\subsection{Synopsis}

In Section \ref{sec:lat-dir-edge} we asserted that the directedness of square
lattice reservoirs will allow us to peer into the ``black box'' character of
reservoirs. Traditional reservoir generation is driven by ad-hoc methodology,
resulting in reservoirs which internals are difficult to interpret. Hence,
constructing simpler, more deterministic reservoirs will allow us a clearer view
of where and how input will flow in the network. Lattice reservoirs follow a
straightforward connectivity scheme and are embedded in space, making analysis
easier.

In this section we attempt to gain a deeper understanding into what makes a
square lattice reservoir perform well on the NARMA-10 benchmark. First we remove
nodes from the lattice in an incremental manner, where each iteration removes
the node that results in the lowest increase in error. Then we take the opposite
route, adding nodes along the frontier of the lattice, always adding the node
and directing the edges in the manner causing the largest decrease in
error. Both shrinking and growing square lattice reservoirs thus follow a
simplistic, exhaustive approach.

\subsection{Results and Discussion}

\subsubsection{Shrinking Reservoirs}

\begin{figure}[htb]
  \centering
  \includegraphics[width=3.5in]{figures/removal-hist.png}
  \caption{
    Impact of node removal from a $12 \times 12$ square lattice reservoir. The
original NRMSE of the reservoir is 0.28.
  }
  \label{fig:rt-removal-hist}
\end{figure}

We begin the experiment by creating a single $12 \times 12$ square lattice
reservoir and evaluating the impact of removing singular nodes in individual
copies. Figure \ref{fig:rt-removal-hist} shows the distribution of the impact
the removal of a node has on a reservoir with an original benchmark NRMSE of
0.28. Few node removals make a difference, and the few that do only change the
NRMSE marginally. This speaks to an inherent robustness in the reservoir, as
dead nodes do not degenerate performance entirely. Interestingly, a minority of
nodes provide a decrease in error upon removal, indicating a noisiness that
causes a hinderance.

\begin{figure}[htb]
  \centering
  \includegraphics[width=3.5in]{figures/shrink-performance.png}
  \caption{
    NRMSE as a function of nodes removed from the original reservoir. Both
traditional ESN and square lattice reservoirs are investigated, and compared to
randomly a randomly generated ESN of the same size.
  }
  \label{fig:sq-shrink-performance}
\end{figure}

When node removals have been exhausted, the node causing the smallest increase
in benchmark error is removed. This is then repeated until there is a single
node left in the reservoir. Figure \ref{fig:sq-shrink-performance} details the
development of reservoir error for each iteration. We conduct the same
experiment with default ESNs, comparing both models to randomly generated ESNs
of the same size.

Results show that both square lattice reservoirs and ESN reservoirs benefit from
removal of a few noisy nodes. Furthermore, almost half the reservoirs may be
removed in this manner before performance degrades to values below that of the
initial state. Clearly, numerous of the hidden nodes in the reservoirs that were
generated originally serve little purpose. Removing such nodes result in a
marginal increase in benchmark error, or even a decrease in some cases. Pruning
ESNs has been attempted before. For example, by pruning output connections as
regularization methods \cite{dutoit_pruning_2009}, or by using the concept of
attack tolerance to remove nodes which output weights $\mathbf{W}^{out}$
correspond to large or small values \cite{haluszczynski_reservoir_2020}. Results
that indicate optimization potential in network pruning are interesting in a
physical RC context, as the resources required to realize reservoirs naturally
increases with size.

\begin{figure}[htb]
  \centering
  \begin{subfigure}{.40\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/sq-grid-130.png}
    \caption{$N = 130$, NRMSE = 0.21.}
    \label{fig:sq-grid-130}
  \end{subfigure}
  \begin{subfigure}{.40\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/sq-grid-70.png}
    \caption{$N = 70$, NRMSE = 0.27.}
    \label{fig:sq-grid-70}
  \end{subfigure}
  \vskip\baselineskip

  \begin{subfigure}{.40\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/sq-grid-35.png}
    \caption{$N = 35$, NRMSE = 0.29.}
    \label{fig:sq-grid-35}
  \end{subfigure}
  \begin{subfigure}{.40\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/sq-grid-20.png}
    \caption{$N = 20$, NRMSE = 0.40.}
    \label{fig:sq-grid-20}
  \end{subfigure}
  \caption{
    Evaluation during incremental removal of nodes from a $12 \times 12$ square
lattice reservoir.
  }
  \label{fig:sq-grid}
\end{figure}

Pruning reservoir size is relevant not only to reduce costs, it is also valuable
for theoretical analysis. Figure \ref{fig:sq-grid} depicts a snapshot of
reservoir geometry at four different points during shrinking. Performance
degrades to that of a delay line when there is around $N = 20$ nodes left in the
reservoir, shown Figure \ref{fig:sq-grid-20}. This is also exactly what is
remaining: a delay line of length 11, and two cycles of length 4. Moreover, an
NRMSE of 0.29 achieved with just $N = 35$ nodes in Figure
\ref{fig:sq-grid-35}. We see a ``core'' providing the required short-term
memory, with nodes along this stem for processing purposes. This core is also
present in the original 144-node network, but is now easier to spot.

We have illustrated the value of square lattice reservoirs as an analytical
tool. This rather simple experiment has deduced that heavy lifting to solve the
NARMA-10 benchmark is done by a core stem of memory augmented by surrounding
nodes. A comparable analysis is harder to make for the corresponding shrunk ESN
reservoir from Figure \ref{fig:sq-shrink-performance}, as even just embedding
the network in a metric space requires complicated spring models such as
force-directed graph drawing.

In summary, in this section we have deviated slightly from the original goal of
the thesis relating spatial constraints to physical substrates to investigate
the spatial structures that emerge in lattice reservoirs that solve the NARMA-10
benchmark. We suggest the analysis herein to be only the tip of the iceberg, as
heuristics such as average node degree, investigation of the cyclic structures,
and adding skip edges to square lattices to improve our understanding of the ESN
black box.

\subsubsection{Growing Reservoirs}

Growing reservoirs in the same manner as shrinking them is an enticing
approach. There is a finite amount of positions where new nodes may be inserted
into the network, and for each such position there is only a handful of ways to
direct the incident edges. By exhausting all possibilities, we may attach the
node that causes the largest decrease in benchmark error.

\begin{figure}[htb]
  \centering
  \includegraphics[width=3.5in]{figures/grow-performance.png}
  \caption{
    NRMSE as a function of hidden nodes when growing a square lattice
reservoir. The process is begun from a reservoir of size $N = 74$, depicted in
Figure \protect\ref{fig:sq-grid-grow-74}.
  }
  \label{fig:sq-grow-performance}
\end{figure}

Figure \ref{fig:sq-grow-performance} shows the NRMSE development of a reservoir
grown with this method. We begin with a node of $N = 74$ hidden nodes, found to
be the best performing reservoir when shrinking a $12 \times 12$ square lattice
reservoir in the previous section. We see a steady decrease in benchmark error
for each node addition. Achieving a great benchmark score is of course
desirable, as the results clearly show that building reservoirs specialized for
specific tasks will outperform randomly generated ones.

However, as previously mentioned, our intention with square lattice reservoirs
is not to provide yet another ESN competitor, but to gain insights into their
inner workings. Figure \ref{fig:sq-grid-grow} depicts the reservoir at three
different points during its growth in Figure
\ref{fig:sq-grow-performance}. Curiously, holes appear in the lattice in Figure
\ref{fig:sq-grid-grow-250}, demonstrating that there are parts of the lattice
where adding nodes, irrelevant of the directions of its incident edges, will
cause little performance gain.

Figure \ref{fig:sq-grid-grow-250} also illustrates the apparent difficulty of
analyzing networks without good heuristics. Consider for example the difference
between Figure \ref{fig:sq-grid-grow-250}, and the much smaller Figure
\ref{fig:sq-grid-35}. The latter provides a much clearer picture of what it is
\textit{doing} during the NARMA-10 benchmark. We thus refer to the approaches
suggested in the summary of the previous section on shrinking reservoirs --
there seems to be tremendous potential in researching the creation of
structured, deterministic reservoirs.

\begin{figure}[htb]
  \centering
  \begin{subfigure}{1.0\textwidth} \centering
\includegraphics[width=0.5\linewidth]{figures/sq-grid-grow-74.png}
    \caption{$N = 74$, NRMSE = 0.26.}
    \label{fig:sq-grid-grow-74}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth} \centering
\includegraphics[width=1.0\linewidth]{figures/sq-grid-grow-124.png}
    \caption{$N = 124$, NRMSE = 0.18.}
    \label{fig:sq-grid-grow-124}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth} \centering
\includegraphics[width=1.0\linewidth]{figures/sq-grid-grow-250.png}
    \caption{$N = 250$, NRMSE = 0.15.}
    \label{fig:sq-grid-grow-250}
  \end{subfigure}
  \caption{
    Evaluation during incremental addition of nodes to a square lattice
reservoir. The process starts from the reservoir in (a), resulting in a decrease
in benchmark error according to Figure \protect\ref{fig:sq-grow-performance}.
  }
  \label{fig:sq-grid-grow}
\end{figure}

\section{Restoring Bidirectional Edges}

\subsection{Synopsis}

Among the discoveries of our previous experiments, the importance of directed
edges to create a flow of information is of major relevancy to physical RC
settings. In this section we analyze the effect of restoring bidirectional edges
by following the greedy approach in Section \ref{sec:shrink-grow}. A base $12
\times 12$ square lattice reservoir is generated, and its 264 edges are
incrementally made bidirectional by choosing the edge causing the least increase
in error for each iteration.

\subsection{Results and Discussion}

\begin{figure}[htb]
  \centering
  \includegraphics[width=3.5in]{figures/undir-performance.png}
  \caption{
    NRMSE as a function of the fraction of edges of a square lattice reservoir
that are changed to be undirected. A $12 \times 12$ lattice is used, starting
from 264 directed edges.
  }
  \label{fig:undirection-performance}
\end{figure}

Results are shown in Figure \ref{fig:undirection-performance}. First, there is a
small dip in error for the initial 20 or so edges. When an edge goes from
directed to undirected, its entry in $\mathbf{W}^{out}$ is in practice mirrored
along the diagonal axis, essentially \textit{adding} an incident edge in the
opposite direction. When greedily choosing which edges to change, it is
unsurprising that there are instances where an additional edge is beneficial.

As an increasing fraction of edges made undirected, the reservoir benchmark
error rises steadily. Performance degrades drastically after about 90\% of edges
have been changed. Again we see that a non-symmetric weight matrix enables
richer dynamics, here as a direct consequence of the restored symmetry.

\section{Conclusions}

The goal of this chapter was to investigate the computational feasibility of
lattice models as reservoirs. We have investigated how the fixed geometry can
cause degradation in reservoir quality, and evaluated methods to
deterministically construct spatially restricted reservoirs.

Analysis of bidirectional lattice reservoirs revealed that there is little
difference between square, hexagonal and triangular tilings. The marginal
difference between the different tilings again suggest that the \textit{overall
structure} of the reservoir determines its feasibility, as the difference
between the tilings account for little more than the number of edges incident
each node. This simple, sparse lattice architecture was shown to provide decent
results overall.

When edges of the lattice were changed to be directed, reservoir quality
increased to values almost comparable to traditional ESNs. Changing the input
scheme to a fixed, global input saw an additional gain in performance,
outperforming traditional ESNs on the NARMA-10 benchmark. The importance of
directedness found in Chapter \ref{ch:rgg} thus resurfaced in lattice
experiments, strengthening previous conclusions.

Interestingly, there is a discrepancy between the reservoir quality predicted by
the kernel quality, and the resulting benchmark evaluation. We found that
lattice reservoirs in general may attain a narrower range of kernel qualities
than ESNs, but perform comparably on benchmark tasks. Equivalent conclusions
have been drawn for ring topologies previously, illustrating that
deterministically constructed reservoirs may perform well on suitable tasks.

By deterministically removing and adding nodes to lattice reservoirs, we
illustrated methodology to explore their inner workings. For example, for the
NARMA-10 benchmark we found that lattice reservoirs seem work by augmenting a
``core'' stem of nodes which make up the short-term memory, with augmentative
nodes around it. We reasoned that constructing reservoirs in deterministic ways
may pave the way to a deeper understanding of ESN internals.

Hence, we argue that there are three main contributions in this chapter. (i) We
found physical reservoirs with lattice structures to be feasible, but may
require imposed directedness for scalability. (ii) We presented a lattice model
suitable for theoretical analysis of ESN internals. (iii) We showed example
analyses, suggesting further methods and heuristics.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End:
