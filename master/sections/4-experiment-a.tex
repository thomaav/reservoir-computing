\chapter{Experiments: Random Geometric Graphs}
\label{ch:rgg}

% (TODO): t!
\begin{figure}[t]
  \centering
  \includegraphics[width=3.5in]{figures/RGG-example.png}
  \caption{
    Example vertices drawn to generate a random geometric graph embedded in a
two-dimensional Euclidean space. The neighborhood radius $r$ of a single node is
shown.
  }
  \label{fig:rgg-example}
\end{figure}

The first idea that springs to mind when considering spatially constrained
graphs, is of course to place the vertices in some metric space. When
investigating spatial reservoirs, it is thus only natural that we begin with the
simplest spatial network model -- the random geometric graph (RGG). Figure
\ref{fig:rgg-example} depicts an example graph.

To construct a reservoir based on the RGG model, we place its $N$ internal nodes
randomly in an underlying metric space $[0, l)^{dim}$, giving each node a
position attribute sampled from a uniform distribution. For any given pair of
nodes $x, y$, we consider the Euclidean distance between them

\begin{equation}
  d(x, y) = \norm{x-y}_{2} = \sqrt{\sum_{i=1}^{dim}{(x_{i}-y_{i})^2}}.
  \label{eq:rgg-dist}
\end{equation}

Nodes are connected only to the nodes within their neighborhood radius $r$,
i.e. where $d < r$. However, since we are interested in the behavior in physical
materials, we set $r = \infty$ to allow full connectivity in all
experiments. Thus, edges are weighted according to several distance functions,
$1/d$, $1/d^2$, and $1/d^3$, to model how the interaction strength diminishes
with distance in many physical substrates. For example, the spins in artificial
spin ice is subject to a magnetic field from all neighboring spins that
diminishes according to $1/d^3$ \cite{jensen_computation_2018}. A dimensionality
of 3, i.e. $[0, l)^{3}$, is used.

\section{Size of the Underlying Volume}

\subsection{Synopsis}

Our first experiment is concerned with the size of the metric space in which we
embed the reservoir, the $l$ of $[0, l)^3$. The behavior of the ESN created will
obviously be affected by the magnitude of the weights of the network edges. A
space that is too small, will result in weights that are too big, and vice
versa.

\subsection{Results and Discussion}

% (TODO): t!
\begin{figure*}[t!]
  \centering
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/RGG-volume-size-inv.png}
    \caption{}
    \label{fig:size-graph-volume-a}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/RGG-volume-size-inv-squared.png}
    \caption{}
    \label{fig:size-graph-volume-b}
  \end{subfigure}
  \caption{
    Spectral radius and performance of generated random geometric graphs of size
$N = 100$, as a function of the size of the underlying volume. Illustrated is
node coupling using two different distance functions: $1/d$ (a) and $1/d^2$ (b).
  }
  \label{fig:size-graph-volume}
\end{figure*}

The results of tuning the underlying volume size $l$ is shown in Figure
\ref{fig:size-graph-volume}. Shown are the results for the distance functions
$1/d$ and $1/d^2$. The plots include both the NRMSE of the network on the
NARMA-10 benchmark, and the resulting spectral radius of its internal reservoir
matrix $\mathbf{W}^{res}$ for the same network.

First, we see that an underlying volume of size $l = 1$ is near useless. The
weights generated by the $1/d$ and $1/d^2$ distance functions are too big, as
expected. The consequence of big weights is that the spectral radius of the
$\mathbf{W}^{res}$ grows too large as well. In fact, the $tanh$ transfer
function of the internal nodes stays completely saturated throughout the entire
benchmark run.

Next, we see that as the spectral radius of $\mathbf{W}^{res}$ decreases, so
does the benchmark error. It is well-known that a spectral radius that is too
small will cause ordered or fixed-point behavior, and one that is too big may
cause unbounded deviation from initial states. Hence, the valleys seen in Figure
\ref{fig:size-graph-volume}, are equivalent to those seen in early explorations
of the spectral radius property in reservoirs
\cite{verstraeten_experimental_2007}. However, in our case, the spectral radius
is implicitly scaled by stretching the size of the underlying volume in which
the graph is embedded.

Knowledge that stretching the underlying space is equivalent to scaling them
spectral radius of the reservoir is of key interest. In physical contexts, this
means that we can scale the degree of dynamical richness in the reservoir by
moving nodes, or otherwise strengthening or lessening their connectivity. This
conclusion seems obvious, but is of importance for following experiments, as we
can directly scale the spectral radius $\mathbf{W}^{res}$ with confidence that
we could do the same by scaling the metric space.

Performance-wise, spatially constraining ESN reservoirs has caused an increase
in error compared to the non-constrained approach. The best reservoirs, seen at
the bottom of the valley in Figure \ref{fig:size-graph-volume-b}, achieve an
NRMSE around 0.5, with a spectral radius $\boldsymbol{\rho}$ around 0.8. This is
worse than what is achieved using a shift register with perfect memory. There is
thus an indication that spatial restrictions do cause a performance penalty, and
figuring out what is causing this is a main theme of the following sections.

\section{Choice of Distance Function}

\subsection{Synopsis}

\textcolor{red}{
  Different systems may exhibit different characteristics. For example flatspin
uses couplings of $1/d^3$. Not necessarily directly comparable, but it is an
interesting problem from a theoretical standpoint.
}

\textcolor{red}{
  In this section we compare the distance functions $1/d$, $1/d^2$ and $1/d^3$,
and also see how they compare to the standard echo state network. We also look a
little deeper into why these functions seem to be unable to reach the level of
the ESN by looking at short-term memory capacity.
}

\textcolor{red}{
  \textbf{Methodology}: Here we are using what we figured out in the previous
section, and scale the spectral radius directly. We set the spectral radius to
0.9, and use the same input scheme again, which is uniform sampling in the
interval [-0.5, 0.5].
}

\subsection{Results and Discussion}

% (TODO): t!
\begin{figure*}[t]
  \centering
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/RGG-dist-performance-mean.png}
    \caption{}
    \label{fig:dist-performance-a}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/RGG-dist-performance-min.png}
    \caption{}
    \label{fig:dist-performance-b}
  \end{subfigure}
  \caption{
    NRMSE on the NARMA-10 task with use of different distance functions to
generate connection weights. Plots shown are the mean (a) and minimum (b) error
aggregations over 10 runs per individual parameter setup. Distance functions are
compared to the standard echo state network.
  }
  \label{fig:dist-performance}
\end{figure*}

% (TODO): Consider 3d plots for hidden nodes.
% (TODO): t!
\begin{figure*}[t]
  \centering
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/RGG-dist-mc.png}
    \caption{}
    \label{fig:dist-performance-is-a}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/RGG-dist-performance-is.png}
    \caption{}
    \label{fig:dist-performance-is-b}
  \end{subfigure}
  \caption{
    Effect of input scaling on reservoirs generated as random geometric
graphs. There is a correlation between the short-term memory capacity of the
reservoirs (a), and error rates for the NARMA-10 task (b). The distance function
used is $1/d^2$.
  }
  \label{fig:dist-performance-is}
\end{figure*}

\textcolor{red}{
  We see that all distance functions are quite far away from the echo state
network, as was established earlier. We also notice that they seem to be quite
unstable compared to the ESN in generation, especially $1/d^3$ has a bad mean
over the runs, but a min that is on par with $1/d^2$. Why is not immediately
clear, but will be resolved in the next section with additions of signed and
directed edges.
}

\textcolor{red}{
  We see that all the distance functions see a significant improvement from
increasing the size of the reservoir, but not as great as that of the echo state
network. Note that this is without exhausting the parameters that are usually
swept: input scaling and spectral radius. In a further experiment, we look at
why these networks are significantly worse than ESNs by choosing the $1/d^2$
distance function, and looking and something that intuitively seems to be the
cause: short-term memory, which is shown in Figure
\ref{fig:dist-performance-is}.
}

\textcolor{red}{
  A thought that may arise when looking at this, is that the random geometric
graph reservoirs will never reach the performance of a delay line, even with
bigger reservoirs. This indicates that their memory capacity is perhaps not up
to par, which is further illustrated in Figure
\ref{fig:dist-performance-is}. The setup for this experiment was 10 runs, where
the short-term memory is the max value, and the NRMSE is the min value for the
reservoirs generated.
}

\textcolor{red}{
  Thus, we see that we may increase the performance of RGG reservoirs by using
the commonly swept input scaling parameter to exchange nonlinear dynamics for
memory. Reservoirs seem to get significantly better once we reach the required
memory capacity of 10, but is still slightly behind the echo state network when
comparing e.g. the 80 node network in Figure \ref{fig:dist-performance-is-b} to
that of the echo state network of size 80 nodes in Figure
\ref{fig:dist-performance}
}

\textcolor{red}{
  We see that some distance functions seem to perform better, namely $1/d^2$
seems to be both the best performing, while also being quite stable. We also see
that the default performance of the random geometric graphs for $1/d^2$ suffer
from a low short-term memory capacity, which, in the case of NARMA-10, may be
remedied by lowering the input scaling to wash older inputs out slower.
}

\section{Restoring Echo State Network Performance}

\subsection{Synopsis}

\textcolor{red}{
  We choose the distance function for $1/d$ when we move on, as this was found
to give more stable results, as the weights become so small with the other
distance functions. Similar results were seen with $1/d^2$ as well, but less
pronounced.
}

\textcolor{red}{
  \textit{Why} we are doing this deviates quite a bit from the goal of being
related to physical reservoir computing, as this is not something one can do
willy-nilly in such cases. However, this is a measure to gain some insight into
what has happened, such that we can reason about why this physical restriction
is an issue.
}

\subsection{Results and Discussion}

% (TODO): t!
\begin{figure*}[t]
  \centering
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/perf-rest-undir.png}
    \caption{}
    \label{fig:perf-restore-a}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/perf-rest-dir.png}
    \caption{}
    \label{fig:perf-restore-b}
  \end{subfigure}
  \caption{
    \textcolor{red}{
      Adding signed weights and directedness. Undirected (a) and directed
(b). Directed means all nodes are directed, 50/50 which direction.
    }
  }
  \label{fig:perf-restore}
\end{figure*}

\textcolor{red}{
  Figure \ref{fig:perf-restore} shows the importance of adding signedness and
directedness.
}

\textcolor{blue}{
  Dale et al. investigates this a bit in their paper: ``It then becomes clear
that how weights are structured and directed, controlling information flow, has
a greater affect on quality of the network. This supports similar results using
hierarchical networks, where structure and number of parameters also
significantly impact performance [6].''
}

% (TODO): t!
\begin{figure}[t]
  \centering
  \includegraphics[width=3.5in]{figures/perf-rest-comp.png}
  \caption{
    \textcolor{red}{
      No caption yet.
    }
  }
  \label{fig:perf-rest-comp}
\end{figure}

\textcolor{red}{
  In Figure \ref{fig:perf-rest-comp} we see that once signedness and
directedness are both introduced together, the performance is close to the same
as that of the ESN.
}

\section{Reservoir Weight Distributions}

\subsection{Synopsis}

Cite common ones in practice from practical applications paper.

\subsection{Results and Discussion}

% (TODO): t!
\begin{figure}[t]
  \centering
  \includegraphics[width=3.5in]{figures/rgg-dist.png}
  \caption{
    \textcolor{red}{
      Weight distribution of a network from the findings in the previous
section. We have removed all zero-elements, as half of the entries will be zero
when directed. The distribution resembles $1/d$, which is the distance function,
and also resembles the reciprocal normal distribution.
    }
  }
  \label{fig:rgg-dist}
\end{figure}

\textcolor{red}{
  Mention that changing from xyz to xy or xyzw does not change the distribution
much, and thus not the reservoir either. So the dimensionality is not really
relevant with this global connectivity where the in-degree will be the same
regardless of dimension.
}

\section{Conclusion}

\textcolor{red}{
  We have investigated imposing the simplest type of spatial restriction onto
echo state networks, and found that they by default will worsen significantly by
default. Care must be taken to ensure that the spacing between nodes will result
in node interactions that induce reservoir dynamics that are suitable for the
task at hand. This is especially important in physical reservoir computing where
the distance between nodes decides their coupling, and it can not be changed
after-the-fact.
}

% (TODO): Look more into this if found _why_ this causes this improvement,
% concretely.
\textcolor{red}{
  Further we investigated why the spatial restrictions worsened the networks
significantly, arriving at some interesting conclusions regarding flow of
information.
}

\textcolor{red}{
  Highlight the important discoveries in this chapter that lead well into the
next chapter. We would like to highlight two key discoveries: \textbf{(1)} the
fact that there are other weighting distribution schemes that work just as well
as uniform and normal, here we have discovered something that resembles the
reciprocal normal distribution using $1/d$, $1/d^2$ etc. And \textbf{(2)} that
reservoirs worsen significantly if there is no inherent flow of information,
which we in this case restored by introducing signedness and directedness to the
reservoir edges. These discoveries are important springboards into the next
chapter.
}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End:
