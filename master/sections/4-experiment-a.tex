\chapter{Experiments: Random Geometric Graphs}
\label{ch:rgg}

\begin{figure}[t]
  \centering
  \begin{subfigure}{.47\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/RGG-example.png}
    \caption{}
    \label{fig:rgg-example-a}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/RGG-example-connected.png}
    \caption{}
    \label{fig:rgg-example-b}
  \end{subfigure}
  \caption{
    (a) Example vertices drawn to generate an RGG embedded in a two-dimensional
Euclidean space. The neighborhood radius $r$ of a single node is shown. (b) RGG
instance with neighborhood radius $r = 0.1$.
  }
  \label{fig:rgg-example}
\end{figure}

The first idea that springs to mind when considering spatially constrained
graphs, is of course to place the vertices in some metric space. When
investigating spatial reservoirs, it is thus only natural that we begin with the
simplest spatial network model -- the random geometric graph (RGG).

To construct a reservoir based on the RGG model, we place its $N$ internal nodes
randomly in an underlying metric space $[0, l)^{dim}$, giving each node a
position attribute sampled from a uniform distribution. An example distribution
of the vertices is shown in Figure \ref{fig:rgg-example-a}. For any given pair
of nodes $x, y$, we consider the Euclidean distance between them

\begin{equation}
  d(x, y) = \norm{x-y}_{2} = \sqrt{\sum_{i=1}^{dim}{(x_{i}-y_{i})^2}}.
  \label{eq:rgg-dist}
\end{equation}

Nodes are connected only to the nodes within their neighborhood radius $r$,
i.e. where $d < r$, depicted in Figure \ref{fig:rgg-example-b}. However, since
we are interested in the behavior in physical materials, we set $r = \infty$ to
allow full connectivity in all experiments. Thus, edges are weighted according
to several distance functions, $1/d$, $1/d^2$, and $1/d^3$, to model how the
interaction strength diminishes with distance in many physical substrates. For
example, the spins in artificial spin ice, presented in Section \ref{ssec:asi},
is subject to a magnetic field from all neighboring spins that diminishes
according to $1/d^3$ \cite{jensen_computation_2018}. Lastly, a dimensionality of
3, i.e. $[0, l)^{3}$, is used.

\section{Size of the Underlying Volume}
\label{sec:volume-size}

\subsection{Synopsis}

Our first experiment is concerned with the size of the metric space in which we
embed the reservoir, the $l$ of $[0, l)^3$. The behavior of the ESN created will
obviously be affected by the magnitude of the weights of the network edges. A
space that is too small will result in weights that are too big, and vice versa.

\subsection{Results and Discussion}

The results of tuning the underlying volume size $l$ is shown in Figure
\ref{fig:size-graph-volume}. Shown are the results for the distance functions
$1/d$ and $1/d^2$, respectively. The plots include both the NRMSE of the network
on the NARMA-10 benchmark, and the resulting spectral radius of its internal
reservoir matrix $\mathbf{W}^{res}$ for the same network.

First, we see that an underlying volume of size $l = 1$ is near useless. The
weights generated by the $1/d$ and $1/d^2$ distance functions are too big, as
expected. The consequence of big weights is that the spectral radius of the
$\mathbf{W}^{res}$ grows too large as well. In fact, the $\tanh$ transfer
function of the internal nodes stays completely saturated throughout the entire
benchmark run.

Next, we see that as the spectral radius of $\mathbf{W}^{res}$ decreases, so
does the benchmark error. It is well-known that a spectral radius that is too
small will cause ordered or fixed-point behavior, and one that is too big may
cause unbounded deviation from initial states. Hence, the valleys seen in Figure
\ref{fig:size-graph-volume}, are equivalent to those seen in early explorations
of the spectral radius property in reservoirs
\cite{verstraeten_experimental_2007}. However, in our case, the spectral radius
is implicitly scaled by stretching the size of the underlying volume in which
the graph is embedded.

\begin{figure}[htb]
  \centering
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/RGG-volume-size-inv.png}
    \caption{}
    \label{fig:size-graph-volume-a}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/RGG-volume-size-inv-squared.png}
    \caption{}
    \label{fig:size-graph-volume-b}
  \end{subfigure}
  \caption{
    Spectral radius and performance of generated random geometric graphs of size
$N = 100$, as a function of the size of the underlying volume. Illustrated is
node coupling using two different distance functions: $1/d$ (a) and $1/d^2$
(b). For the $1/d^2$ (b) distance function the minimum NRMSE value over 20 runs
is used instead of the mean, as it exhibits a higher variability, explained
further in Chapter
\ref{sec:dist-func}.
  }
  \label{fig:size-graph-volume}
\end{figure}

Knowledge that stretching the underlying space is equivalent to scaling the
spectral radius of the reservoir is of key interest. In physical contexts, this
means that we can scale the degree of dynamical richness in the reservoir by
moving nodes, or otherwise strengthening or lessening their connectivity. This
conclusion seems obvious, but is of importance for following experiments, as we
can directly scale the spectral radius $\mathbf{W}^{res}$ with confidence that
we could do the same by scaling the metric space.

Performance-wise, spatially constraining ESN reservoirs has caused an increase
in error compared to the non-constrained approach. The best reservoirs, seen at
the bottom of the valley in Figure \ref{fig:size-graph-volume-b}, achieve an
NRMSE around 0.5, with a spectral radius $\boldsymbol{\rho}$ around 0.8. This is
worse than what is achieved using a shift register with perfect memory. There is
thus an indication that spatial restrictions do cause a performance penalty, and
figuring out what is causing this is a main theme of the following sections.

\section{Distance Functions and Memory Capacity}
\label{sec:dist-func}

\subsection{Synopsis}

Before exploring changes to improve upon our RGG model, it is reasonable to
examine the differences between the distance functions we have available, $1/d$,
$1/d^2$, and $1/d^3$. The increasing degree of the inverse of the distance
essentially acts to reduce the neighborhood size $r$. For example, with a
distance function of $1/d^3$, nodes that are distant will have little or no
impact on each other.

Using the knowledge we gained in Section \ref{sec:volume-size}, we now also
directly scale the spectral radius of $\mathbf{W}^{res}$ to 0.9 by multiplying
it by the appropriate scalar.

\subsection{Results and Discussion}

\begin{figure}[htb]
  \centering
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/RGG-dist-performance-mean.png}
    \caption{}
    \label{fig:dist-performance-a}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/RGG-dist-performance-min.png}
    \caption{}
    \label{fig:dist-performance-b}
  \end{subfigure}
  \caption{
    NRMSE on the NARMA-10 task with use of different distance functions to
generate connection weights. Plots shown are the mean (a) and minimum (b) error
aggregations over 20 runs per individual parameter setup. Distance functions are
compared to the standard echo state network.
  }
  \label{fig:dist-performance}
\end{figure}

Figure \ref{fig:dist-performance} compares the performance of the distance
functions to that of default ESNs. Plotted is NARMA-10 NRMSE as a function of
reservoir size. The average over 20 runs, shown in Figure
\ref{fig:dist-performance-a}, shows little noticeable decrease in error for the
distance functions as reservoir size increases. Additionally, it is clear that
there is some degree of variability in the error when using the $1/d^2$ and
$1/d^3$ functions. This notion is strengthened by Figure
\ref{fig:dist-performance-b}, where we see best performances for $1/d$ is about
the same as the mean, while $1/d^2$ and $1/d^3$ show a slight decrease in error
with reservoir size.

\begin{figure}[htb]
  \centering
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/RGG-dist-mc.png}
    \caption{}
    \label{fig:dist-performance-is-a}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/RGG-dist-performance-is.png}
    \caption{}
    \label{fig:dist-performance-is-b}
  \end{subfigure}
  \caption{
    Effect of input scaling on reservoirs generated as random geometric graphs,
size $N = 80$. There is a correlation between the short-term memory capacity of
the reservoirs (a), and error rates for the NARMA-10 task (b). The distance
function used is $1/d$.
  }
  \label{fig:dist-performance-is}
\end{figure}

Empirically, we have found that reservoirs that get no significant performance
increase on the NARMA-10 task with an increasing reservoir size, tend to be
limited by memory capacity. The inability of the reservoirs to reach the
performance of a delay line supports this hypothesis. Figure
\ref{fig:dist-performance-is} highlights this, presenting both the short-term
memory capacity and the NARMA-10 NRMSE of reservoirs of size $N = 80$ as a
function of input scaling. As we decrease the input scaling, the memory capacity
increases, and error decreases. This is expected, as it determines how nonlinear
the responses of the reservoir are, and there exists a memory-nonlinearity
trade-off, described in Section \ref{ssec:metrics}.

To summarize, we have found that the average performance of our distance
functions seem to hover around similar values. A general trend is that the RGG
model lacks sufficient memory capacity to solve the NARMA-10 task well,
regardless of the distance function used. Memory retention is improved by
lowering the input scaling. Further exploration can now be done by choosing a
single distance function, as our results suggest that they suffer from problems
of a similar nature.

\section{Restoring Echo State Network Performance}
\label{sec:restore}

\subsection{Synopsis}

Next, we will make changes to the $\mathbf{W}^{res}$ generated with the RGG
model to move its performance close to the ESN model. This may seem
counter-intuitive, as we primarily concern ourselves with physical reservoir
computing. Making arbitrary changes to the model generation is not a procedure
that will necessarily translate well to physical substrates and their
restrictive nature. However, we argue that determining the root cause of the
difference in error seen in Figure \ref{fig:dist-performance} will uncover
important properties of reservoir design. In turn, this improves our fundamental
understanding of the paradigm.

In this section, we therefore introduce signedness and directedness to the edges
of the RGG. The signedness of the reservoir is given by some percentage,
e.g. 10\%, such that each edge has a corresponding chance of becoming
negative. Reservoirs are either directed or undirected, where edges in a
directed reservoir have a 50\% chance of going in either direction. Note that
making an edge negative is not the same as reversing its direction, as it simply
means that a node will weight the value of its neighbor negatively.

For these experiments, we use the $1/d$ distance function. In Section
\ref{sec:dist-func} we found this function to give the most stable
results. Although it does not strictly produce the \textit{best} results, we
found the stability to easier to work with when looking for definitive trends in
performance. Similar results were seen with $1/d^2$, though less pronounced. We
also found lowering the input scaling to improve memory capacity, and
experiments in this section will use an input scaling of 0.1

\subsection{Results and Discussion}

\begin{figure}[htb]
  \centering
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/perf-rest-undir.png}
    \caption{}
    \label{fig:perf-restore-a}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/perf-rest-dir.png}
    \caption{}
    \label{fig:perf-restore-b}
  \end{subfigure}
  \caption{
    Introducing signed weights to RGG reservoirs. Results shown are for
undirected (a) and directed (b) reservoirs.
  }
  \label{fig:perf-restore}
\end{figure}

The impact of introducing signed, directed edges to reservoirs is shown in
Figure \ref{fig:perf-restore}. An increased fraction of signed weights decrease
the benchmark error in all cases, except for very small reservoirs. Moreover,
making the reservoir directed decreases error drastically when the fraction of
negative edges is 0.5. This is particularly true for bigger reservoirs.

Firstly, negative weights discard some of the inherent symmetry of the
network. Although the weight matrix is still symmetric in magnitude, it now
allows for a wider range of node behavior, especially considering that the lower
half of $\tanh$ becomes available. The addition of directed edges makes it clear
that a non-symmetric weight matrix enables richer dynamics, as we move below the
performance of a delay line.

\begin{figure}[htb]
  \centering
  \includegraphics[width=3.5in]{figures/perf-rest-comp.png}
  \caption{
    Comparing RGG reservoirs with signed and directed edges to traditional
ESNs. Both RGG reservoirs have a fraction of negative edges of 0.5.
  }
  \label{fig:perf-rest-comp}
\end{figure}

Figure \ref{fig:perf-rest-comp} compares the performance of these reservoirs
with ESNs. Directed reservoirs with a signed weight fraction of 0.5 perform
comparably to ESNs, and scale similarly with reservoir size. Our interpretation
of this result is that the importance of \textit{flow of information} in
reservoirs should not be understated. It seems that the \textit{structure} of
the reservoir network is crucial, and that \textit{where} information flows is
as important as its magnitude.

A similar conclusion was reached in \cite{mcquillan_role_2019}, where directed
networks were shown to cover a bigger behavior space than their undirected
counterparts.

\section{Reservoir Weight Distribution}
\label{sec:rwd}

\subsection{Synopsis}

By reintroducing signed and directed edges to the RGG model, it goes back to
resembling traditional ESNs. In fact, the major difference between the RGG model
and ESNs was their distributions of internal reservoir weights. In this section
we make a short comparison between the two.

\subsection{Results and Discussion}

\begin{figure}[htb]
  \centering
  \includegraphics[width=3.5in]{figures/rgg-dist.png}
  \caption{
    Weight distribution of a reservoir with both signed and directed
edges. Zero-elements have been removed, as half of the matrix entries contain
zeroes. The distribution follows the $1/d$ distance function, which incidentally
looks similar to the reciprocal normal distribution.
  }
  \label{fig:rgg-dist}
\end{figure}

The distribution of the internal weights of RGG reservoirs with signed and
directed edges is shown in Figure \ref{fig:rgg-dist}. Three key points are worth
mentioning: (i) the distribution resembles the used distance function $1/d$,
(ii) the distribution is symmetric around zero, due to signed edges, and (iii)
elements of zero magnitude have been removed, as half of $\mathbf{W}^{res}$
contain zero-valued entries due to the directedness of the reservoir.

Distributions commonly proposed to provide ESNs with good performance include a
symmetrical uniform distribution, or a normal distribution centered around zero
\cite{montavon_practical_2012}. We end up with a symmetric distribution
resulting from the distance function $1/d$, a distribution resembling the
inverse Gaussian distribution. This distribution has, to our knowledge, not been
used in previous experiments.

Finally, we mention that the dimensionality of the underlying metric space of
the RGG plays a small role in the performance of the resulting
reservoirs. Changing the dimensionality will simply shift the weight
distribution slightly, causing little change in performance.

\section{Conclusions}

The experiments in this chapter demonstrate how inherent structural limitations
may impact computational capacity in reservoirs. By employing RGG reservoirs, we
have illustrated how reservoir computing translates to a simple, spatially
constrained topology, and have reasoned about the degradation in performance.

First, we found that the spacing between nodes in RGGs is vital. We presented a
correlation between the spacing and the spectral radius of the resulting
internal connectivity matrix of the reservoir, suggesting that scaling of the
volume in which the graph is embedded is equivalent to traditional scaling of
spectral radius.

Further, we evaluated multiple distance functions to determine node coupling,
finding that the $1/d$, $1/d^2$, and $1/d^3$ functions perform almost the same,
suggesting that they produce similar structure and weight
distributions. Additionally, we discovered that a major reason for low
performance is low memory retention, which can be remedied by lowering input
strength.

We discovered that RGG performance becomes equivalent to that of ESNs once we
re-introduce signed and directed edges to the reservoir. We interpret this to
indicate that the information flow is a key component in reservoirs.

Moreover, we found that the resulting weight distribution of an RGG with
directed edges is different from the traditional uniform or normal
distributions. We understand this to imply that multiple weight distributions
are suitable, given that the resulting \textit{structure} of the reservoir
network is suitable.

This chapter thus serves as an introductory evaluation of spatially constrained
reservoirs. The main contribution provided is the notion that directedness,
i.e. directed information flow, is a crucial property in quality reservoirs. A
deeper investigation is conducted in the following chapter on lattice
reservoirs.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End:
