
@article{wheeler_vitro_2011,
	title = {In {Vitro} {Microelectrode} {Array} {Technology} and {Neural} {Recordings}},
	volume = {39},
	issn = {0278-940X},
	url = {http://www.dl.begellhouse.com/journals/4b27cbfc562e21b8,7d5e987c7e576915,41cf5d45336340f0.html},
	doi = {10.1615/CritRevBiomedEng.v39.i1.40},
	abstract = {In vitro microelectrode array (MEA) technology has evolved into a widely used and effective methodology to study cultured neural networks. An MEA forms a unique electrical interface with the cultured neurons in that neurons are directly grown on top of the electrode (neuron-on-electrode configuration). Theoretical models and experimental results suggest that physical configuration and biological environments of the cell-electrode interface play a key role in the outcome of neural recordings, such as yield of recordings, signal shape, and signalto-noise ratio. Recent interdisciplinary approaches have shown that MEA performance can be enhanced through novel nanomaterials, structures, surface chemistry, and biotechnology. In vitro and in vivo neural interfaces share some common factors, and in vitro neural interface issues can be extended to solve in vivo neural interface problems of brain-machine interface or neuromodulation techniques.},
	language = {en},
	number = {1},
	urldate = {2019-04-02},
	journal = {Critical Reviews™ in Biomedical Engineering},
	author = {Wheeler, Bruce C. and Nam, Yoonkey},
	year = {2011},
	pages = {45--61},
	file = {Wheeler and Nam - 2011 - In Vitro Microelectrode Array Technology and Neura.pdf:/home/thomaav/Zotero/storage/WMBCZVFD/Wheeler and Nam - 2011 - In Vitro Microelectrode Array Technology and Neura.pdf:application/pdf}
}

@article{heinricher_principles_nodate,
	title = {Principles of {Extracellular} {Single}-{Unit} {Recording}},
	language = {en},
	author = {Heinricher, Mary M},
	pages = {6},
	file = {Heinricher - Principles of Extracellular Single-Unit Recording.pdf:/home/thomaav/Zotero/storage/2YMPBVEP/Heinricher - Principles of Extracellular Single-Unit Recording.pdf:application/pdf}
}

@article{panzeri_neural_2015,
	title = {Neural population coding: combining insights from microscopic and mass signals},
	volume = {19},
	issn = {13646613},
	shorttitle = {Neural population coding},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661315000030},
	doi = {10.1016/j.tics.2015.01.002},
	language = {en},
	number = {3},
	urldate = {2019-04-02},
	journal = {Trends in Cognitive Sciences},
	author = {Panzeri, Stefano and Macke, Jakob H. and Gross, Joachim and Kayser, Christoph},
	month = mar,
	year = {2015},
	pages = {162--172},
	file = {Panzeri et al. - 2015 - Neural population coding combining insights from .pdf:/home/thomaav/Zotero/storage/8R7WVYPJ/Panzeri et al. - 2015 - Neural population coding combining insights from .pdf:application/pdf;Panzeri et al. - 2015 - Neural population coding combining insights from .pdf:/home/thomaav/Zotero/storage/R22U6ETU/Panzeri et al. - 2015 - Neural population coding combining insights from .pdf:application/pdf}
}

@article{brown_multiple_2004,
	title = {Multiple neural spike train data analysis: state-of-the-art and future challenges},
	volume = {7},
	issn = {1097-6256, 1546-1726},
	shorttitle = {Multiple neural spike train data analysis},
	url = {http://www.nature.com/articles/nn1228},
	doi = {10.1038/nn1228},
	language = {en},
	number = {5},
	urldate = {2019-04-02},
	journal = {Nature Neuroscience},
	author = {Brown, Emery N and Kass, Robert E and Mitra, Partha P},
	month = may,
	year = {2004},
	pages = {456--461},
	file = {Brown et al. - 2004 - Multiple neural spike train data analysis state-o.pdf:/home/thomaav/Zotero/storage/Y957DCKQ/Brown et al. - 2004 - Multiple neural spike train data analysis state-o.pdf:application/pdf}
}

@article{zador_critique_2019,
	title = {A {Critique} of {Pure} {Learning}: {What} {Artificial} {Neural} {Networks} can {Learn} from {Animal} {Brains}},
	shorttitle = {A {Critique} of {Pure} {Learning}},
	url = {http://biorxiv.org/lookup/doi/10.1101/582643},
	doi = {10.1101/582643},
	abstract = {Over the last decade, artiﬁcial neural networks (ANNs), have undergone a revolution, catalyzed in large part by better tools for supervised learning. However, training such networks requires enormous data sets of labeled examples, whereas young animals (including humans) typically learn with few or no labeled examples. This stark contrast with biological learning has led many in the ANN community posit that instead of supervised paradigms, animals must rely instead primarily on unsupervised learning, leading the search for better unsupervised algorithms. Here we argue that much of an animal’s behavioral repertoire is not the result of clever learning algorithms—supervised or unsupervised—but arises instead from behavior programs already present at birth. These programs arise through evolution, are encoded in the genome, and emerge as a consequence of wiring up the brain. Speciﬁcally, animals are born with highly structured brain connectivity, which enables them learn very rapidly. Recognizing the importance of the highly structured connectivity suggests a path toward building ANNs capable of rapid learning.},
	language = {en},
	urldate = {2019-04-02},
	journal = {bioRxiv},
	author = {Zador, Anthony M.},
	month = mar,
	year = {2019},
	file = {Zador - 2019 - A Critique of Pure Learning What Artificial Neura.pdf:/home/thomaav/Zotero/storage/LKM3VAWT/Zador - 2019 - A Critique of Pure Learning What Artificial Neura.pdf:application/pdf}
}

@article{bertschinger_real-time_2004,
	title = {Real-{Time} {Computation} at the {Edge} of {Chaos} in {Recurrent} {Neural} {Networks}},
	volume = {16},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/089976604323057443},
	doi = {10.1162/089976604323057443},
	abstract = {Depending on the connectivity recurrent networks of simple computational units can show very different types of dynamics ranging from totally ordered to chaotic. We analyze how the type of dynamics (ordered or chaotic) exhibited by randomly connected networks of threshold gates driven by a time varying input signal depends on the parameters describing the distribution of the connectivity matrix. In particular we calculate the critical boundary in parameter space where the transition from ordered to chaotic dynamics takes places. Employing a recently developed framework for analyzing real-time computations we show that only near the critical boundary such networks can perform complex computations on time series. Hence, this result strongly supports conjectures that dynamical systems which are capable of doing complex computational tasks should operate near the edge of chaos, i.e. the transition from ordered to chaotic dynamics.},
	language = {en},
	number = {7},
	urldate = {2019-04-01},
	journal = {Neural Computation},
	author = {Bertschinger, Nils and Natschläger, Thomas},
	month = jul,
	year = {2004},
	keywords = {Read, Printed},
	pages = {1413--1436},
	file = {Bertschinger and Natschläger - 2004 - Real-Time Computation at the Edge of Chaos in Recu.pdf:/home/thomaav/Zotero/storage/8GVX8EMD/Bertschinger and Natschläger - 2004 - Real-Time Computation at the Edge of Chaos in Recu.pdf:application/pdf}
}

@article{mnih_playing_2013,
	title = {Playing {Atari} with {Deep} {Reinforcement} {Learning}},
	abstract = {We present the ﬁrst deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We ﬁnd that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	language = {en},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	year = {2013},
	pages = {9},
	file = {Mnih et al. - Playing Atari with Deep Reinforcement Learning.pdf:/home/thomaav/Zotero/storage/K6TDJQ2K/Mnih et al. - Playing Atari with Deep Reinforcement Learning.pdf:application/pdf}
}

@inproceedings{verstraeten_memory_2010,
	address = {Barcelona, Spain},
	title = {Memory versus non-linearity in reservoirs},
	isbn = {978-1-4244-6916-1},
	url = {http://ieeexplore.ieee.org/document/5596492/},
	doi = {10.1109/IJCNN.2010.5596492},
	abstract = {Reservoir Computing (RC) is increasingly being used as a conceptually simple yet powerful method for using the temporal processing of recurrent neural networks (RNN). However, because fundamental insight in the exact functionality of the reservoir is as yet still lacking, in practice there is still a lot of manual parameter tweaking or brute-force searching involved in optimizing these systems. In this contribution we aim to enhance the insights into reservoir operation, by experimentally studying the interplay of the two crucial reservoir properties, memory and non-linear mapping. For this, we introduce a novel metric which measures the deviation of the reservoir from a linear regime and use it to deﬁne different regions of dynamical behaviour. Next, we study the relationship of two important reservoir parameters, input scaling and spectral radius, on two properties of an artiﬁcial task, namely memory and nonlinearity.},
	language = {en},
	urldate = {2019-03-04},
	booktitle = {The 2010 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Verstraeten, David and Dambre, Joni and Dutoit, Xavier and Schrauwen, Benjamin},
	month = jul,
	year = {2010},
	keywords = {Favorite, Read, Printed},
	pages = {1--8},
	file = {Verstraeten et al. - 2010 - Memory versus non-linearity in reservoirs.pdf:/home/thomaav/Zotero/storage/WH2MXY4N/Verstraeten et al. - 2010 - Memory versus non-linearity in reservoirs.pdf:application/pdf}
}

@article{legenstein_what_2005,
	title = {What makes a dynamical system computationally powerful?},
	language = {en},
	author = {Legenstein, Robert and Maass, Wolfgang},
	year = {2005},
	keywords = {Printed},
	pages = {31},
	file = {Legenstein and Maass - What makes a dynamical system computationally powe.pdf:/home/thomaav/Zotero/storage/AGLP723Z/Legenstein and Maass - What makes a dynamical system computationally powe.pdf:application/pdf}
}

@article{lee_yass:_2017,
	title = {{YASS}: {Yet} {Another} {Spike} {Sorter}},
	shorttitle = {{YASS}},
	url = {http://biorxiv.org/lookup/doi/10.1101/151928},
	doi = {10.1101/151928},
	abstract = {Spike sorting is a critical ﬁrst step in extracting neural signals from large-scale electrophysiological data. This manuscript describes an efﬁcient, reliable pipeline for spike sorting on dense multi-electrode arrays (MEAs), where neural signals appear across many electrodes and spike sorting currently represents a major computational bottleneck. We present several new techniques that make dense MEA spike sorting more robust and scalable. Our pipeline is based on an efﬁcient multistage “triage-then-cluster-then-pursuit” approach that initially extracts only clean, high-quality waveforms from the electrophysiological time series by temporarily skipping noisy or “collided” events (representing two neurons ﬁring synchronously). This is accomplished by developing a neural network detection method followed by efﬁcient outlier triaging. The clean waveforms are then used to infer the set of neural spike waveform templates through nonparametric Bayesian clustering. Our clustering approach adapts a “coreset” approach for data reduction and uses efﬁcient inference methods in a Dirichlet process mixture model framework to dramatically improve the scalability and reliability of the entire pipeline. The “triaged” waveforms are then ﬁnally recovered with matching-pursuit deconvolution techniques. The proposed methods improve on the state-of-the-art in terms of accuracy and stability on both real and biophysically-realistic simulated MEA data. Furthermore, the proposed pipeline is efﬁcient, learning templates and clustering much faster than real-time for a 500-electrode dataset, using primarily a single CPU core.},
	language = {en},
	urldate = {2019-03-07},
	journal = {bioRxiv},
	author = {Lee, JinHyung and Carlson, David and Shokri, Hooshmand and Yao, Weichi and Goetz, Georges and Hagen, Espen and Batty, Eleanor and Chichilnisky, EJ and Einevoll, Gaute and Paninski, Liam},
	month = jun,
	year = {2017},
	file = {Lee et al. - 2017 - YASS Yet Another Spike Sorter.pdf:/home/thomaav/Zotero/storage/83P22WT6/Lee et al. - 2017 - YASS Yet Another Spike Sorter.pdf:application/pdf}
}

@article{nichele_deep_2017,
	title = {Deep {Reservoir} {Computing} {Using} {Cellular} {Automata}},
	url = {http://arxiv.org/abs/1703.02806},
	abstract = {Recurrent Neural Networks (RNNs) have been a prominent concept within artiﬁcial intelligence. They are inspired by Biological Neural Networks (BNNs) and provide an intuitive and abstract representation of how BNNs work. Derived from the more generic Artiﬁcial Neural Networks (ANNs), the recurrent ones are meant to be used for temporal tasks, such as speech recognition, because they are capable of memorizing historic input. However, such networks are very time consuming to train as a result of their inherent nature. Recently, Echo State Networks and Liquid State Machines have been proposed as possible RNN alternatives, under the name of Reservoir Computing (RC). RCs are far more easy to train.},
	language = {en},
	urldate = {2019-03-05},
	journal = {arXiv:1703.02806 [cs]},
	author = {Nichele, Stefano and Molund, Andreas},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.02806},
	keywords = {Computer Science - Emerging Technologies, Computer Science - Neural and Evolutionary Computing, Printed},
	file = {Nichele and Molund - 2017 - Deep Reservoir Computing Using Cellular Automata.pdf:/home/thomaav/Zotero/storage/S83URC95/Nichele and Molund - 2017 - Deep Reservoir Computing Using Cellular Automata.pdf:application/pdf}
}

@incollection{alippi_quantification_2009,
	address = {Berlin, Heidelberg},
	title = {On the {Quantification} of {Dynamics} in {Reservoir} {Computing}},
	volume = {5768},
	isbn = {978-3-642-04273-7 978-3-642-04274-4},
	url = {http://link.springer.com/10.1007/978-3-642-04274-4_101},
	abstract = {Reservoir Computing (RC) oﬀers a computationally eﬃcient and well performing technique for using the temporal processing power of Recurrent Neural Networks (RNNs), while avoiding the traditional long training times and stability problems. The method is both simple and elegant: a random RNN (called the reservoir) is constructed using only a few global parameters to tune the dynamics into a desirable regime, and the dynamic response of the reservoir is used to train a simple linear regression function called the readout function - the reservoir itself remains untrained. This technique has shown some experimentally very convincing results on a variety of tasks, but a thorough understanding of the importance of the dynamics for the performance is still lacking. This contribution aims to extend this understanding, by presenting a more sophisticated extension on the traditional way of characterizing the reservoir dynamics, by using the dynamic proﬁle of the Jacobian of the reservoir instead of static, a priori measures such as the standard spectral radius. We show that this measure gives a more accurate description of the reservoir dynamics, and can serve as predictor for the performance. Additionally, due to the theoretical background from dynamical systems theory, this measure oﬀers some insight into the underlying mechanisms of RC.},
	language = {en},
	urldate = {2019-03-05},
	booktitle = {Artificial {Neural} {Networks} – {ICANN} 2009},
	publisher = {Springer Berlin Heidelberg},
	author = {Verstraeten, David and Schrauwen, Benjamin},
	editor = {Alippi, Cesare and Polycarpou, Marios and Panayiotou, Christos and Ellinas, Georgios},
	year = {2009},
	doi = {10.1007/978-3-642-04274-4_101},
	keywords = {Favorite, Read, Printed},
	pages = {985--994},
	file = {Verstraeten and Schrauwen - 2009 - On the Quantification of Dynamics in Reservoir Com.pdf:/home/thomaav/Zotero/storage/6VLW4AB4/Verstraeten and Schrauwen - 2009 - On the Quantification of Dynamics in Reservoir Com.pdf:application/pdf}
}

@article{michiels_van_kessenich_synaptic_2016,
	title = {Synaptic plasticity and neuronal refractory time cause scaling behaviour of neuronal avalanches},
	volume = {6},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/srep32071},
	doi = {10.1038/srep32071},
	language = {en},
	number = {1},
	urldate = {2019-03-05},
	journal = {Scientific Reports},
	author = {Michiels van Kessenich, L. and de Arcangelis, L. and Herrmann, H. J.},
	month = oct,
	year = {2016},
	keywords = {Read, Printed},
	file = {Michiels van Kessenich et al. - 2016 - Synaptic plasticity and neuronal refractory time c.pdf:/home/thomaav/Zotero/storage/UKCS97I2/Michiels van Kessenich et al. - 2016 - Synaptic plasticity and neuronal refractory time c.pdf:application/pdf}
}

@article{sompolinsky_computational_2014,
	title = {Computational neuroscience: beyond the local circuit},
	volume = {25},
	issn = {09594388},
	shorttitle = {Computational neuroscience},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0959438814000373},
	doi = {10.1016/j.conb.2014.02.002},
	language = {en},
	urldate = {2019-03-05},
	journal = {Current Opinion in Neurobiology},
	author = {Sompolinsky, Haim},
	month = apr,
	year = {2014},
	pages = {xiii--xviii},
	file = {Sompolinsky - 2014 - Computational neuroscience beyond the local circu.pdf:/home/thomaav/Zotero/storage/H3DZS7SC/Sompolinsky - 2014 - Computational neuroscience beyond the local circu.pdf:application/pdf}
}

@article{maass_searching_2016,
	title = {Searching for principles of brain computation},
	volume = {11},
	issn = {23521546},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S235215461630119X},
	doi = {10.1016/j.cobeha.2016.06.003},
	language = {en},
	urldate = {2019-03-05},
	journal = {Current Opinion in Behavioral Sciences},
	author = {Maass, Wolfgang},
	month = oct,
	year = {2016},
	keywords = {Read, Printed},
	pages = {81--92},
	file = {Maass - 2016 - Searching for principles of brain computation.pdf:/home/thomaav/Zotero/storage/U8SRT6H4/Maass - 2016 - Searching for principles of brain computation.pdf:application/pdf}
}

@article{rey_past_2015,
	title = {Past, present and future of spike sorting techniques},
	volume = {119},
	issn = {03619230},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0361923015000684},
	doi = {10.1016/j.brainresbull.2015.04.007},
	abstract = {Spike sorting is a crucial step to extract information from extracellular recordings. With new recording opportunities provided by the development of new electrodes that allow monitoring hundreds of neurons simultaneously, the scenario for the new generation of algorithms is both exciting and challenging. However, this will require a new approach to the problem and the development of a common reference framework to quickly assess the performance of new algorithms. In this work, we review the basic concepts of spike sorting, including the requirements for different applications, together with the problems faced by presently available algorithms. We conclude by proposing a roadmap stressing the crucial points to be addressed to support the neuroscientiﬁc research of the near future.},
	language = {en},
	urldate = {2019-03-05},
	journal = {Brain Research Bulletin},
	author = {Rey, Hernan Gonzalo and Pedreira, Carlos and Quian Quiroga, Rodrigo},
	month = oct,
	year = {2015},
	keywords = {Read, Printed},
	pages = {106--117},
	file = {Rey et al. - 2015 - Past, present and future of spike sorting techniqu.pdf:/home/thomaav/Zotero/storage/I5RTPI58/Rey et al. - 2015 - Past, present and future of spike sorting techniqu.pdf:application/pdf;Rey et al. - 2015 - Past, present and future of spike sorting techniqu.pdf:/home/thomaav/Zotero/storage/MXRVGT6G/Rey et al. - 2015 - Past, present and future of spike sorting techniqu.pdf:application/pdf}
}

@article{oslo_and_akershus_university_college_of_applied_sciences_deep_2017,
	title = {Deep {Learning} with {Cellular} {Automaton}-{Based} {Reservoir} {Computing}},
	volume = {26},
	issn = {08912513},
	url = {http://www.complex-systems.com/abstracts/v26_i04_a02/},
	doi = {10.25088/ComplexSystems.26.4.319},
	language = {en},
	number = {4},
	urldate = {2019-03-05},
	journal = {Complex Systems},
	author = {{Oslo and Akershus University College of Applied Sciences} and Nichele, Stefano and Molund, Andreas and {Norwegian University of Science and Technology}},
	month = dec,
	year = {2017},
	pages = {319--339},
	file = {Oslo and Akershus University College of Applied Sciences et al. - 2017 - Deep Learning with Cellular Automaton-Based Reserv.pdf:/home/thomaav/Zotero/storage/8EXPSAZU/Oslo and Akershus University College of Applied Sciences et al. - 2017 - Deep Learning with Cellular Automaton-Based Reserv.pdf:application/pdf}
}

@inproceedings{nichele_towards_2017,
	address = {Honolulu, HI},
	title = {Towards a plant bio-machine},
	isbn = {978-1-5386-2726-6},
	url = {http://ieeexplore.ieee.org/document/8285288/},
	doi = {10.1109/SSCI.2017.8285288},
	abstract = {Plants are very efﬁcient computing machines. They are able to sense diverse environmental conditions and quickly react through chemical and electrical signaling. In this paper, we present an interface between plants and machines (a cybernetic plant), with the goal of augmenting the capabilities of plants towards the creation of plant biosensors. We implement a data acquisition system able to stimulate the plant through different electrical signals, as well as record the electrical activity of plants in response to changing electrical stimulations, light conditions, and chemicals. The results serve as a proof of concept that sensing capabilities of plants are a viable option for the development of plant bio-machines. Different future scenarios (some speculative) are discussed. The work herein is carried out as a collaboration between the EU project Flora Robotica and the EU project NASCENCE.},
	language = {en},
	urldate = {2019-03-05},
	booktitle = {2017 {IEEE} {Symposium} {Series} on {Computational} {Intelligence} ({SSCI})},
	publisher = {IEEE},
	author = {Nichele, Stefano and Risi, Sebastian and Tufte, Gunnar and Beloff, Laura},
	month = nov,
	year = {2017},
	pages = {1--8},
	file = {Nichele et al. - 2017 - Towards a plant bio-machine.pdf:/home/thomaav/Zotero/storage/GCR2XAG8/Nichele et al. - 2017 - Towards a plant bio-machine.pdf:application/pdf}
}

@article{holme_rare_2019,
	title = {Rare and everywhere: {Perspectives} on scale-free networks},
	volume = {10},
	issn = {2041-1723},
	shorttitle = {Rare and everywhere},
	url = {http://www.nature.com/articles/s41467-019-09038-8},
	doi = {10.1038/s41467-019-09038-8},
	language = {en},
	number = {1},
	urldate = {2019-03-04},
	journal = {Nature Communications},
	author = {Holme, Petter},
	month = dec,
	year = {2019},
	keywords = {Read, Printed},
	file = {Holme - 2019 - Rare and everywhere Perspectives on scale-free ne.pdf:/home/thomaav/Zotero/storage/QZX2ZJI3/Holme - 2019 - Rare and everywhere Perspectives on scale-free ne.pdf:application/pdf}
}

@article{verstraeten_verstraeten_2009,
	title = {Verstraeten - 2009 - {Reservoir} {Computing} computation with dynamical systems.pdf},
	author = {Verstraeten},
	year = {2009},
	file = {Verstraeten - 2009 - Reservoir Computing computation with dynamical systems.pdf:/home/thomaav/Zotero/storage/WFLJBZ92/Verstraeten - 2009 - Reservoir Computing computation with dynamical systems.pdf:application/pdf}
}

@article{vandoorne_experimental_2014,
	title = {Experimental demonstration of reservoir computing on a silicon photonics chip},
	volume = {5},
	issn = {2041-1723},
	url = {http://www.nature.com/articles/ncomms4541},
	doi = {10.1038/ncomms4541},
	language = {en},
	number = {1},
	urldate = {2019-03-04},
	journal = {Nature Communications},
	author = {Vandoorne, Kristof and Mechet, Pauline and Van Vaerenbergh, Thomas and Fiers, Martin and Morthier, Geert and Verstraeten, David and Schrauwen, Benjamin and Dambre, Joni and Bienstman, Peter},
	month = dec,
	year = {2014},
	keywords = {Favorite, Read, Printed},
	file = {Vandoorne et al. - 2014 - Experimental demonstration of reservoir computing .pdf:/home/thomaav/Zotero/storage/ZHBP4BCL/Vandoorne et al. - 2014 - Experimental demonstration of reservoir computing .pdf:application/pdf}
}

@inproceedings{gibbons_unifying_2010,
	address = {Barcelona, Spain},
	title = {Unifying quality metrics for reservoir networks},
	isbn = {978-1-4244-6916-1},
	url = {http://ieeexplore.ieee.org/document/5596307/},
	doi = {10.1109/IJCNN.2010.5596307},
	abstract = {Several metrics for the quality of reservoirs have been proposed and linked to reservoir performance in Echo State Networks and Liquid State Machines. A method to visualize the quality of a reservoir, called the separation ratio graph, is developed from these existing metrics leading to a generalized approach to measuring reservoir quality.},
	language = {en},
	urldate = {2019-03-04},
	booktitle = {The 2010 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Gibbons, Thomas E.},
	month = jul,
	year = {2010},
	keywords = {Read, Printed},
	pages = {1--7},
	file = {Gibbons - 2010 - Unifying quality metrics for reservoir networks.pdf:/home/thomaav/Zotero/storage/52ZCBLFE/Gibbons - 2010 - Unifying quality metrics for reservoir networks.pdf:application/pdf}
}

@article{dambre_information_2012,
	title = {Information {Processing} {Capacity} of {Dynamical} {Systems}},
	volume = {2},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/srep00514},
	doi = {10.1038/srep00514},
	language = {en},
	number = {1},
	urldate = {2019-03-04},
	journal = {Scientific Reports},
	author = {Dambre, Joni and Verstraeten, David and Schrauwen, Benjamin and Massar, Serge},
	month = dec,
	year = {2012},
	keywords = {Favorite, Read, Printed},
	file = {Dambre et al. - 2012 - Information Processing Capacity of Dynamical Syste.pdf:/home/thomaav/Zotero/storage/9YXZ6LKT/Dambre et al. - 2012 - Information Processing Capacity of Dynamical Syste.pdf:application/pdf}
}

@article{appeltant_information_2011,
	title = {Information processing using a single dynamical node as complex system},
	volume = {2},
	issn = {2041-1723},
	url = {http://www.nature.com/articles/ncomms1476},
	doi = {10.1038/ncomms1476},
	language = {en},
	number = {1},
	urldate = {2019-02-26},
	journal = {Nature Communications},
	author = {Appeltant, L. and Soriano, M.C. and Van der Sande, G. and Danckaert, J. and Massar, S. and Dambre, J. and Schrauwen, B. and Mirasso, C.R. and Fischer, I.},
	month = sep,
	year = {2011},
	keywords = {Favorite, Read, Printed},
	file = {Appeltant et al. - 2011 - Information processing using a single dynamical no.pdf:/home/thomaav/Zotero/storage/Z2NCGKAL/Appeltant et al. - 2011 - Information processing using a single dynamical no.pdf:application/pdf}
}

@article{maass_real-time_2002,
	title = {Real-{Time} {Computing} {Without} {Stable} {States}: {A} {New} {Framework} for {Neural} {Computation} {Based} on {Perturbations}},
	volume = {14},
	issn = {0899-7667, 1530-888X},
	shorttitle = {Real-{Time} {Computing} {Without} {Stable} {States}},
	url = {http://www.mitpressjournals.org/doi/10.1162/089976602760407955},
	doi = {10.1162/089976602760407955},
	language = {en},
	number = {11},
	urldate = {2019-02-25},
	journal = {Neural Computation},
	author = {Maass, Wolfgang and Natschläger, Thomas and Markram, Henry},
	month = nov,
	year = {2002},
	keywords = {Favorite, Read, Printed},
	pages = {2531--2560},
	file = {Maass et al. - 2002 - Real-Time Computing Without Stable States A New F:/home/thomaav/Zotero/storage/X83UFYQZ/Maass et al. - 2002 - Real-Time Computing Without Stable States A New F:application/pdf}
}

@article{dockendorf_liquid_2009,
	title = {Liquid state machines and cultured cortical networks: {The} separation property},
	volume = {95},
	issn = {03032647},
	shorttitle = {Liquid state machines and cultured cortical networks},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0303264708001792},
	doi = {10.1016/j.biosystems.2008.08.001},
	abstract = {In vitro neural networks of cortical neurons interfaced to a computer via multichannel microelectrode arrays (MEA) provide a unique paradigm to create a hybrid neural computer. Unfortunately, only rudimentary information about these in vitro network’s computational properties or the extent of their abilities are known. To study those properties, a liquid state machine (LSM) approach was employed in which the liquid (typically an artiﬁcial neural network) was replaced with a living cortical network and the input and readout functions were replaced by the MEA–computer interface. A key requirement of the LSM architecture is that inputs into the liquid state must result in separable outputs based on the liquid’s response (separation property). In this paper, high and low frequency multi-site stimulation patterns were applied to the living cortical networks. Two template-based classiﬁers, one based on Euclidean distance and a second based on a cross-correlation were then applied to measure the separation of the input–output relationship. The result was over a 95\% (99.8\% when nonstationarity is compensated) input reconstruction accuracy for the high and low frequency patterns, conﬁrming the existence of the separation property in these biological networks.},
	language = {en},
	number = {2},
	urldate = {2019-02-26},
	journal = {Biosystems},
	author = {Dockendorf, Karl P. and Park, Il and He, Ping and Príncipe, José C. and DeMarse, Thomas B.},
	month = feb,
	year = {2009},
	keywords = {Favorite, Read},
	pages = {90--97},
	file = {Dockendorf et al. - 2009 - Liquid state machines and cultured cortical networ.pdf:/home/thomaav/Zotero/storage/WVLI267S/Dockendorf et al. - 2009 - Liquid state machines and cultured cortical networ.pdf:application/pdf}
}

@article{horsman_when_2014,
	title = {When does a physical system compute?},
	volume = {470},
	issn = {1364-5021, 1471-2946},
	url = {http://arxiv.org/abs/1309.7979},
	doi = {10.1098/rspa.2014.0182},
	abstract = {Computing is a high-level process of a physical system. Recent interest in non-standard computing systems, including quantum and biological computers, has brought this physical basis of computing to the forefront. There has been, however, no consensus on how to tell if a given physical system is acting as a computer or not; leading to confusion over novel computational devices, and even claims that every physical event is a computation. In this paper we introduce a formal framework that can be used to determine whether or not a physical system is performing a computation. We demonstrate how the abstract computational level interacts with the physical device level, drawing the comparison with the use of mathematical models to represent physical objects in experimental science. This powerful formulation allows a precise description of the similarities between experiments, computation, simulation, and technology, leading to our central conclusion: physical computing is the use of a physical system to predict the outcome of an abstract evolution. We give conditions that must be satisfied in order for computation to be occurring, and illustrate these with a range of non-standard computing scenarios. The framework also covers broader computing contexts, where there is no obvious human computer user. We define the critical notion of a 'computational entity', and show the role this plays in defining when computing is taking place in physical systems.},
	language = {en},
	number = {2169},
	urldate = {2019-02-27},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Horsman, Clare and Stepney, Susan and Wagner, Rob C. and Kendon, Viv},
	month = jul,
	year = {2014},
	note = {arXiv: 1309.7979},
	keywords = {Computer Science - Emerging Technologies, Physics - History and Philosophy of Physics, Quantum Physics},
	pages = {20140182--20140182},
	file = {Horsman et al. - 2014 - When does a physical system compute.pdf:/home/thomaav/Zotero/storage/FG85ZPHA/Horsman et al. - 2014 - When does a physical system compute.pdf:application/pdf}
}

@article{massobrio_self-organized_2015,
	title = {Self-organized criticality in cortical assemblies occurs in concurrent scale-free and small-world networks},
	volume = {5},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/srep10578},
	doi = {10.1038/srep10578},
	language = {en},
	number = {1},
	urldate = {2019-02-27},
	journal = {Scientific Reports},
	author = {Massobrio, Paolo and Pasquale, Valentina and Martinoia, Sergio},
	month = sep,
	year = {2015},
	file = {Massobrio et al. - 2015 - Self-organized criticality in cortical assemblies .pdf:/home/thomaav/Zotero/storage/LIS4F7B8/Massobrio et al. - 2015 - Self-organized criticality in cortical assemblies .pdf:application/pdf}
}

@article{gershenson_introduction_2004,
	title = {Introduction to {Random} {Boolean} {Networks}},
	url = {http://arxiv.org/abs/nlin/0408006},
	abstract = {The goal of this tutorial is to promote interest in the study of random Boolean networks (RBNs). These can be very interesting models, since one does not have to assume any functionality or particular connectivity of the networks to study their generic properties. Like this, RBNs have been used for exploring the conﬁgurations where life could emerge. The fact that RBNs are a generalization of cellular automata makes their research a very important topic.},
	language = {en},
	urldate = {2019-02-25},
	journal = {arXiv:nlin/0408006},
	author = {Gershenson, Carlos},
	month = aug,
	year = {2004},
	note = {arXiv: nlin/0408006},
	keywords = {Computer Science - Computational Complexity, Condensed Matter - Statistical Mechanics, Nonlinear Sciences - Adaptation and Self-Organizing Systems, Nonlinear Sciences - Cellular Automata and Lattice Gases, Quantitative Biology - Molecular Networks, Quantitative Biology - Quantitative Methods, Favorite, Read, Printed},
	file = {Gershenson - 2004 - Introduction to Random Boolean Networks:/home/thomaav/Zotero/storage/ZZV5FHFI/Gershenson - 2004 - Introduction to Random Boolean Networks:application/pdf}
}

@article{snyder_computational_2013,
	title = {Computational {Capabilities} of {Random} {Automata} {Networks} for {Reservoir} {Computing}},
	volume = {87},
	issn = {1539-3755, 1550-2376},
	url = {http://arxiv.org/abs/1212.1744},
	doi = {10.1103/PhysRevE.87.042808},
	abstract = {This paper underscores the conjecture that intrinsic computation is maximal in systems at the "edge of chaos." We study the relationship between dynamics and computational capability in Random Boolean Networks (RBN) for Reservoir Computing (RC). RC is a computational paradigm in which a trained readout layer interprets the dynamics of an excitable component (called the reservoir) that is perturbed by external input. The reservoir is often implemented as a homogeneous recurrent neural network, but there has been little investigation into the properties of reservoirs that are discrete and heterogeneous. Random Boolean networks are generic and heterogeneous dynamical systems and here we use them as the reservoir. An RBN is typically a closed system; to use it as a reservoir we extend it with an input layer. As a consequence of perturbation, the RBN does not necessarily fall into an attractor. Computational capability in RC arises from a trade-off between separability and fading memory of inputs. We find the balance of these properties predictive of classification power and optimal at critical connectivity. These results are relevant to the construction of devices which exploit the intrinsic dynamics of complex heterogeneous systems, such as biomolecular substrates.},
	language = {en},
	number = {4},
	urldate = {2019-02-25},
	journal = {Physical Review E},
	author = {Snyder, David and Goudarzi, Alireza and Teuscher, Christof},
	month = apr,
	year = {2013},
	note = {arXiv: 1212.1744},
	keywords = {Computer Science - Neural and Evolutionary Computing, Nonlinear Sciences - Adaptation and Self-Organizing Systems, Condensed Matter - Disordered Systems and Neural Networks, Favorite, Read, Printed},
	file = {Snyder et al. - 2013 - Computational Capabilities of Random Automata Netw:/home/thomaav/Zotero/storage/5VU6Q2ZI/Snyder et al. - 2013 - Computational Capabilities of Random Automata Netw:application/pdf}
}

@article{langton_computation_1990,
	title = {Computation at the edge of chaos: {Phase} transitions and emergent computation},
	volume = {42},
	issn = {01672789},
	shorttitle = {Computation at the edge of chaos},
	url = {http://linkinghub.elsevier.com/retrieve/pii/016727899090064V},
	doi = {10.1016/0167-2789(90)90064-V},
	language = {en},
	number = {1-3},
	urldate = {2019-02-25},
	journal = {Physica D: Nonlinear Phenomena},
	author = {Langton, Chris G.},
	month = jun,
	year = {1990},
	keywords = {Good, Read},
	pages = {12--37},
	file = {Langton - 1990 - Computation at the edge of chaos Phase transition:/home/thomaav/Zotero/storage/JW66I5FS/Langton - 1990 - Computation at the edge of chaos Phase transition:application/pdf}
}

@article{schrauwen_overview_2007,
	title = {An overview of reservoir computing: theory, applications and implementations},
	abstract = {Training recurrent neural networks is hard. Recently it has however been discovered that it is possible to just construct a random recurrent topology, and only train a single linear readout layer. State-ofthe-art performance can easily be achieved with this setup, called Reservoir Computing. The idea can even be broadened by stating that any high dimensional, driven dynamic system, operated in the correct dynamic regime can be used as a temporal ‘kernel’ which makes it possible to solve complex tasks using just linear post-processing techniques.},
	language = {en},
	journal = {Proceedings of the 15th European Sympsosium on Artificial Neural Networks},
	author = {Schrauwen, Benjamin},
	month = apr,
	year = {2007},
	keywords = {Good},
	pages = {471--482},
	file = {Schrauwen - An overview of reservoir computing theory, applic.pdf:/home/thomaav/Zotero/storage/A23Y4MPM/Schrauwen - An overview of reservoir computing theory, applic.pdf:application/pdf}
}

@article{lukosevicius_reservoir_2009,
	title = {Reservoir computing approaches to recurrent neural network training},
	volume = {3},
	issn = {15740137},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1574013709000173},
	doi = {10.1016/j.cosrev.2009.03.005},
	abstract = {Echo State Networks and Liquid State Machines introduced a new paradigm in artiﬁcial recurrent neural network (RNN) training, where an RNN (the reservoir ) is generated randomly and only a readout is trained. The paradigm, becoming known as reservoir computing, greatly facilitated the practical application of RNNs and outperformed classical fully trained RNNs in many tasks. It has lately become a vivid research ﬁeld with numerous extensions of the basic idea, including reservoir adaptation, thus broadening the initial paradigm to using diﬀerent methods for training the reservoir and the readout. This review systematically surveys both current ways of generating/adapting the reservoirs and training diﬀerent types of readouts. It oﬀers a natural conceptual classiﬁcation of the techniques, which transcends boundaries of the current “brandnames” of reservoir methods, and thus aims to help in unifying the ﬁeld and providing the reader with a detailed “map” of it.},
	language = {en},
	number = {3},
	urldate = {2019-02-25},
	journal = {Computer Science Review},
	author = {Lukoševičius, Mantas and Jaeger, Herbert},
	month = aug,
	year = {2009},
	keywords = {Favorite, Read, Printed},
	pages = {127--149},
	file = {Lukoševičius and Jaeger - 2009 - Reservoir computing approaches to recurrent neural.pdf:/home/thomaav/Zotero/storage/XINWLMRZ/Lukoševičius and Jaeger - 2009 - Reservoir computing approaches to recurrent neural.pdf:application/pdf;Lukoševičius and Jaeger - 2009 - Reservoir computing approaches to recurrent neural.pdf:/home/thomaav/Zotero/storage/N3C5NJ8J/Lukoševičius and Jaeger - 2009 - Reservoir computing approaches to recurrent neural.pdf:application/pdf}
}

@article{thompson_reflections_1984,
	title = {Reflections on {Trusting} {Trust}},
	language = {en},
	author = {Thompson, Ken},
	month = aug,
	year = {1984},
	keywords = {Read, Printed},
	pages = {3},
	file = {Thompson - Reflections on Trusting Trust:/home/thomaav/Zotero/storage/W69ZWHBL/Thompson - Reflections on Trusting Trust:application/pdf}
}

@article{keshav_how_2016,
	title = {How to {Read} a {Paper}},
	author = {Keshav, S.},
	month = feb,
	year = {2016},
	keywords = {Read, Printed},
	file = {How to Read a Paper - Keshav:/home/thomaav/Zotero/storage/9P27EILA/How to Read a Paper - Keshav:application/pdf}
}

@article{burkow_exploring_2016,
	title = {Exploring {Physical} {Reservoir} {Computing} using {Random} {Boolean} {Networks}.},
	language = {en},
	author = {Burkow, Aleksander Vognild},
	month = jun,
	year = {2016},
	keywords = {Read, Printed},
	pages = {64},
	file = {Burkow - Exploring Physical Reservoir Computing using Rando:/home/thomaav/Zotero/storage/PVYLQ8B6/Burkow - Exploring Physical Reservoir Computing using Rando:application/pdf}
}

@article{heylighen_science_1999,
	title = {{THE} {SCIENCE} {OF} {SELF}- {ORGANIZATION} {AND} {ADAPTIVITY}},
	abstract = {The theory of self-organization and adaptivity has grown out of a variety of disciplines, including thermodynamics, cybernetics and computer modelling. The present article reviews its most important concepts and principles. It starts with an intuitive overview, illustrated by the examples of magnetization and Bénard convection, and concludes with the basics of mathematical modelling. Self-organization can be defined as the spontaneous creation of a globally coherent pattern out of local interactions. Because of its distributed character, this organization tends to be robust, resisting perturbations. The dynamics of a self-organizing system is typically non-linear, because of circular or feedback relations between the components. Positive feedback leads to an explosive growth, which ends when all components have been absorbed into the new configuration, leaving the system in a stable, negative feedback state. Non-linear systems have in general several stable states, and this number tends to increase (bifurcate) as an increasing input of energy pushes the system farther from its thermodynamic equilibrium. To adapt to a changing environment, the system needs a variety of stable states that is large enough to react to all perturbations but not so large as to make its evolution uncontrollably chaotic. The most adequate states are selected according to their fitness, either directly by the environment, or by subsystems that have adapted to the environment at an earlier stage. Formally, the basic mechanism underlying self-organization is the (often noise-driven) variation which explores different regions in the system’s state space until it enters an attractor. This precludes further variation outside the attractor, and thus restricts the freedom of the system’s components to behave independently. This is equivalent to the increase of coherence, or decrease of statistical entropy, that defines selforganization.},
	language = {en},
	author = {Heylighen, Francis},
	year = {1999},
	keywords = {Read},
	pages = {26},
	file = {Heylighen - THE SCIENCE OF SELF- ORGANIZATION AND ADAPTIVITY:/home/thomaav/Zotero/storage/HDBS4KBH/Heylighen - THE SCIENCE OF SELF- ORGANIZATION AND ADAPTIVITY:application/pdf}
}

@article{auer_p-delta_2002,
	title = {The p-{Delta} {Learning} {Rule} for {Parallel} {Perceptrons}},
	abstract = {A learning algorithm is presented for circuits consisting of a single layer of perceptrons (= threshold gates, or equivalently gates with a Heaviside activation function). We refer to such circuits as parallel perceptrons. In spite of their simplicity, these circuits can compute any boolean function if one views the majority of the binary perceptron outputs as the binary outputs of the parallel perceptron, and they are universal approximators for arbitrary continuous functions with values in [0, 1] if one views the fraction of perceptrons that output 1 as the analog output of the parallel perceptron. For a long time one has thought that there exists no competitive learning algorithms for these extremely simple circuits consisting of gates with binary outputs, which also became known as committee machines. It is commonly believed that one ∗Research for this article was partially supported by the ESPRIT Working Group NeuroCOLT, No. 8556, and the Fonds zur Fo¨rderung der wissenschaftlichen Forschung (FWF), Austria, project P12153.},
	language = {en},
	author = {Auer, Peter and Burgsteiner, Harald M and Maass, Wolfgang},
	month = mar,
	year = {2002},
	keywords = {Read, Printed},
	pages = {29},
	file = {Auer et al. - The p-Delta Learning Rule for Parallel Perceptrons:/home/thomaav/Zotero/storage/4BQQ8NNH/Auer et al. - The p-Delta Learning Rule for Parallel Perceptrons:application/pdf}
}

@article{aaser_shodan_2018,
	title = {{SHODAN}, {Master}'s {Thesis}},
	author = {Aaser, Peter},
	year = {2018},
	keywords = {Read},
	file = {SHODAN, Master's Thesis - Peter Aaser:/home/thomaav/Zotero/storage/N2SFI926/SHODAN, Master's Thesis - Peter Aaser:application/pdf}
}

@article{rastogi_causality_2008,
	title = {Causality principle, non-equilibrium thermodynamics and non-linear science of open systems},
	volume = {67},
	abstract = {In real life situations, open systems in non-equilibrium are sometimes in steady state and sometimes in non-steady states. Steady state is obtained when forces and counter forces interact. However, when multiple forces are operative involving autocatalysis (positive feedback) and inhibitory step (negative feedback), exotic non-equilibrium phenomena are observed. Extensive theoretical studies based on non-equilibrium thermodynamics and non-linear dynamics and experimental studies related to simple and complex non-equilibrium physico-chemical systems, which have possible application in real systems in nature, has been reported. Present paper intends to: i) Elucidate concepts related to causality principle developed on the basis of experimental and theoretical studies; and ii) To illustrate manner, in which multiple forces interact in phenomena involving multiple processes and multiple cause-effect sequence such as: a) non-equilibrium steady states; b) bifurcation from steady state to bi-stability and oscillatory state; c) temporal and spatio-temporal oscillations; d) chaos; and e) pattern formation and fractal growth in space.},
	language = {en},
	journal = {LINEAR SYSTEMS},
	author = {Rastogi, R P and Srivastava, R C},
	year = {2008},
	keywords = {Read, Printed},
	pages = {12},
	file = {Rastogi and Srivastava - 2008 - Causality principle, non-equilibrium thermodynamic:/home/thomaav/Zotero/storage/NYBDX94H/Rastogi and Srivastava - 2008 - Causality principle, non-equilibrium thermodynamic:application/pdf}
}

@inproceedings{jensen_computation_2018,
	address = {Tokyo, Japan},
	title = {Computation in artificial spin ice},
	url = {https://www.mitpressjournals.org/doi/abs/10.1162/isal_a_00011},
	doi = {10.1162/isal_a_00011},
	abstract = {We explore artiﬁcial spin ice (ASI) as a substrate for material computation. ASI consists of large numbers of nanomagnets arranged in a 2D lattice. Local interactions between the magnets gives rise to a range of complex collective behavior. The ferromagnets form large networks of nonlinear nodes, which in many ways resemble artiﬁcial neural networks. In this work, we investigate key computational properties of ASI through micromagnetic simulations. Our nanomagnetic system exhibits a large number of reachable stable states and a wide range of available dynamics when perturbed by an external magnetic ﬁeld. Furthermore, we ﬁnd that the system is able to store and process temporal input patterns. The emergent behavior is highly tunable by varying the parameters of the external ﬁeld. Our ﬁndings highlight ASI as a very promising substrate for in-materio computation at the nanoscale.},
	language = {en},
	urldate = {2019-02-25},
	booktitle = {The 2018 {Conference} on {Artificial} {Life}},
	publisher = {MIT Press},
	author = {Jensen, Johannes H. and Folven, Erik and Tufte, Gunnar},
	year = {2018},
	keywords = {Read, Printed},
	pages = {15--22},
	file = {Jensen et al. - 2018 - Computation in artificial spin ice. Jensen:/home/thomaav/Zotero/storage/PBWL7B83/Jensen et al. - 2018 - Computation in artificial spin ice. Jensen:application/pdf}
}

@inproceedings{jensen_reservoir_2017,
	address = {Lyon, France},
	title = {Reservoir computing with a chaotic circuit},
	isbn = {978-0-262-34633-7},
	url = {https://www.mitpressjournals.org/doi/abs/10.1162/isal_a_039},
	doi = {10.7551/ecal_a_039},
	abstract = {Reservoir Computing has been highlighted as a promising methodology to perform computation in dynamical systems. This makes Reservoir Computing particularly interesting for exploiting physical systems directly as computing substrates, where the computation happens “for free” in the rich physical domain. In this work we consider a simple chaotic circuit as a reservoir: the Driven Chua’s circuit. Its rich variety of available dynamics makes it versatile as a reservoir. At the same time, its simplicity offers insight into what physical properties can be useful for computation. We demonstrate both through simulation and in-circuit experiments, that such a simple circuit can be readily exploited for computation. Our results show excellent performance on two non-temporal tasks. The fact that such a simple nonlinear circuit can be used, suggests that a wide variety of physical systems can be viewed as potential reservoirs.},
	language = {en},
	urldate = {2019-02-25},
	booktitle = {Proceedings of the 14th {European} {Conference} on {Artificial} {Life} {ECAL} 2017},
	publisher = {MIT Press},
	author = {Jensen, Johannes H. and Tufte, Gunnar},
	month = sep,
	year = {2017},
	keywords = {Read, Printed},
	pages = {222--229},
	file = {Jensen and Tufte - 2017 - Reservoir computing with a chaotic circuit. Jensen:/home/thomaav/Zotero/storage/XCW4C94I/Jensen and Tufte - 2017 - Reservoir computing with a chaotic circuit. Jensen:application/pdf}
}

@article{ladyman_what_2013,
	title = {What is a complex system?},
	volume = {3},
	issn = {1879-4912, 1879-4920},
	url = {http://link.springer.com/10.1007/s13194-012-0056-8},
	doi = {10.1007/s13194-012-0056-8},
	language = {en},
	number = {1},
	urldate = {2019-02-25},
	journal = {European Journal for Philosophy of Science},
	author = {Ladyman, James and Lambert, James and Wiesner, Karoline},
	month = jan,
	year = {2013},
	keywords = {Read, Printed},
	pages = {33--67},
	file = {Ladyman et al. - 2013 - What is a complex system:/home/thomaav/Zotero/storage/G26KZ22H/Ladyman et al. - 2013 - What is a complex system:application/pdf}
}

@article{sipper_emergence_1999,
	title = {The emergence of cellular computing},
	volume = {32},
	issn = {00189162},
	url = {http://ieeexplore.ieee.org/document/774914/},
	doi = {10.1109/2.774914},
	language = {en},
	number = {7},
	urldate = {2019-02-25},
	journal = {Computer},
	author = {Sipper, M.},
	month = jul,
	year = {1999},
	keywords = {Read},
	pages = {18--26},
	file = {Sipper - 1999 - The emergence of cellular computing:/home/thomaav/Zotero/storage/HWQNGB6K/Sipper - 1999 - The emergence of cellular computing:application/pdf}
}

@article{tanaka_recent_2018,
	title = {Recent {Advances} in {Physical} {Reservoir} {Computing}: {A} {Review}},
	shorttitle = {Recent {Advances} in {Physical} {Reservoir} {Computing}},
	url = {http://arxiv.org/abs/1808.04962},
	abstract = {Reservoir computing is a computational framework suited for temporal/sequential data processing. It is derived from several recurrent neural network models, including echo state networks and liquid state machines. A reservoir computing system consists of a reservoir for mapping inputs into a high-dimensional space and a readout for extracting features of the inputs. Further, training is carried out only in the readout. Thus, the major advantage of reservoir computing is fast and simple learning compared to other recurrent neural networks. Another advantage is that the reservoir can be realized using physical systems, substrates, and devices, instead of recurrent neural networks. In fact, such physical reservoir computing has attracted increasing attention in various ﬁelds of research. The purpose of this review is to provide an overview of recent advances in physical reservoir computing by classifying them according to the type of the reservoir. We discuss the current issues and perspectives related to physical reservoir computing, in order to further expand its practical applications and develop next-generation machine learning systems.},
	language = {en},
	urldate = {2019-02-25},
	journal = {arXiv:1808.04962 [cs]},
	author = {Tanaka, Gouhei and Yamane, Toshiyuki and Héroux, Jean Benoit and Nakane, Ryosho and Kanazawa, Naoki and Takeda, Seiji and Numata, Hidetoshi and Nakano, Daiju and Hirose, Akira},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.04962},
	keywords = {Computer Science - Emerging Technologies, Favorite, Read, Printed},
	file = {Tanaka et al. - 2018 - Recent Advances in Physical Reservoir Computing A:/home/thomaav/Zotero/storage/ZC58RPTI/Tanaka et al. - 2018 - Recent Advances in Physical Reservoir Computing A:application/pdf;Tanaka et al. - 2019 - Recent Advances in Physical Reservoir Computing A. - 2018 - Recent Advances in Physical Reservoir Computing A:/home/thomaav/Zotero/storage/T63XVJ4L/Tanaka et al. - 2019 - Recent Advances in Physical Reservoir Computing A. - 2018 - Recent Advances in Physical Reservoir Computing A:application/pdf}
}

@article{clauset_power-law_2009,
	title = {Power-law distributions in empirical data},
	volume = {51},
	issn = {0036-1445, 1095-7200},
	url = {http://arxiv.org/abs/0706.1062},
	doi = {10.1137/070710111},
	abstract = {Power-law distributions occur in many situations of scientiﬁc interest and have signiﬁcant consequences for our understanding of natural and man-made phenomena. Unfortunately, the detection and characterization of power laws is complicated by the large ﬂuctuations that occur in the tail of the distribution—the part of the distribution representing large but rare events—and by the diﬃculty of identifying the range over which power-law behavior holds. Commonly used methods for analyzing power-law data, such as least-squares ﬁtting, can produce substantially inaccurate estimates of parameters for power-law distributions, and even in cases where such methods return accurate answers they are still unsatisfactory because they give no indication of whether the data obey a power law at all. Here we present a principled statistical framework for discerning and quantifying power-law behavior in empirical data. Our approach combines maximum-likelihood ﬁtting methods with goodness-of-ﬁt tests based on the Kolmogorov-Smirnov statistic and likelihood ratios. We evaluate the eﬀectiveness of the approach with tests on synthetic data and give critical comparisons to previous approaches. We also apply the proposed methods to twenty-four real-world data sets from a range of diﬀerent disciplines, each of which has been conjectured to follow a powerlaw distribution. In some cases we ﬁnd these conjectures to be consistent with the data while in others the power law is ruled out.},
	language = {en},
	number = {4},
	urldate = {2019-02-25},
	journal = {SIAM Review},
	author = {Clauset, Aaron and Shalizi, Cosma Rohilla and Newman, M. E. J.},
	month = nov,
	year = {2009},
	note = {arXiv: 0706.1062},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Physics - Data Analysis, Statistics and Probability, Statistics - Applications, Statistics - Methodology, Printed},
	pages = {661--703},
	file = {Clauset et al. - 2009 - Power-law distributions in empirical data:/home/thomaav/Zotero/storage/72A8EC2L/Clauset et al. - 2009 - Power-law distributions in empirical data:application/pdf}
}

@incollection{goos_pattern_2003,
	address = {Berlin, Heidelberg},
	title = {Pattern {Recognition} in a {Bucket}},
	volume = {2801},
	isbn = {978-3-540-20057-4 978-3-540-39432-7},
	url = {http://link.springer.com/10.1007/978-3-540-39432-7_63},
	abstract = {This paper demonstrates that the waves produced on the surface of water can be used as the medium for a “Liquid State Machine” that pre-processes inputs so allowing a simple perceptron to solve the XOR problem and undertake speech recognition. Interference between waves allows non-linear parallel computation upon simultaneous sensory inputs. Temporal patterns of stimulation are converted to spatial patterns of water waves upon which a linear discrimination can be made. Whereas Wolfgang Maass’ Liquid State Machine requires ﬁne tuning of the spiking neural network parameters, water has inherent self-organising properties such as strong local interactions, time-dependent spread of activation to distant areas, inherent stability to a wide variety of inputs, and high complexity. Water achieves this “for free”, and does so without the time-consuming computation required by realistic neural models. An analogy is made between water molecules and neurons in a recurrent neural network.},
	language = {en},
	urldate = {2019-02-25},
	booktitle = {Advances in {Artificial} {Life}},
	publisher = {Springer Berlin Heidelberg},
	author = {Fernando, Chrisantha and Sojakka, Sampsa},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Banzhaf, Wolfgang and Ziegler, Jens and Christaller, Thomas and Dittrich, Peter and Kim, Jan T.},
	year = {2003},
	doi = {10.1007/978-3-540-39432-7_63},
	keywords = {Read, Printed},
	pages = {588--597},
	file = {Fernando and Sojakka - 2003 - Pattern Recognition in a Bucket:/home/thomaav/Zotero/storage/QNPFRZ7W/Fernando and Sojakka - 2003 - Pattern Recognition in a Bucket:application/pdf}
}

@article{rohm_reservoir_2019,
	title = {Reservoir {Computing} {Using} {Laser} {Networks}},
	issn = {1077-260X, 1558-4542},
	url = {https://ieeexplore.ieee.org/document/8758193/},
	doi = {10.1109/JSTQE.2019.2927578},
	abstract = {Reservoir computing is a neuromorphic computing scheme inspired by the human brain. It has found great success as a versatile hardware-compatible application of machine learning concepts. In this paper we highlight the fundamental working principles and important characteristics of reservoir computing with a particular focus on photonic systems and networks. These systems can further be enhanced by the inclusion of delayed variables to produce complex spatiotemporally mixed ’timemultiplexed’ networks. We use a simple nonlinear oscillator model, that is not only applicable to lasers, but can also describe a variety of other oscillating systems.},
	language = {en},
	urldate = {2019-07-27},
	journal = {IEEE Journal of Selected Topics in Quantum Electronics},
	author = {Rohm, Andre and Jaurigue, Lina and Luedge, K.},
	year = {2019},
	keywords = {Read, Printed},
	pages = {1--1},
	file = {Rohm et al. - 2019 - Reservoir Computing Using Laser Networks.pdf:/home/thomaav/Zotero/storage/BLC52FH2/Rohm et al. - 2019 - Reservoir Computing Using Laser Networks.pdf:application/pdf;Rohm et al. - 2019 - Reservoir Computing Using Laser Networks.pdf:/home/thomaav/Zotero/storage/WETHVZZE/Rohm et al. - 2019 - Reservoir Computing Using Laser Networks.pdf:application/pdf}
}

@article{liu_analyzing_2018,
	title = {Analyzing the {Noise} {Robustness} of {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1810.03913},
	abstract = {Deep neural networks (DNNs) are vulnerable to maliciously generated adversarial examples. These examples are intentionally designed by making imperceptible perturbations and often mislead a DNN into making an incorrect prediction. This phenomenon means that there is signiﬁcant risk in applying DNNs to safety-critical applications, such as driverless cars. To address this issue, we present a visual analytics approach to explain the primary cause of the wrong predictions introduced by adversarial examples. The key is to analyze the datapaths of the adversarial examples and compare them with those of the normal examples. A datapath is a group of critical neurons and their connections. To this end, we formulate the datapath extraction as a subset selection problem and approximately solve it based on back-propagation. A multi-level visualization consisting of a segmented DAG (layer level), an Euler diagram (feature map level), and a heat map (neuron level), has been designed to help experts investigate datapaths from the high-level layers to the detailed neuron activations. Two case studies are conducted that demonstrate the promise of our approach in support of explaining the working mechanism of adversarial examples.},
	language = {en},
	urldate = {2019-07-28},
	journal = {arXiv:1810.03913 [cs, stat]},
	author = {Liu, Mengchen and Liu, Shixia and Su, Hang and Cao, Kelei and Zhu, Jun},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.03913},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Liu et al. - 2018 - Analyzing the Noise Robustness of Deep Neural Netw.pdf:/home/thomaav/Zotero/storage/B4BU5MUK/Liu et al. - 2018 - Analyzing the Noise Robustness of Deep Neural Netw.pdf:application/pdf}
}

@article{goudarzi_comparative_2014,
	title = {A {Comparative} {Study} of {Reservoir} {Computing} for {Temporal} {Signal} {Processing}},
	url = {http://arxiv.org/abs/1401.2224},
	abstract = {Reservoir computing (RC) is a novel approach to time series prediction using recurrent neural networks. In RC, an input signal perturbs the intrinsic dynamics of a medium called a reservoir. A readout layer is then trained to reconstruct a target output from the reservoir’s state. The multitude of RC architectures and evaluation metrics poses a challenge to both practitioners and theorists who study the task-solving performance and computational power of RC. In addition, in contrast to traditional computation models, the reservoir is a dynamical system in which computation and memory are inseparable, and therefore hard to analyze. Here, we compare echo state networks (ESN), a popular RC architecture, with tapped-delay lines (DL) and nonlinear autoregressive exogenous (NARX) networks, which we use to model systems with limited computation and limited memory respectively. We compare the performance of the three systems while computing three common benchmark time series: He´non Map, NARMA10, and NARMA20. We ﬁnd that the role of the reservoir in the reservoir computing paradigm goes beyond providing a memory of the past inputs. The DL and the NARX network have higher memorization capability, but fall short of the generalization power of the ESN.},
	language = {en},
	urldate = {2019-09-11},
	journal = {arXiv:1401.2224 [cs]},
	author = {Goudarzi, Alireza and Banda, Peter and Lakin, Matthew R. and Teuscher, Christof and Stefanovic, Darko},
	month = jan,
	year = {2014},
	note = {arXiv: 1401.2224},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning, Read, Printed},
	file = {Goudarzi et al. - 2014 - A Comparative Study of Reservoir Computing for Tem.pdf:/home/thomaav/Zotero/storage/F2CU48TI/Goudarzi et al. - 2014 - A Comparative Study of Reservoir Computing for Tem.pdf:application/pdf}
}

@article{greenberg_framework_nodate,
	title = {A {Framework} for {Realistic} {Image} {Synthesis}},
	abstract = {Our goal is to develop physically based lighting models and perceptually based rendering procedures for computer graphics that will produce synthetic images that are visually and measurably indistinguishable from real-world images. Fidelity of the physical simulation is of primary concern. Our research framework is subdivided into three sub-sections: the local light reflection model, the energy transport simulation, and the visual display algorithms. The first two subsections are physically based, and the last is perceptually based.},
	language = {en},
	author = {Greenberg, Donald P and Arvo, James and Lafortune, Eric and Torrance, Kenneth E and Ferwerda, James A and Walter, Bruce and Trumbore, Ben and Shirley, Peter and Pattanaik, Sumanta and Foo, Sing-Choong},
	pages = {18},
	file = {Greenberg et al. - A Framework for Realistic Image Synthesis.pdf:/home/thomaav/Zotero/storage/9KZV4XWW/Greenberg et al. - A Framework for Realistic Image Synthesis.pdf:application/pdf}
}

@article{verstraeten_pattern_2008,
	title = {Pattern classiﬁcation with {CNNs} as reservoirs},
	abstract = {Reservoir Computing is a novel method in the ﬁeld of neural networks and machine learning, which combines the computational power of a nonlinear dynamic system with the ease of training of a linear classiﬁer. The basic setup is as follows: a suﬃciently complex network of nonlinear nodes (called the reservoir) is excited by an input signal, and the instantaneous dynamic response of the system is then used to train a simple linear readout function.},
	language = {en},
	author = {Verstraeten, D and Xavier-de-Souza, S and Schrauwen, B and Suykens, J and Stroobandt, D and Vandewalle, J},
	year = {2008},
	pages = {5},
	file = {Verstraeten et al. - Pattern classiﬁcation with CNNs as reservoirs.pdf:/home/thomaav/Zotero/storage/P9G8JDBH/Verstraeten et al. - Pattern classiﬁcation with CNNs as reservoirs.pdf:application/pdf}
}

@article{ganguli_memory_2008,
	title = {Memory traces in dynamical systems},
	volume = {105},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0804451105},
	doi = {10.1073/pnas.0804451105},
	language = {en},
	number = {48},
	urldate = {2019-09-18},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Ganguli, S. and Huh, D. and Sompolinsky, H.},
	month = dec,
	year = {2008},
	pages = {18970--18975},
	file = {Ganguli et al. - 2008 - Memory traces in dynamical systems.pdf:/home/thomaav/Zotero/storage/295KKCWA/Ganguli et al. - 2008 - Memory traces in dynamical systems.pdf:application/pdf}
}

@article{jaeger_short_2002,
	title = {Short {Term} {Memory} in {Echo} {State} {Networks}},
	journal = {GMD - German National Research Institute for Computer Science},
	author = {Jaeger, Herbert},
	year = {2002},
	file = {Short_Term_Memory_in_Echo_State_Networks.pdf:/home/thomaav/Zotero/storage/PDBQ9NJV/Short_Term_Memory_in_Echo_State_Networks.pdf:application/pdf}
}

@article{verstraeten_experimental_2007,
	title = {An experimental unification of reservoir computing methods},
	volume = {20},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S089360800700038X},
	doi = {10.1016/j.neunet.2007.04.003},
	abstract = {Three different uses of a recurrent neural network (RNN) as a reservoir that is not trained but instead read out by a simple external classiﬁcation layer have been described in the literature: Liquid State Machines (LSMs), Echo State Networks (ESNs) and the Backpropagation Decorrelation (BPDC) learning rule. Individual descriptions of these techniques exist, but a overview is still lacking. Here, we present a series of experimental results that compares all three implementations, and draw conclusions about the relation between a broad range of reservoir parameters and network dynamics, memory, node complexity and performance on a variety of benchmark tests with different characteristics. Next, we introduce a new measure for the reservoir dynamics based on Lyapunov exponents. Unlike previous measures in the literature, this measure is dependent on the dynamics of the reservoir in response to the inputs, and in the cases we tried, it indicates an optimal value for the global scaling of the weight matrix, irrespective of the standard measures. We also describe the Reservoir Computing Toolbox that was used for these experiments, which implements all the types of Reservoir Computing and allows the easy simulation of a wide range of reservoir topologies for a number of benchmarks.},
	language = {en},
	number = {3},
	urldate = {2019-09-18},
	journal = {Neural Networks},
	author = {Verstraeten, D. and Schrauwen, B. and D’Haene, M. and Stroobandt, D.},
	month = apr,
	year = {2007},
	keywords = {Printed},
	pages = {391--403},
	file = {Verstraeten et al. - 2007 - An experimental unification of reservoir computing.pdf:/home/thomaav/Zotero/storage/D32YXPZW/Verstraeten et al. - 2007 - An experimental unification of reservoir computing.pdf:application/pdf}
}

@article{scellier_equilibrium_2016,
	title = {Equilibrium {Propagation}: {Bridging} the {Gap} {Between} {Energy}-{Based} {Models} and {Backpropagation}},
	shorttitle = {Equilibrium {Propagation}},
	url = {http://arxiv.org/abs/1602.05179},
	abstract = {We introduce Equilibrium Propagation, a learning framework for energy-based models. It involves only one kind of neural computation, performed in both the ﬁrst phase (when the prediction is made) and the second phase of training (after the target or prediction error is revealed). Although this algorithm computes the gradient of an objective function just like Backpropagation, it does not need a special computation or circuit for the second phase, where errors are implicitly propagated. Equilibrium Propagation shares similarities with Contrastive Hebbian Learning and Contrastive Divergence while solving the theoretical issues of both algorithms: our algorithm computes the gradient of a well deﬁned objective function. Because the objective function is deﬁned in terms of local perturbations, the second phase of Equilibrium Propagation corresponds to only nudging the prediction (ﬁxed point, or stationary distribution) towards a conﬁguration that reduces prediction error. In the case of a recurrent multi-layer supervised network, the output units are slightly nudged towards their target in the second phase, and the perturbation introduced at the output layer propagates backward in the hidden layers. We show that the signal ’back-propagated’ during this second phase corresponds to the propagation of error derivatives and encodes the gradient of the objective function, when the synaptic update corresponds to a standard form of spike-timing dependent plasticity. This work makes it more plausible that a mechanism similar to Backpropagation could be implemented by brains, since leaky integrator neural computation performs both inference and error back-propagation in our model. The only local difference between the two phases is whether synaptic changes are allowed or not. We also show experimentally that multi-layer recurrently connected networks with 1, 2 and 3 hidden layers can be trained by Equilibrium Propagation on the permutation-invariant MNIST task.},
	language = {en},
	urldate = {2019-09-18},
	journal = {arXiv:1602.05179 [cs]},
	author = {Scellier, Benjamin and Bengio, Yoshua},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.05179},
	keywords = {Computer Science - Machine Learning},
	file = {Scellier and Bengio - 2016 - Equilibrium Propagation Bridging the Gap Between .pdf:/home/thomaav/Zotero/storage/QZURZQ6R/Scellier and Bengio - 2016 - Equilibrium Propagation Bridging the Gap Between .pdf:application/pdf}
}

@article{orlans_survey_nodate,
	title = {A {Survey} of {Synthetic} {Biometrics}: {Capabilities} and {Benefits}},
	abstract = {Computer generated “synthetic” biometrics are not widely used within the biometrics community beyond their current use as a research tool. Yet they offer a number of potential advantages that can be developed further to support the science and practical use of biometrics. They can be used to improve the understanding of a biometric system’s robustness and as an engineering tool to predict system performance. This paper surveys the state of synthetic biometrics generation, provides a glimpse at some benefits that can be obtained from their use, and discusses the issues retarding their adoption by the biometrics community.},
	language = {en},
	author = {Orlans, Nicholas M and Buettner, Douglas J and Box, P O and Marques, Joe},
	keywords = {Printed},
	pages = {7},
	file = {Orlans et al. - A Survey of Synthetic Biometrics Capabilities and.pdf:/home/thomaav/Zotero/storage/LAZPHSZB/Orlans et al. - A Survey of Synthetic Biometrics Capabilities and.pdf:application/pdf}
}

@inproceedings{jones_is_2007,
	address = {Honolulu, HI, USA},
	title = {Is there a {Liquid} {State} {Machine} in the {Bacterium} {Escherichia} {Coli}?},
	isbn = {978-1-4244-0701-9},
	url = {http://ieeexplore.ieee.org/document/4218885/},
	doi = {10.1109/ALIFE.2007.367795},
	abstract = {The bacterium Escherichia coli has the capacity to respond to a wide range of environmental inputs, which have the potential to change suddenly and rapidly. Although the functions of many of its signal transduction and gene regulation networks have been identiﬁed, E.Coli’s capacity for perceptual categorization, especially for discrimination between complex temporal patterns of chemical inputs, has been experimentally neglected. Real-time computations on time-varying inputs can be undertaken by a system possessing a high dimensional analog fading memory, i.e. a liquid-state machine (LSM). For example, the cortical microcolumn is hypothesized to be a LSM. A model of the gene regulation network (GRN) of E.Coli was assessed for its LSM properties for a range of increasingly complex stimuli. Cooperativity between transcription factors (TFs) is necessary for complex temporal discriminations. However, the low recurrence within the GRNs autonomous dynamics decreases its capacity for a rich fading memory, and hence for integrating temporal sequence information. We conclude that coupling of the GRN with signal transduction networks possessing cross-talk, and with metabolic networks is expected to increase the extent of nonautonomous recurrence and hence to facilitate enhanced LSM properties.},
	language = {en},
	urldate = {2019-09-13},
	booktitle = {2007 {IEEE} {Symposium} on {Artificial} {Life}},
	publisher = {IEEE},
	author = {Jones, Ben and Stekel, Dov and Rowe, Jon and Fernando, Chrisantha},
	month = apr,
	year = {2007},
	keywords = {Read, Printed},
	pages = {187--191},
	file = {Jones et al. - 2007 - Is there a Liquid State Machine in the Bacterium E.pdf:/home/thomaav/Zotero/storage/WVXSXABS/Jones et al. - 2007 - Is there a Liquid State Machine in the Bacterium E.pdf:application/pdf}
}

@article{floris_larger_nodate,
	title = {Larger et al. - 2017 - {High}-{Speed} {Photonic} {Reservoir} {Computing} {Using} a {Time}-{Delay}-{Based} {Architecture} {Million} {Words} per {Second} {Classif}(2)-annotated.pdf},
	author = {Floris, Laporte and Dambre, Joni and Bienstman, Peter},
	file = {Larger et al. - 2017 - High-Speed Photonic Reservoir Computing Using a Time-Delay-Based Architecture Million Words per Second Classif(2)-annotated.pdf:/home/thomaav/Zotero/storage/J9Z2ULF2/Larger et al. - 2017 - High-Speed Photonic Reservoir Computing Using a Time-Delay-Based Architecture Million Words per Second Classif(2)-annotated.pdf:application/pdf}
}

@article{haas_reservoir_2018,
	title = {Reservoir {Computing} {Based} on {Cellular} {Automata} ({ReCA}) in {Sequence} {Learning}},
	volume = {17},
	issn = {1071-4413, 1556-3022},
	url = {http://www.tandfonline.com/doi/abs/10.1080/1071441950170102},
	doi = {10.1080/1071441950170102},
	abstract = {ReCA is a reservoir computing architecture based on cellular automata in which the inputs pass on a cellular automaton instead of a recurrent neural network reservoir. ReCA has been tested using pathological synthetic sequence tasks (well-known benchmark tasks within the reservoir computing (RC) community), and has been showing promising results that reduce complexity compared with other RC approaches such as echo state networks (ESNs). In this paper, a number of methods for feature extraction from the cellular automata (CA) reservoir are introduced to improve ReCA by reducing its complexity while maintaining accuracy. The proposed method reduces the feature dimension by using a few states from every time step (EACH) in the reservoir and/or using only one side of the CA evolution (HALF) and/or reducing the CA evolution in space (expansion ratio f ). Due to the rich dynamics of the CA reservoir, the three methods of reduction (EACH, HALF and f ) can be used together to reduce the feature dimension by up to 98\% in some pathological tasks compared with the state-of-the-art ReCA results.},
	language = {en},
	number = {1},
	urldate = {2019-10-03},
	journal = {Review of Education, Pedagogy, and Cultural Studies},
	author = {Haas, Lynda},
	year = {2018},
	pages = {1--6},
	file = {Reservoir Computing Based on Cellular Automata (ReCA) in Sequence Learning:/home/thomaav/Zotero/storage/DUX84DZZ/Haas - 1995 - Reprints available directly from the publisher Pho.pdf:application/pdf}
}

@article{krishnagopal_generalized_nodate,
	title = {Generalized {Learning} with {Reservoir} {Computing}},
	language = {en},
	author = {Krishnagopal, Sanjukta and Aloimonos, Yiannis},
	pages = {21},
	file = {Krishnagopal and Aloimonos - Generalized Learning with Reservoir Computing.pdf:/home/thomaav/Zotero/storage/7A9ZMFET/Krishnagopal and Aloimonos - Generalized Learning with Reservoir Computing.pdf:application/pdf}
}

@article{wootton_optimizing_2017,
	title = {Optimizing {Echo} {State} {Networks} for {Static} {Pattern} {Recognition}},
	volume = {9},
	issn = {1866-9956, 1866-9964},
	url = {http://link.springer.com/10.1007/s12559-017-9468-2},
	doi = {10.1007/s12559-017-9468-2},
	language = {en},
	number = {3},
	urldate = {2019-10-03},
	journal = {Cognitive Computation},
	author = {Wootton, Adam J. and Taylor, Sarah L. and Day, Charles R. and Haycock, Peter W.},
	month = jun,
	year = {2017},
	pages = {391--399},
	file = {Wootton et al. - 2017 - Optimizing Echo State Networks for Static Pattern .pdf:/home/thomaav/Zotero/storage/R6YY8UTK/Wootton et al. - 2017 - Optimizing Echo State Networks for Static Pattern .pdf:application/pdf}
}

@article{tsunegi_physical_2019,
	title = {Physical reservoir computing based on spin torque oscillator with forced synchronization},
	volume = {114},
	issn = {0003-6951, 1077-3118},
	url = {http://aip.scitation.org/doi/10.1063/1.5081797},
	doi = {10.1063/1.5081797},
	abstract = {We investigated physical reservoir computing (RC) using a vortex-type spin torque oscillator (STO) as a resource of nonlinear dynamics, which is essential for processing information in time-series data. Forced synchronization was used to suppress the thermal ﬂuctuation of the oscillation trajectory of the STO. We examined the memory property of the STO dynamics, called short-term memory (STM), by using a virtual node technique. The STM capacity increased about twofold compared with that obtained without forced synchronization. The performance index for the nonlinear transformation of the STO also increased; it was evaluated in a parity-check task. The results prove that the synchronized STO has great potential for physical RC based on nanotechnology.},
	language = {en},
	number = {16},
	urldate = {2019-10-03},
	journal = {Applied Physics Letters},
	author = {Tsunegi, Sumito and Taniguchi, Tomohiro and Nakajima, Kohei and Miwa, Shinji and Yakushiji, Kay and Fukushima, Akio and Yuasa, Shinji and Kubota, Hitoshi},
	month = apr,
	year = {2019},
	pages = {164101},
	file = {Tsunegi et al. - 2019 - Physical reservoir computing based on spin torque .pdf:/home/thomaav/Zotero/storage/XL7GLJ7Z/Tsunegi et al. - 2019 - Physical reservoir computing based on spin torque .pdf:application/pdf}
}

@inproceedings{dat_tran_memcapacitive_2017,
	address = {Newport, RI, USA},
	title = {Memcapacitive reservoir computing},
	isbn = {978-1-5090-6037-5},
	url = {http://ieeexplore.ieee.org/document/8053719/},
	doi = {10.1109/NANOARCH.2017.8053719},
	abstract = {Memristors have successfully been used to build efﬁcient reservoir computers. The power consumption of memristive reservoirs, however, is bounded by the resistive nature of such devices. Here, we show that memcapacitors, another device in the mem-device family, offer great promise for power-efﬁcient reservoir computers. We simulated memcapacitive reservoirs with two different device models and benchmarked them with the NARMA-30 and the MNIST task. The results were compared to two memristive reservoirs as well as to a software echo state network. The memcapacitive reservoirs achieved comparable performance as the memcapacitive reservoirs but reduced the power consumption by about a factor of 500× for both tasks. We argue that memcapacitive reservoirs thus have great potential for low-power neuromorphic applications.},
	language = {en},
	urldate = {2019-10-03},
	booktitle = {2017 {IEEE}/{ACM} {International} {Symposium} on {Nanoscale} {Architectures} ({NANOARCH})},
	publisher = {IEEE},
	author = {Dat Tran, S. J. and Teuscher, Christof},
	month = jul,
	year = {2017},
	pages = {115--116},
	file = {Dat Tran and Teuscher - 2017 - Memcapacitive reservoir computing.pdf:/home/thomaav/Zotero/storage/5I5T652U/Dat Tran and Teuscher - 2017 - Memcapacitive reservoir computing.pdf:application/pdf}
}

@incollection{liu_waveform_2017,
	address = {Cham},
	title = {Waveform {Classification} by {Memristive} {Reservoir} {Computing}},
	volume = {10637},
	isbn = {978-3-319-70092-2 978-3-319-70093-9},
	url = {http://link.springer.com/10.1007/978-3-319-70093-9_48},
	abstract = {Reservoir computing is one of the computational frameworks based on recurrent neural networks for learning sequential data. We study the memristive reservoir computing where a network of memristors, instead of recurrent neural networks, provides a nonlinear mapping from input sequential signals to high-dimensional spatiotemporal dynamics. First we formulate the circuit equations of the memristive networks and describe the simulation methods. Then we use the memristive reservoir computing for solving a waveform classiﬁcation problem. We demonstrate how the classiﬁcation ability depends on the number of reservoir outputs and the variability of the memristive elements. Our methods are useful for ﬁnding a better architecture of the memristive reservoir under the inevitable element variability when implemented with nano/microscale devices.},
	language = {en},
	urldate = {2019-10-03},
	booktitle = {Neural {Information} {Processing}},
	publisher = {Springer International Publishing},
	author = {Tanaka, Gouhei and Nakane, Ryosho and Yamane, Toshiyuki and Takeda, Seiji and Nakano, Daiju and Nakagawa, Shigeru and Hirose, Akira},
	editor = {Liu, Derong and Xie, Shengli and Li, Yuanqing and Zhao, Dongbin and El-Alfy, El-Sayed M.},
	year = {2017},
	doi = {10.1007/978-3-319-70093-9_48},
	pages = {457--465},
	file = {Tanaka et al. - 2017 - Waveform Classification by Memristive Reservoir Co.pdf:/home/thomaav/Zotero/storage/QDCWMPL8/Tanaka et al. - 2017 - Waveform Classification by Memristive Reservoir Co.pdf:application/pdf}
}

@article{suarez_evaluation_2018,
	title = {Evaluation of the computational capabilities of a memristive random network ({MN3}) under the context of reservoir computing},
	volume = {106},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608018302028},
	doi = {10.1016/j.neunet.2018.07.003},
	abstract = {This work presents the simulation results of a novel recurrent, memristive neuromorphic architecture, the MN3 and explores its computational capabilities in the performance of a temporal pattern recognition task by considering the principles of the reservoir computing approach. A simple methodology based on the definitions of ordered and chaotic dynamical systems was used to determine the separation and fading memory properties of the architecture. The results show the potential use of this architecture as a reservoir for the on-line processing of time-varying inputs.},
	language = {en},
	urldate = {2019-10-03},
	journal = {Neural Networks},
	author = {Suarez, Laura E. and Kendall, Jack D. and Nino, Juan C.},
	month = oct,
	year = {2018},
	keywords = {Printed},
	pages = {223--236},
	file = {Suarez et al. - 2018 - Evaluation of the computational capabilities of a .pdf:/home/thomaav/Zotero/storage/23E8V7B9/Suarez et al. - 2018 - Evaluation of the computational capabilities of a .pdf:application/pdf}
}

@article{seoane_evolutionary_2018,
	title = {Evolutionary aspects of {Reservoir} {Computing}},
	url = {http://arxiv.org/abs/1810.04672},
	abstract = {Reservoir Computing (RC) is a powerful computational paradigm that allows high versatility with cheap learning. While other artiﬁcial intelligence approaches need exhaustive resources to specify their inner workings, RC is based on a reservoir with highly non-linear dynamics that does not require a ﬁne tuning of its parts. These dynamics project input signals into high-dimensional spaces, where training linear readouts to extract input features is vastly simpliﬁed. Thus, inexpensive learning provides very powerful tools for decision making, controlling dynamical systems, classiﬁcation, etc. RC also facilitates solving multiple tasks in parallel, resulting in a high throughput. Existing literature focuses on applications in artiﬁcial intelligence and neuroscience. We review this literature from an evolutionary perspective. RC’s versatility make it a great candidate to solve outstanding problems in biology, which raises relevant questions: Is RC as abundant in Nature as its advantages should imply? Has it evolved? Once evolved, can it be easily sustained? Under what circumstances? (In other words, is RC an evolutionarily stable computing paradigm?) To tackle these issues we introduce a conceptual morphospace that would map computational selective pressures that could select for or against RC and other computing paradigms. This guides a speculative discussion about the questions above and allows us to propose a solid research line that brings together computation and evolution with RC as a working bench.},
	language = {en},
	urldate = {2019-10-03},
	journal = {arXiv:1810.04672 [cond-mat, physics:nlin]},
	author = {Seoane, Luís F.},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.04672},
	keywords = {Nonlinear Sciences - Adaptation and Self-Organizing Systems, Condensed Matter - Disordered Systems and Neural Networks},
	file = {Seoane - 2018 - Evolutionary aspects of Reservoir Computing.pdf:/home/thomaav/Zotero/storage/XUY4UMRY/Seoane - 2018 - Evolutionary aspects of Reservoir Computing.pdf:application/pdf}
}

@article{prychynenko_magnetic_2018,
	title = {A magnetic skyrmion as a non-linear resistive element - a potential building block for reservoir computing},
	volume = {9},
	issn = {2331-7019},
	url = {http://arxiv.org/abs/1702.04298},
	doi = {10.1103/PhysRevApplied.9.014034},
	abstract = {Inspired by the human brain, there is a strong effort to ﬁnd alternative models of information processing capable of imitating the high energy efﬁciency of neuromorphic information processing. One possible realization of cognitive computing are reservoir computing networks. These networks are built out of non-linear resistive elements which are recursively connected. We propose that a skyrmion network embedded in frustrated magnetic ﬁlms may provide a suitable physical implementation for reservoir computing applications. The signiﬁcant key ingredient of such a network is a two-terminal device with non-linear voltage characteristics originating from single-layer magnetoresistive effects, like the anisotropic magnetoresistance or the recently discovered non-collinear magnetoresistance. The most basic element for a reservoir computing network built from “skyrmion fabrics” is a single skyrmion embedded in a ferromagnetic ribbon. In order to pave the way towards reservoir computing systems based on skyrmion fabrics, here we simulate and analyze i) the current ﬂow through a single magnetic skyrmion due to the anisotropic magneto-resistive effect and ii) the combined physics of local pinning and the anisotropic magneto-resistive effect.},
	language = {en},
	number = {1},
	urldate = {2019-10-03},
	journal = {Physical Review Applied},
	author = {Prychynenko, Diana and Sitte, Matthias and Litzius, Kai and Krüger, Benjamin and Bourianoff, George and Kläui, Mathias and Sinova, Jairo and Everschor-Sitte, Karin},
	month = jan,
	year = {2018},
	note = {arXiv: 1702.04298},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Other Condensed Matter, Condensed Matter - Strongly Correlated Electrons},
	pages = {014034},
	file = {Prychynenko et al. - 2018 - A magnetic skyrmion as a non-linear resistive elem.pdf:/home/thomaav/Zotero/storage/7TETPZNA/Prychynenko et al. - 2018 - A magnetic skyrmion as a non-linear resistive elem.pdf:application/pdf}
}

@article{pinna_reservoir_2018,
	title = {Reservoir {Computing} with {Random} {Skyrmion} {Textures}},
	url = {http://arxiv.org/abs/1811.12623},
	abstract = {The Reservoir Computing (RC) paradigm posits that sufficiently complex physical systems can be used to massively simplify pattern recognition tasks and nonlinear signal prediction. This work demonstrates how random topological magnetic textures present sufficiently complex resistance responses for the implementation of RC as applied to A/C current pulses. In doing so, we stress how the applicability of this paradigm hinges on very general dynamical properties which are satisfied by a large class of physical systems where complexity can be put to computational use. By harnessing the complex resistance response exhibited by random magnetic skyrmion textures and using it to demonstrate pattern recognition, we explain how spintronically accessible magnetic systems offer an advantage in the search for an ideal reservoir computer. The low-power properties of compact skyrmion fabrics, coupled with their CMOS integrability operating on similar length and timescales, open the door for their RC employment geared towards industrial application.},
	language = {en},
	urldate = {2019-10-03},
	journal = {arXiv:1811.12623 [cond-mat]},
	author = {Pinna, Daniele and Bourianoff, George and Everschor-Sitte, Karin},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.12623},
	keywords = {Condensed Matter - Mesoscale and Nanoscale Physics},
	file = {Pinna et al. - 2018 - Reservoir Computing with Random Skyrmion Textures.pdf:/home/thomaav/Zotero/storage/RED7PMY3/Pinna et al. - 2018 - Reservoir Computing with Random Skyrmion Textures.pdf:application/pdf}
}

@article{nomura_reservoir_2019,
	title = {Reservoir computing with dipole-coupled nanomagnets},
	volume = {58},
	issn = {0021-4922, 1347-4065},
	url = {http://arxiv.org/abs/1810.13140},
	doi = {10.7567/1347-4065/ab2406},
	abstract = {The feasibility of reservoir computing based on dipole-coupled nanomagnets is demonstrated using micro-magnetic simulations. The reservoir consists of an 2x10 array of nanomagnets. The static-magnetization directions of the nanomagnets are used as reservoir states. To update these states, we change the magnetization of one nanomagnet according to a single-bit-sequential signal. We also change the uniaxial anisotropy of the other nanomagnets using a voltage-induced magnetic-anisotropy change to enhance information flow, storage, and linear/nonlinear calculations. Binary tasks with AND, OR, and XOR operations were performed to evaluate the performance of the magnetic-array reservoir. The reservoir-computing output matrix was found to be trainable to perform AND, OR, and XOR operations with an input delay of up to three bits.},
	language = {en},
	number = {7},
	urldate = {2019-10-03},
	journal = {Japanese Journal of Applied Physics},
	author = {Nomura, Hikaru and Furuta, Taishi and Tsujimoto, Kazuki and Kuwabiraki, Yuki and Peper, Ferdinand and Tamura, Eiiti and Miwa, Shinji and Goto, Minori and Nakatani, Ryoichi and Suzuki, Yoshishige},
	month = jul,
	year = {2019},
	note = {arXiv: 1810.13140},
	keywords = {Computer Science - Emerging Technologies, Condensed Matter - Mesoscale and Nanoscale Physics},
	pages = {070901},
	file = {Nomura et al. - 2019 - Reservoir computing with dipole-coupled nanomagnet.pdf:/home/thomaav/Zotero/storage/4UTQRSME/Nomura et al. - 2019 - Reservoir computing with dipole-coupled nanomagnet.pdf:application/pdf}
}

@article{nikiforou_investigation_2017,
	title = {An {Investigation} of the {Dynamical} {Transitions} in {Harmonically} {Driven} {Random} {Networks} of {Firing}-{Rate} {Neurons}},
	volume = {9},
	issn = {1866-9956, 1866-9964},
	url = {http://link.springer.com/10.1007/s12559-017-9464-6},
	doi = {10.1007/s12559-017-9464-6},
	language = {en},
	number = {3},
	urldate = {2019-10-03},
	journal = {Cognitive Computation},
	author = {Nikiforou, Kyriacos and Mediano, Pedro A. M. and Shanahan, Murray},
	month = jun,
	year = {2017},
	pages = {351--363},
	file = {Nikiforou et al. - 2017 - An Investigation of the Dynamical Transitions in H.pdf:/home/thomaav/Zotero/storage/8PAYFCNT/Nikiforou et al. - 2017 - An Investigation of the Dynamical Transitions in H.pdf:application/pdf}
}

@article{nakajima_boosting_2019,
	title = {Boosting computational power through spatial multiplexing in quantum reservoir computing},
	volume = {11},
	issn = {2331-7019},
	url = {http://arxiv.org/abs/1803.04574},
	doi = {10.1103/PhysRevApplied.11.034021},
	abstract = {Quantum reservoir computing provides a framework for exploiting the natural dynamics of quantum systems as a computational resource. It can implement real-time signal processing and solve temporal machine learning problems in general, which requires memory and nonlinear mapping of the recent input stream using the quantum dynamics in computational supremacy region, where the classical simulation of the system is intractable. A nuclear magnetic resonance spin-ensemble system is one of the realistic candidates for such physical implementations, which is currently available in laboratories. In this paper, considering these realistic experimental constraints for implementing the framework, we introduce a scheme, which we call a spatial multiplexing technique, to effectively boost the computational power of the platform. This technique exploits disjoint dynamics, which originate from multiple different quantum systems driven by common input streams in parallel. Accordingly, unlike designing a single large quantum system to increase the number of qubits for computational nodes, it is possible to prepare a huge number of qubits from multiple but small quantum systems, which are operationally easy to handle in laboratory experiments. We numerically demonstrate the effectiveness of the technique using several benchmark tasks and quantitatively investigate its specifications, range of validity, and limitations in detail.},
	language = {en},
	number = {3},
	urldate = {2019-10-03},
	journal = {Physical Review Applied},
	author = {Nakajima, Kohei and Fujii, Keisuke and Negoro, Makoto and Mitarai, Kosuke and Kitagawa, Masahiro},
	month = mar,
	year = {2019},
	note = {arXiv: 1803.04574},
	keywords = {Quantum Physics},
	pages = {034021},
	file = {Nakajima et al. - 2019 - Boosting computational power through spatial multi.pdf:/home/thomaav/Zotero/storage/RJMX6LQB/Nakajima et al. - 2019 - Boosting computational power through spatial multi.pdf:application/pdf}
}

@incollection{shigeno_muscular-hydrostat_2017,
	address = {Tokyo},
	title = {Muscular-{Hydrostat} {Computers}: {Physical} {Reservoir} {Computing} for {Octopus}-{Inspired} {Soft} {Robots}},
	isbn = {978-4-431-56467-6 978-4-431-56469-0},
	shorttitle = {Muscular-{Hydrostat} {Computers}},
	url = {http://link.springer.com/10.1007/978-4-431-56469-0_18},
	abstract = {The octopus has been one of the major sources of inspiration for roboticists for many years. It can harness its complex body dynamics in a highly sophisticated manner despite the fact that its body does not contain any rigid components through the interaction between the characteristic organization of its nervous system and its speciﬁc body morphology and muscle characteristics, called muscular-hydrostats. Inspired by these biological ﬁndings, we investigated the potential information processing capacity of its soft body dynamics with the help of a machine learning approach called reservoir computing. We review a series of concepts and platforms, called muscular-hydrostat computers throughout our study and suggest that the diverse body dynamics of soft materials can be exploited as a computational resource. This approach could enable some controls to be embedded into the robot body.},
	language = {en},
	urldate = {2019-10-03},
	booktitle = {Brain {Evolution} by {Design}},
	publisher = {Springer Japan},
	author = {Nakajima, Kohei},
	editor = {Shigeno, Shuichi and Murakami, Yasunori and Nomura, Tadashi},
	year = {2017},
	doi = {10.1007/978-4-431-56469-0_18},
	pages = {403--414},
	file = {Nakajima - 2017 - Muscular-Hydrostat Computers Physical Reservoir C.pdf:/home/thomaav/Zotero/storage/E8TSBCZH/Nakajima - 2017 - Muscular-Hydrostat Computers Physical Reservoir C.pdf:application/pdf}
}

@article{moran_reservoir_2018,
	title = {Reservoir {Computing} {Hardware} with {Cellular} {Automata}},
	url = {http://arxiv.org/abs/1806.04932},
	abstract = {Elementary cellular automata (ECA) is a widely studied one-dimensional processing methodology where the successive iteration of the automaton may lead to the recreation of a rich pattern dynamic. Recently, cellular automata have been proposed as a feasible way to implement Reservoir Computing (RC) systems in which the automata rule is ﬁxed and the training is performed using a linear regression. In this work we perform an exhaustive study of the performance of the diﬀerent ECA rules when applied to pattern recognition of time-independent input signals using a RC scheme. Once the diﬀerent ECA rules have been tested, the most accurate one (rule 90) is selected to implement a digital circuit. Rule 90 is easily reproduced using a reduced set of XOR gates and shift-registers, thus representing a high-performance alternative for RC hardware implementation in terms of processing time, circuit area, power dissipation and system accuracy. The model (both in software and its hardware implementation) has been tested using a pattern recognition task of handwritten numbers (the MNIST database) for which we obtained competitive results in terms of accuracy, speed and power dissipation. The proposed model can be considered to be a low-cost method to implement fast pattern recognition digital circuits.},
	language = {en},
	urldate = {2019-10-03},
	journal = {arXiv:1806.04932 [nlin]},
	author = {Morán, Alejandro and Frasser, Christiam F. and Rosselló, Josep L.},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.04932},
	keywords = {Computer Science - Neural and Evolutionary Computing, Nonlinear Sciences - Cellular Automata and Lattice Gases, Computer Science - Computer Vision and Pattern Recognition},
	file = {Morán et al. - 2018 - Reservoir Computing Hardware with Cellular Automat.pdf:/home/thomaav/Zotero/storage/FCJGDXHH/Morán et al. - 2018 - Reservoir Computing Hardware with Cellular Automat.pdf:application/pdf}
}

@article{ma_deep-esn:_2017,
	title = {Deep-{ESN}: {A} {Multiple} {Projection}-encoding {Hierarchical} {Reservoir} {Computing} {Framework}},
	shorttitle = {Deep-{ESN}},
	url = {http://arxiv.org/abs/1711.05255},
	abstract = {As an efﬁcient recurrent neural network (RNN) model, reservoir computing (RC) models, such as Echo State Networks, have attracted widespread attention in the last decade. However, while they have had great success with time series data [1], [2], many time series have a multiscale structure, which a single-hidden-layer RC model may have difﬁculty capturing. In this paper, we propose a novel hierarchical reservoir computing framework we call Deep Echo State Networks (Deep-ESNs). The most distinctive feature of a Deep-ESN is its ability to deal with time series through hierarchical projections. Speciﬁcally, when an input time series is projected into the high-dimensional echo-state space of a reservoir, a subsequent encoding layer (e.g., a PCA, autoencoder, or a random projection) can project the echo-state representations into a lower-dimensional space. These low-dimensional representations can then be processed by another ESN. By using projection layers and encoding layers alternately in the hierarchical framework, a Deep-ESN can not only attenuate the effects of the collinearity problem in ESNs, but also fully take advantage of the temporal kernel property of ESNs to explore multiscale dynamics of time series. To fuse the multiscale representations obtained by each reservoir, we add connections from each encoding layer to the last output layer. Theoretical analyses prove that stability of a Deep-ESN is guaranteed by the echo state property (ESP), and the time complexity is equivalent to a conventional ESN. Experimental results on some artiﬁcial and real world time series demonstrate that Deep-ESNs can capture multiscale dynamics, and outperform both standard ESNs and previous hierarchical ESN-based models.},
	language = {en},
	urldate = {2019-10-03},
	journal = {arXiv:1711.05255 [cs]},
	author = {Ma, Qianli and Shen, Lifeng and Cottrell, Garrison W.},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.05255},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Ma et al. - 2017 - Deep-ESN A Multiple Projection-encoding Hierarchi.pdf:/home/thomaav/Zotero/storage/8QGY7232/Ma et al. - 2017 - Deep-ESN A Multiple Projection-encoding Hierarchi.pdf:application/pdf}
}

@article{markovic_reservoir_2019,
	title = {Reservoir computing with the frequency, phase and amplitude of spin-torque nano-oscillators},
	volume = {114},
	issn = {0003-6951, 1077-3118},
	url = {http://arxiv.org/abs/1811.00309},
	doi = {10.1063/1.5079305},
	abstract = {Spin-torque nano-oscillators can emulate neurons at the nanoscale. Recent works show that the non-linearity of their oscillation amplitude can be leveraged to achieve waveform classification for an input signal encoded in the amplitude of the input voltage. Here we show that the frequency and the phase of the oscillator can also be used to recognize waveforms. For this purpose, we phase-lock the oscillator to the input waveform, which carries information in its modulated frequency. In this way we considerably decrease amplitude, phase and frequency noise. We show that this method allows classifying sine and square waveforms with an accuracy above 99\% when decoding the output from the oscillator amplitude, phase or frequency. We find that recognition rates are directly related to the noise and non-linearity of each variable. These results prove that spin-torque nano-oscillators offer an interesting platform to implement different computing schemes leveraging their rich dynamical features.},
	language = {en},
	number = {1},
	urldate = {2019-10-03},
	journal = {Applied Physics Letters},
	author = {Marković, Danijela and Leroux, Nathan and Riou, Mathieu and Araujo, Flavio Abreu and Torrejon, Jacob and Querlioz, Damien and Fukushima, Akio and Yuasa, Shinji and Trastoy, Juan and Bortolotti, Paolo and Grollier, Julie},
	month = jan,
	year = {2019},
	note = {arXiv: 1811.00309},
	keywords = {Condensed Matter - Mesoscale and Nanoscale Physics, Physics - Applied Physics, Physics - Computational Physics},
	pages = {012409},
	file = {Marković et al. - 2019 - Reservoir computing with the frequency, phase and .pdf:/home/thomaav/Zotero/storage/9XD2MTIE/Marković et al. - 2019 - Reservoir computing with the frequency, phase and .pdf:application/pdf}
}

@article{larger_high-speed_2017,
	title = {High-{Speed} {Photonic} {Reservoir} {Computing} {Using} a {Time}-{Delay}-{Based} {Architecture}: {Million} {Words} per {Second} {Classification}},
	volume = {7},
	issn = {2160-3308},
	shorttitle = {High-{Speed} {Photonic} {Reservoir} {Computing} {Using} a {Time}-{Delay}-{Based} {Architecture}},
	url = {https://link.aps.org/doi/10.1103/PhysRevX.7.011015},
	doi = {10.1103/PhysRevX.7.011015},
	language = {en},
	number = {1},
	urldate = {2019-10-03},
	journal = {Physical Review X},
	author = {Larger, Laurent and Baylón-Fuentes, Antonio and Martinenghi, Romain and Udaltsov, Vladimir S. and Chembo, Yanne K. and Jacquot, Maxime},
	month = feb,
	year = {2017},
	pages = {011015},
	file = {Larger et al. - 2017 - High-Speed Photonic Reservoir Computing Using a Ti.pdf:/home/thomaav/Zotero/storage/DTJGPK5U/Larger et al. - 2017 - High-Speed Photonic Reservoir Computing Using a Ti.pdf:application/pdf}
}

@article{katumba_neuromorphic_2018,
	title = {Neuromorphic {Computing} {Based} on {Silicon} {Photonics} and {Reservoir} {Computing}},
	volume = {24},
	issn = {1077-260X, 1558-4542},
	url = {https://ieeexplore.ieee.org/document/8331848/},
	doi = {10.1109/JSTQE.2018.2821843},
	abstract = {We present our latest progress using new neuromorphic paradigms for optical information processing in silicon photonics. We show how passive reservoir computing chips can be used to perform a variety of tasks (bit level tasks, nonlinear dispersion compensation, etc.) at high speeds and low power consumption. In addition, we present a spatial analog of reservoir computing based on pillar scatterers and a cavity that can be used to speed up classiﬁcation of biological cells.},
	language = {en},
	number = {6},
	urldate = {2019-10-03},
	journal = {IEEE Journal of Selected Topics in Quantum Electronics},
	author = {Katumba, Andrew and Freiberger, Matthias and Laporte, Floris and Lugnan, Alessio and Sackesyn, Stijn and Ma, Chonghuai and Dambre, Joni and Bienstman, Peter},
	month = nov,
	year = {2018},
	pages = {1--10},
	file = {Katumba et al. - 2018 - Neuromorphic Computing Based on Silicon Photonics .pdf:/home/thomaav/Zotero/storage/UTN6NWHA/Katumba et al. - 2018 - Neuromorphic Computing Based on Silicon Photonics .pdf:application/pdf}
}

@article{jiang_physical_2019,
	title = {Physical reservoir computing built by spintronic devices for temporal information processing},
	url = {http://arxiv.org/abs/1901.07879},
	abstract = {Spintronic nanodevices have ultrafast nonlinear dynamic and recurrence behaviors on a nanosecond scale that promises to enable spintronic reservoir computing (RC) system. Here two physical RC systems based on a single magnetic skyrmion memristor (MSM) and 24 spin-torque nano-oscillators (STNOs) were proposed and modeled to process image classification task and nonlinear dynamic system prediction, respectively. Based on our micromagnetic simulation results on the nonlinear responses of MSM and STNO with current pulses stimulation, the handwritten digits recognition task domesticates that an RC system using one single MSM has the outstanding performance on image classification. In addition, the complex unknown nonlinear dynamic problems can also be well solved by a physical RC system consisted of 24 STNOs confirmed in a second-order nonlinear dynamic system and NARMA10 tasks. The capability of both high accuracy and fast information processing promises to enable one type of brain-like chip based on spintronics for various artificial intelligence tasks.},
	language = {en},
	urldate = {2019-10-03},
	journal = {arXiv:1901.07879 [physics]},
	author = {Jiang, Wencong and Chen, Lina and Zhou, Kaiyuan and Li, Liyuan and Fu, Qingwei and Du, Youwei and Liu, Ronghua},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.07879},
	keywords = {Computer Science - Emerging Technologies, Physics - Applied Physics},
	file = {Jiang et al. - 2019 - Physical reservoir computing built by spintronic d.pdf:/home/thomaav/Zotero/storage/DQI5KUZ2/Jiang et al. - 2019 - Physical reservoir computing built by spintronic d.pdf:application/pdf}
}

@article{inubushi_reservoir_2017,
	title = {Reservoir {Computing} {Beyond} {Memory}-{Nonlinearity} {Trade}-off},
	volume = {7},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/s41598-017-10257-6},
	doi = {10.1038/s41598-017-10257-6},
	language = {en},
	number = {1},
	urldate = {2019-10-03},
	journal = {Scientific Reports},
	author = {Inubushi, Masanobu and Yoshimura, Kazuyuki},
	month = dec,
	year = {2017},
	pages = {10199},
	file = {Inubushi and Yoshimura - 2017 - Reservoir Computing Beyond Memory-Nonlinearity Tra.pdf:/home/thomaav/Zotero/storage/4CTU4WIU/Inubushi and Yoshimura - 2017 - Reservoir Computing Beyond Memory-Nonlinearity Tra.pdf:application/pdf}
}

@article{hadaeghi_computing_2019,
	title = {Computing optimal discrete readout weights in reservoir computing is {NP}-hard},
	volume = {338},
	issn = {09252312},
	url = {http://arxiv.org/abs/1809.01021},
	doi = {10.1016/j.neucom.2019.02.009},
	abstract = {We show NP-hardness of a generalized quadratic programming problem, which we called unconstrained n-ary quadratic programming (UNQP). This problem has recently become practically relevant in the context of novel memristor-based neuromorphic microchip designs, where solving the UNQP is a key operation for on-chip training of the neural network implemented on the chip. UNQP is the problem of ﬁnding a vector v ∈ SN which minimizes vT Q v + vT c, where S = \{s1, . . . , sn\} ⊂ Z is a given set of eligible parameters for v, Q ∈ ZN×N is positive semi-deﬁnite, and c ∈ ZN . In memristor-based neuromorphic hardware, S is physically given by a ﬁnite (and small) number of possible memristor states. The proof of NP-hardness is by reduction from the unconstrained binary quadratic programming problem, which is a special case of UNQP where S = \{0, 1\} and which is known to be NP-hard.},
	language = {en},
	urldate = {2019-10-03},
	journal = {Neurocomputing},
	author = {Hadaeghi, Fatemeh and Jaeger, Herbert},
	month = apr,
	year = {2019},
	note = {arXiv: 1809.01021},
	keywords = {Computer Science - Computational Complexity},
	pages = {233--236},
	file = {Hadaeghi and Jaeger - 2019 - Computing optimal discrete readout weights in rese.pdf:/home/thomaav/Zotero/storage/DF9QZNMY/Hadaeghi and Jaeger - 2019 - Computing optimal discrete readout weights in rese.pdf:application/pdf}
}

@article{du_reservoir_2017,
	title = {Reservoir computing using dynamic memristors for temporal information processing},
	volume = {8},
	issn = {2041-1723},
	url = {http://www.nature.com/articles/s41467-017-02337-y},
	doi = {10.1038/s41467-017-02337-y},
	language = {en},
	number = {1},
	urldate = {2019-10-03},
	journal = {Nature Communications},
	author = {Du, Chao and Cai, Fuxi and Zidan, Mohammed A. and Ma, Wen and Lee, Seung Hwan and Lu, Wei D.},
	month = dec,
	year = {2017},
	pages = {2204},
	file = {Du et al. - 2017 - Reservoir computing using dynamic memristors for t.pdf:/home/thomaav/Zotero/storage/YPY7U6FF/Du et al. - 2017 - Reservoir computing using dynamic memristors for t.pdf:application/pdf}
}

@article{coarer_all-optical_2018,
	title = {All-{Optical} {Reservoir} {Computing} on a {Photonic} {Chip} {Using} {Silicon}-{Based} {Ring} {Resonators}},
	volume = {24},
	issn = {1077-260X, 1558-4542},
	url = {https://ieeexplore.ieee.org/document/8359197/},
	doi = {10.1109/JSTQE.2018.2836985},
	abstract = {We present in our work numerical results on the performance of a 4 × 4 swirl-topology photonic reservoir integrated on a silicon chip. Nonlinear microring resonators are used as nodes. We analyse the performance of such a reservoir on a classical nonlinear boolean task (the delayed XOR task) for (i) various designs of the reservoir in terms of lengths of the waveguides between consecutive nodes, and (ii) various injection parameters (injected power and optical detuning). From this analysis, we ﬁnd that this kind of reservoir can perform - for a large variety of parameters - the delayed XOR task at 20 Gb/s with bit error rates lower than 10−3, and an averaged injection power lower than 2.5 mW.},
	language = {en},
	number = {6},
	urldate = {2019-10-03},
	journal = {IEEE Journal of Selected Topics in Quantum Electronics},
	author = {Coarer, Florian Denis-Le and Sciamanna, Marc and Katumba, Andrew and Freiberger, Matthias and Dambre, Joni and Bienstman, Peter and Rontani, Damien},
	month = nov,
	year = {2018},
	pages = {1--8},
	file = {Coarer et al. - 2018 - All-Optical Reservoir Computing on a Photonic Chip.pdf:/home/thomaav/Zotero/storage/H8JAHE9E/Coarer et al. - 2018 - All-Optical Reservoir Computing on a Photonic Chip.pdf:application/pdf}
}

@incollection{adamatzky_reservoir_2017,
	address = {Cham},
	title = {Reservoir {Computing} as a {Model} for {In}-{Materio} {Computing}},
	volume = {22},
	isbn = {978-3-319-33923-8 978-3-319-33924-5},
	url = {http://link.springer.com/10.1007/978-3-319-33924-5_22},
	abstract = {Research in substrate-based computing has shown that materials contain rich properties that can be exploited to solve computational problems. One such technique known as Evolution-in-materio uses evolutionary algorithms to manipulate material substrates for computation. However, in general, modelling the computational processes occurring in such systems is a difﬁcult task and understanding what part of the embodied system is doing the computation is still fairly ill-deﬁned. This chapter discusses the prospects of using Reservoir Computing as a model for in-materio computing, introducing new training techniques (taken from Reservoir Computing) that could overcome training difﬁculties found in the current Evolutionin-Materio technique.},
	language = {en},
	urldate = {2019-10-03},
	booktitle = {Advances in {Unconventional} {Computing}},
	publisher = {Springer International Publishing},
	author = {Dale, Matthew and Miller, Julian F. and Stepney, Susan},
	editor = {Adamatzky, Andrew},
	year = {2017},
	doi = {10.1007/978-3-319-33924-5_22},
	keywords = {Favorite},
	pages = {533--571},
	file = {Dale et al. - 2017 - Reservoir Computing as a Model for In-Materio Comp.pdf:/home/thomaav/Zotero/storage/LIDHJJLX/Dale et al. - 2017 - Reservoir Computing as a Model for In-Materio Comp.pdf:application/pdf}
}

@inproceedings{goudarzi_reservoir_2016,
	address = {New York, NY, USA},
	title = {Reservoir {Computing}: {Quo} {Vadis}?},
	isbn = {978-1-4503-4061-8},
	shorttitle = {Reservoir {Computing}},
	url = {http://dl.acm.org/citation.cfm?doid=2967446.2967448},
	doi = {10.1145/2967446.2967448},
	abstract = {Reservoir Computing (RC) is an umbrella term for adaptive computational paradigms that rely on an excitable dynamical system, also called the ”reservoir.” The paradigms have been shown to be particularly promising for temporal signal processing. RC was also explored as a potential candidate for emerging nanoscale architectures. In this article we reﬂect on the current state of RC and muse about its future. In particular, we propose a set of open problems that we think need to be addressed in order to make RC more mainstream.},
	language = {en},
	urldate = {2019-10-02},
	booktitle = {Proceedings of the 3rd {ACM} {International} {Conference} on {Nanoscale} {Computing} and {Communication} - {NANOCOM}'16},
	publisher = {ACM Press},
	author = {Goudarzi, Alireza and Teuscher, Christof},
	year = {2016},
	pages = {1--6},
	file = {Goudarzi and Teuscher - 2016 - Reservoir Computing Quo Vadis.pdf:/home/thomaav/Zotero/storage/TW7ZHEW9/Goudarzi and Teuscher - 2016 - Reservoir Computing Quo Vadis.pdf:application/pdf}
}

@article{dale_substrate-independent_2019,
	title = {A {Substrate}-{Independent} {Framework} to {Characterise} {Reservoir} {Computers}},
	volume = {475},
	issn = {1364-5021, 1471-2946},
	url = {http://arxiv.org/abs/1810.07135},
	doi = {10.1098/rspa.2018.0723},
	abstract = {The Reservoir Computing (RC) framework states that any non-linear, input-driven dynamical system (the reservoir ) exhibiting properties such as a fading memory and input separability can be trained to perform computational tasks. This broad inclusion of systems has led to many new physical substrates for RC. Properties essential for reservoirs to compute are tuned through reconﬁguration of the substrate, such as change in virtual topology or physical morphology. As a result, each substrate possesses a unique “quality” –obtained through reconﬁguration – to realise diﬀerent reservoirs for diﬀerent tasks. Here we describe an experimental framework to characterise the quality of potentially any substrate for RC. Our framework reveals that a deﬁnition of quality is not only useful to compare substrates, but can help map the non-trivial relationship between properties and task performance. In the wider context, the framework oﬀers a greater understanding as to what makes a dynamical system compute, helping improve the design of future substrates for RC.},
	language = {en},
	number = {2226},
	urldate = {2019-10-01},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Dale, Matthew and Miller, Julian F. and Stepney, Susan and Trefzer, Martin A.},
	month = jun,
	year = {2019},
	note = {arXiv: 1810.07135},
	keywords = {Computer Science - Emerging Technologies, Favorite, Read, Printed},
	pages = {20180723},
	file = {Dale et al. - 2019 - A Substrate-Independent Framework to Characterise .pdf:/home/thomaav/Zotero/storage/9NQCWD4W/Dale et al. - 2019 - A Substrate-Independent Framework to Characterise .pdf:application/pdf;Dale et al. - 2019 - A Substrate-Independent Framework to Characterise .pdf:/home/thomaav/Zotero/storage/H387Y9LP/Dale et al. - 2019 - A Substrate-Independent Framework to Characterise .pdf:application/pdf}
}

@article{rodan_minimum_2011,
	title = {Minimum {Complexity} {Echo} {State} {Network}},
	volume = {22},
	issn = {1045-9227, 1941-0093},
	url = {http://ieeexplore.ieee.org/document/5629375/},
	doi = {10.1109/TNN.2010.2089641},
	abstract = {Reservoir computing (RC) refers to a new class of state-space models with a ﬁxed state transition structure (the “reservoir”) and an adaptable readout form the state space. The reservoir is supposed to be sufﬁciently complex so as to capture a large number of features of the input stream that can be exploited by the reservoir-to-output readout mapping. The ﬁeld of RC has been growing rapidly with many successful applications. However, RC has been criticized for not being principled enough. Reservoir construction is largely driven by a series of randomized model building stages, with both researchers and practitioners having to rely on a series of trials and errors. To initialize a systematic study of the ﬁeld, we concentrate on one of the most popular classes of reservoir computing methods - Echo State Network (ESN) - and ask: What is the minimal complexity of reservoir construction for obtaining competitive models and what is the memory capacity of such simpliﬁed reservoirs? On a number of widely used time series benchmarks of different origin and characteristics, as well as by conducting a theoretical analysis we show: A simple deterministically constructed cycle reservoir is comparable to the standard echo state network methodology. The (short term) memory capacity of linear cyclic reservoirs can be made arbitrarily close to the proved optimal value.},
	language = {en},
	number = {1},
	urldate = {2019-09-18},
	journal = {IEEE Transactions on Neural Networks},
	author = {Rodan, A and Tino, P},
	month = jan,
	year = {2011},
	keywords = {Read, Printed},
	pages = {131--144},
	file = {Rodan and Tino - 2011 - Minimum Complexity Echo State Network.pdf:/home/thomaav/Zotero/storage/WU3F6I52/Rodan and Tino - 2011 - Minimum Complexity Echo State Network.pdf:application/pdf;Rodan and Tino - 2011 - Minimum Complexity Echo State Network.pdf:/home/thomaav/Zotero/storage/H4KGKMUN/Rodan and Tino - 2011 - Minimum Complexity Echo State Network.pdf:application/pdf}
}

@article{busing_connectivity_2010,
	title = {Connectivity, {Dynamics}, and {Memory} in {Reservoir} {Computing} with {Binary} and {Analog} {Neurons}},
	volume = {22},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/neco.2009.01-09-947},
	doi = {10.1162/neco.2009.01-09-947},
	language = {en},
	number = {5},
	urldate = {2019-10-01},
	journal = {Neural Computation},
	author = {Büsing, Lars and Schrauwen, Benjamin and Legenstein, Robert},
	month = may,
	year = {2010},
	keywords = {Printed},
	pages = {1272--1311},
	file = {Büsing et al. - 2010 - Connectivity, Dynamics, and Memory in Reservoir Co.pdf:/home/thomaav/Zotero/storage/H6HXHBAU/Büsing, Schrauwen, Legenstein - 2010 - Connectivity, dynamics, and memory in reservoir computing with binary and analog neurons.pdf:application/pdf;Büsing et al. - 2010 - Connectivity, Dynamics, and Memory in Reservoir Co.pdf:/home/thomaav/Zotero/storage/LGA3VR6Z/Büsing et al. - 2010 - Connectivity, Dynamics, and Memory in Reservoir Co.pdf:application/pdf;Büsing et al. - 2010 - Connectivity, Dynamics, and Memory in Reservoir Co.pdf:/home/thomaav/Zotero/storage/S85UPII4/Büsing et al. - 2010 - Connectivity, Dynamics, and Memory in Reservoir Co.pdf:application/pdf;Büsing et al. - 2010 - Connectivity, Dynamics, and Memory in Reservoir Co.pdf:/home/thomaav/Zotero/storage/X2PHHWFL/Büsing et al. - 2010 - Connectivity, Dynamics, and Memory in Reservoir Co.pdf:application/pdf}
}

@article{natschlager_information_2003,
	title = {Information {Dynamics} and {Emergent} {Computation} in {Recurrent} {Circuits} of {Spiking} {Neurons}},
	abstract = {We employ an efﬁcient method using Bayesian and linear classiﬁers for analyzing the dynamics of information in high-dimensional states of generic cortical microcircuit models. It is shown that such recurrent circuits of spiking neurons have an inherent capability to carry out rapid computations on complex spike patterns, merging information contained in the order of spike arrival with previously acquired context information.},
	language = {en},
	author = {Natschläger, Thomas and Maass, Wolfgang},
	year = {2003},
	keywords = {Favorite, Read, Printed},
	pages = {8},
	file = {Natschläger and Maass - Information Dynamics and Emergent Computation in R.pdf:/home/thomaav/Zotero/storage/8K23S8RS/Natschläger and Maass - Information Dynamics and Emergent Computation in R.pdf:application/pdf}
}

@article{jaeger_harnessing_2004,
	title = {Harnessing {Nonlinearity}: {Predicting} {Chaotic} {Systems} and {Saving} {Energy} in {Wireless} {Communication}},
	volume = {304},
	issn = {0036-8075, 1095-9203},
	shorttitle = {Harnessing {Nonlinearity}},
	url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1091277},
	doi = {10.1126/science.1091277},
	language = {en},
	number = {5667},
	urldate = {2019-09-18},
	journal = {Science},
	author = {Jaeger, H.},
	month = apr,
	year = {2004},
	keywords = {Read, Printed},
	pages = {78--80},
	file = {Jaeger - 2004 - Harnessing Nonlinearity Predicting Chaotic System.pdf:/home/thomaav/Zotero/storage/LNK6WYUI/Jaeger - 2004 - Harnessing Nonlinearity Predicting Chaotic System.pdf:application/pdf}
}

@article{hopfield_neural_1982,
	title = {Neural networks and physical systems with emergent collective computational abilities},
	author = {Hopfield},
	year = {1982},
	keywords = {Read, Printed},
	file = {hopfield.pdf:/home/thomaav/Zotero/storage/I83ZKJD5/hopfield.pdf:application/pdf}
}

@inproceedings{verstraeten_reservoir-based_2006,
	address = {Vancouver, BC, Canada},
	title = {Reservoir-based techniques for speech recognition},
	isbn = {978-0-7803-9490-2},
	url = {http://ieeexplore.ieee.org/document/1716215/},
	doi = {10.1109/IJCNN.2006.246804},
	abstract = {A solution for the slow convergence of most learning rules for Recurrent Neural Networks (RNN) has been proposed under the terms Liquid State Machines (LSM) and Echo State Networks (ESN). These methods use a RNN as a reservoir that is not trained. For this article we build upon previous work, where we used reservoir-based techniques to solve the task of isolated digit recognition. We present a straightforward improvement of our previous LSM-based implementation that results in an outperformance of a stateof-the-art Hidden Markov Model (HMM) based recognizer. Also, we apply the Echo State approach to the problem, which allows us to investigate the impact of several interconnection parameters on the performance of our speech recognizer.},
	language = {en},
	urldate = {2019-09-18},
	booktitle = {The 2006 {IEEE} {International} {Joint} {Conference} on {Neural} {Network} {Proceedings}},
	publisher = {IEEE},
	author = {Verstraeten, D. and Schrauwen, B. and Stroobandt, D.},
	year = {2006},
	keywords = {Read, Printed},
	pages = {1050--1053},
	file = {Verstraeten et al. - 2006 - Reservoir-based techniques for speech recognition.pdf:/home/thomaav/Zotero/storage/WVJVH7LA/Verstraeten et al. - 2006 - Reservoir-based techniques for speech recognition.pdf:application/pdf}
}

@article{atiya_new_2000,
	title = {New results on recurrent network training: unifying the algorithms and accelerating convergence},
	volume = {11},
	issn = {10459227},
	shorttitle = {New results on recurrent network training},
	url = {http://ieeexplore.ieee.org/document/846741/},
	doi = {10.1109/72.846741},
	abstract = {How to efficiently train recurrent networks remains a challenging and active research topic. Most of the proposed training approaches are based on computational ways to efficiently obtain the gradient of the error function, and can be generally grouped into five major groups. In this study we present a derivation that unifies these approaches. We demonstrate that the approaches are only five different ways of solving a particular matrix equation. The second goal of this paper is develop a new algorithm based on the insights gained from the novel formulation. The new algorithm, which is based on approximating the error gradient, has lower computational complexity in computing the weight update than the competing techniques for most typical problems. In addition, it reaches the error minimum in a much smaller number of iterations. A desirable characteristic of recurrent network training algorithms is to be able to update the weights in an on-line fashion. We have also developed an on-line version of the proposed algorithm, that is based on updating the error gradient approximation in a recursive manner.},
	language = {en},
	number = {3},
	urldate = {2019-09-18},
	journal = {IEEE Transactions on Neural Networks},
	author = {Atiya, A.F. and Parlos, A.G.},
	month = may,
	year = {2000},
	keywords = {Printed},
	pages = {697--709},
	file = {Atiya and Parlos - 2000 - New results on recurrent network training unifyin.pdf:/home/thomaav/Zotero/storage/3GFZQE76/Atiya and Parlos - 2000 - New results on recurrent network training unifyin.pdf:application/pdf}
}

@incollection{klir_science_1991,
	address = {Boston, MA},
	title = {Science and {Complexity}},
	isbn = {978-1-4899-0720-2 978-1-4899-0718-9},
	url = {http://link.springer.com/10.1007/978-1-4899-0718-9_30},
	language = {en},
	urldate = {2019-10-02},
	booktitle = {Facets of {Systems} {Science}},
	publisher = {Springer US},
	author = {Weaver, Warren},
	collaborator = {Klir, George J.},
	year = {1991},
	doi = {10.1007/978-1-4899-0718-9_30},
	keywords = {Read},
	pages = {449--456},
	file = {Weaver - 1991 - Science and Complexity.pdf:/home/thomaav/Zotero/storage/VR4X6D28/Weaver - 1991 - Science and Complexity.pdf:application/pdf}
}

@article{lloyd_measures_nodate,
	title = {Measures of {Complexity} a non--exhaustive list},
	language = {en},
	author = {Lloyd, Seth},
	keywords = {Read},
	pages = {3},
	file = {Lloyd - Measures of Complexity a non--exhaustive list.PDF:/home/thomaav/Zotero/storage/K2M3AYJ6/Lloyd - Measures of Complexity a non--exhaustive list.PDF:application/pdf}
}

@article{rodriguez_optimal_2019,
	title = {Optimal modularity and memory capacity of neural reservoirs},
	volume = {3},
	issn = {2472-1751},
	url = {https://www.mitpressjournals.org/doi/abs/10.1162/netn_a_00082},
	doi = {10.1162/netn_a_00082},
	abstract = {The neural network is a powerful computing framework that has been exploited by biological evolution and by humans for solving diverse problems. Although the computational capabilities of neural networks are determined by their structure, the current understanding of the relationships between a neural network’s architecture and function is still primitive. Here we reveal that a neural network’s modular architecture plays a vital role in determining the neural dynamics and memory performance of the network of threshold neurons. In particular, we demonstrate that there exists an optimal modularity for memory performance, where a balance between local cohesion and global connectivity is established, allowing optimally modular networks to remember longer. Our results suggest that insights from dynamical analysis of neural networks and information-spreading processes can be leveraged to better design neural networks and may shed light on the brain’s modular organization.},
	language = {en},
	number = {2},
	urldate = {2019-10-01},
	journal = {Network Neuroscience},
	author = {Rodriguez, Nathaniel and Izquierdo, Eduardo and Ahn, Yong-Yeol},
	month = jan,
	year = {2019},
	keywords = {Favorite, Printed},
	pages = {551--566},
	file = {Rodriguez et al. - 2019 - Optimal modularity and memory capacity of neural r.pdf:/home/thomaav/Zotero/storage/I4MWJYB2/Rodriguez et al. - 2019 - Optimal modularity and memory capacity of neural r.pdf:application/pdf}
}

@incollection{sidorov_stability_2010,
	address = {Berlin, Heidelberg},
	title = {Stability and {Topology} in {Reservoir} {Computing}},
	volume = {6438},
	isbn = {978-3-642-16772-0 978-3-642-16773-7},
	url = {http://link.springer.com/10.1007/978-3-642-16773-7_21},
	abstract = {Recently Jaeger and others have put forth the paradigm of "reservoir computing" as a way of computing with highly recurrent neural networks. This reservoir is a collection of neurons randomly connected with each other of fixed weights. Amongst other things, it has been shown to be effective in temporal pattern recognition; and has been held as a model appropriate to explain how certain aspects of the brain work. (Particularly in its guise as “liquid state machine”, due to Maass et al.) In this work we show that although it is known that this model does have generalizability properties and thus is robust to errors in input, it is NOT resistant to errors in the model itself. Thus small malfunctions or distortions make previous training ineffective. Thus this model as currently presented cannot be thought of as appropriate as a biological model; and it also suggests limitations on the applicability in the pattern recognition sphere. However, we show that, with the enforcement of topological constraints on the reservoir, in particular that of small world topology, the model is indeed fault tolerant. Thus this implies that "natural" computational systems must have specific topologies and the uniform random connectivity is not appropriate.},
	language = {en},
	urldate = {2019-10-01},
	booktitle = {Advances in {Soft} {Computing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Manevitz, Larry and Hazan, Hananel},
	editor = {Sidorov, Grigori and Hernández Aguirre, Arturo and Reyes García, Carlos Alberto},
	year = {2010},
	doi = {10.1007/978-3-642-16773-7_21},
	keywords = {Favorite, Read, Printed},
	pages = {245--256},
	file = {Manevitz and Hazan - 2010 - Stability and Topology in Reservoir Computing.pdf:/home/thomaav/Zotero/storage/QVID9W9R/Manevitz, Hazan - 2010 - Stability and Topology in Reservoir Computing.pdf:application/pdf}
}

@incollection{mcquillan_role_2019,
	address = {Cham},
	title = {The {Role} of {Structure} and {Complexity} on {Reservoir} {Computing} {Quality}},
	volume = {11493},
	isbn = {978-3-030-19310-2 978-3-030-19311-9},
	url = {http://link.springer.com/10.1007/978-3-030-19311-9_6},
	abstract = {We explore the eﬀect of structure and connection complexity on the dynamical behaviour of Reservoir Computers (RC). At present, considerable eﬀort is taken to design and hand-craft physical reservoir computers. Both structure and physical complexity are often pivotal to task performance, however, assessing their overall importance is challenging. Using a recently proposed framework, we evaluate and compare the dynamical freedom (referring to quality) of neural network structures, as an analogy for physical systems. The results quantify how structure aﬀects the range of behaviours exhibited by these networks. It highlights that high quality reached by more complex structures is often also achievable in simpler structures with greater network size. Alternatively, quality is often improved in smaller networks by adding greater connection complexity. This work demonstrates the beneﬁts of using abstract behaviour representation, rather than evaluation through benchmark tasks, to assess the quality of computing substrates, as the latter typically has biases, and often provides little insight into the complete computing quality of physical systems.},
	language = {en},
	urldate = {2019-10-01},
	booktitle = {Unconventional {Computation} and {Natural} {Computation}},
	publisher = {Springer International Publishing},
	author = {Dale, Matthew and Dewhirst, Jack and O’Keefe, Simon and Sebald, Angelika and Stepney, Susan and Trefzer, Martin A.},
	editor = {McQuillan, Ian and Seki, Shinnosuke},
	year = {2019},
	doi = {10.1007/978-3-030-19311-9_6},
	keywords = {Favorite, Read, Printed},
	pages = {52--64},
	file = {Dale et al. - 2019 - The Role of Structure and Complexity on Reservoir .pdf:/home/thomaav/Zotero/storage/QZG8QIVJ/Dale et al. - 2019 - The Role of Structure and Complexity on Reservoir .pdf:application/pdf}
}

@incollection{riolo_survey_2011,
	address = {New York, NY},
	title = {A {Survey} of {Self} {Modifying} {Cartesian} {Genetic} {Programming}},
	volume = {8},
	isbn = {978-1-4419-7746-5 978-1-4419-7747-2},
	url = {http://link.springer.com/10.1007/978-1-4419-7747-2_6},
	abstract = {Self-Modifying Cartesian Genetic Programming (SMCGP) is a general purpose, graph-based, developmental form of Cartesian Genetic Programming. In addition to the usual computational functions found in CGP, SMCGP includes functions that can modify the evolved program at run time. This means that programs can be iterated to produce an inﬁnite sequence of phenotypes from a single evolved genotype. Here, we discuss the results of using SMCGP on a variety of diﬀerent problems, and see that SMCGP is able to solve tasks that require scalability and plasticity. We see how SMCGP is able to produce results that would be impossible for conventional, static Genetic Programming techniques.},
	language = {en},
	urldate = {2019-10-02},
	booktitle = {Genetic {Programming} {Theory} and {Practice} {VIII}},
	publisher = {Springer New York},
	author = {Harding, Simon and Banzhaf, Wolfgang and Miller, Julian F.},
	editor = {Riolo, Rick and McConaghy, Trent and Vladislavleva, Ekaterina},
	year = {2011},
	doi = {10.1007/978-1-4419-7747-2_6},
	keywords = {Printed},
	pages = {91--107},
	file = {Harding et al. - 2011 - A Survey of Self Modifying Cartesian Genetic Progr.pdf:/home/thomaav/Zotero/storage/8MTS7N82/Harding et al. - 2011 - A Survey of Self Modifying Cartesian Genetic Progr.pdf:application/pdf}
}

@article{gallicchio_deep_2017,
	title = {Deep reservoir computing: {A} critical experimental analysis},
	volume = {268},
	issn = {09252312},
	shorttitle = {Deep reservoir computing},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231217307567},
	doi = {10.1016/j.neucom.2016.12.089},
	abstract = {In this paper we propose an empirical analysis of deep recurrent neural network (RNN) architectures with stacked layers. The analysis aims at the study and proposal of approaches to develop and enhance hierarchical dynamics in deep recurrent architectures, within the eﬃcient Reservoir Computing (RC) approach for RNN modeling. The eﬀect of a deep layered organization of RC models is investigated in terms of both occurrence of multiple time-scale and increasing of richness of the dynamics. It turns out that a deep layering of recurrent models allows an eﬀective diversiﬁcation of temporal representations in the layers of the hierarchy, by amplifying the eﬀects of the factors inﬂuencing the time-scales and the richness of the dynamics, measured as the entropy of reservoir units activations. The advantages of the proposed approach are also highlighted by measuring the increment of the short term memory capacity of the RC models.},
	language = {en},
	urldate = {2019-10-02},
	journal = {Neurocomputing},
	author = {Gallicchio, Claudio and Micheli, Alessio and Pedrelli, Luca},
	month = dec,
	year = {2017},
	pages = {87--99},
	file = {Gallicchio et al. - 2017 - Deep reservoir computing A critical experimental .pdf:/home/thomaav/Zotero/storage/5DSJD77V/Gallicchio et al. - 2017 - Deep reservoir computing A critical experimental .pdf:application/pdf}
}

@article{lloyd_ultimate_2000,
	title = {Ultimate physical limits to computation},
	volume = {406},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/35023282},
	doi = {10.1038/35023282},
	language = {en},
	number = {6799},
	urldate = {2019-10-02},
	journal = {Nature},
	author = {Lloyd, Seth},
	month = aug,
	year = {2000},
	keywords = {Printed},
	pages = {1047--1054},
	file = {Lloyd - 2000 - Ultimate physical limits to computation.pdf:/home/thomaav/Zotero/storage/P39NFYAK/Lloyd - 2000 - Ultimate physical limits to computation.pdf:application/pdf}
}

@article{xue_decoupled_2007,
	title = {Decoupled echo state networks with lateral inhibition},
	volume = {20},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608007000378},
	doi = {10.1016/j.neunet.2007.04.014},
	abstract = {Building on some prior work, in this paper we describe a novel structure termed the decoupled echo state network (DESN) involving the use of lateral inhibition. Two low-complexity implementation schemes, namely, the DESN with reservoir prediction (DESN + RP) and DESN with maximum available information (DESN + MaxInfo), are developed: (1) In the multiple superimposed oscillator (MSO) problem, DESN + MaxInfo exhibits three important attributes: lower generalization mean-square error (MSE), better robustness with respect to the random generation of reservoir weight matrix and feedback connections, and robustness to variations in the sparseness of reservoir weight matrix, compared to DESN + RP. (2) For a noiseless nonlinear prediction task, DESN + RP outperforms the DESN + MaxInfo and single reservoir-based ESN approach in terms of lower prediction MSE and better robustness to a change in the number of inputs and sparsity of the reservoir weight matrix.},
	language = {en},
	number = {3},
	urldate = {2019-10-02},
	journal = {Neural Networks},
	author = {Xue, Yanbo and Yang, Le and Haykin, Simon},
	month = apr,
	year = {2007},
	keywords = {Printed},
	pages = {365--376},
	file = {Xue et al. - 2007 - Decoupled echo state networks with lateral inhibit.pdf:/home/thomaav/Zotero/storage/3DDFEB2N/Xue et al. - 2007 - Decoupled echo state networks with lateral inhibit.pdf:application/pdf}
}

@article{rodan_simple_2012,
	title = {Simple {Deterministically} {Constructed} {Cycle} {Reservoirs} with {Regular} {Jumps}},
	volume = {24},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/NECO_a_00297},
	doi = {10.1162/NECO_a_00297},
	language = {en},
	number = {7},
	urldate = {2019-10-01},
	journal = {Neural Computation},
	author = {Rodan, Ali and Tiňo, Peter},
	month = jul,
	year = {2012},
	keywords = {Printed},
	pages = {1822--1852},
	file = {Rodan and Tiňo - 2012 - Simple Deterministically Constructed Cycle Reservo.pdf:/home/thomaav/Zotero/storage/7TY9QIZM/Rodan and Tiňo - 2012 - Simple Deterministically Constructed Cycle Reservo.pdf:application/pdf}
}

@incollection{hutchison_simple_2010,
	address = {Berlin, Heidelberg},
	title = {Simple {Deterministically} {Constructed} {Recurrent} {Neural} {Networks}},
	volume = {6283},
	isbn = {978-3-642-15380-8 978-3-642-15381-5},
	url = {http://link.springer.com/10.1007/978-3-642-15381-5_33},
	abstract = {A large number of models for time series processing, forecasting or modeling follows a state-space formulation. Models in the speciﬁc class of state-space approaches, referred to as Reservoir Computing, ﬁx their state-transition function. The state space with the associated state transition structure forms a reservoir, which is supposed to be suﬃciently complex so as to capture a large number of features of the input stream that can be potentially exploited by the reservoir-to-output readout mapping. The largely “black box” character of reservoirs prevents us from performing a deeper theoretical investigation of the dynamical properties of successful reservoirs. Reservoir construction is largely driven by a series of (more-or-less) ad-hoc randomized model building stages, with both the researchers and practitioners having to rely on a series of trials and errors. We show that a very simple deterministically constructed reservoir with simple cycle topology gives performances comparable to those of the Echo State Network (ESN) on a number of time series benchmarks. Moreover, we argue that the memory capacity of such a model can be made arbitrarily close to the proved theoretical limit.},
	language = {en},
	urldate = {2019-10-01},
	booktitle = {Intelligent {Data} {Engineering} and {Automated} {Learning} – {IDEAL} 2010},
	publisher = {Springer Berlin Heidelberg},
	author = {Rodan, Ali and Tiňo, Peter},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Fyfe, Colin and Tino, Peter and Charles, Darryl and Garcia-Osorio, Cesar and Yin, Hujun},
	year = {2010},
	doi = {10.1007/978-3-642-15381-5_33},
	keywords = {Read, Printed},
	pages = {267--274},
	file = {Rodan and Tiňo - 2010 - Simple Deterministically Constructed Recurrent Neu.pdf:/home/thomaav/Zotero/storage/32VHM4ZH/Rodan and Tiňo - 2010 - Simple Deterministically Constructed Recurrent Neu.pdf:application/pdf}
}

@incollection{montavon_practical_2012,
	address = {Berlin, Heidelberg},
	title = {A {Practical} {Guide} to {Applying} {Echo} {State} {Networks}},
	volume = {7700},
	isbn = {978-3-642-35288-1 978-3-642-35289-8},
	url = {http://link.springer.com/10.1007/978-3-642-35289-8_36},
	abstract = {Reservoir computing has emerged in the last decade as an alternative to gradient descent methods for training recurrent neural networks. Echo State Network (ESN) is one of the key reservoir computing “ﬂavors”. While being practical, conceptually simple, and easy to implement, ESNs require some experience and insight to achieve the hailed good performance in many tasks. Here we present practical techniques and recommendations for successfully applying ESNs, as well as some more advanced application-speciﬁc modiﬁcations.},
	language = {en},
	urldate = {2019-10-07},
	booktitle = {Neural {Networks}: {Tricks} of the {Trade}},
	publisher = {Springer Berlin Heidelberg},
	author = {Lukoševičius, Mantas},
	editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
	year = {2012},
	doi = {10.1007/978-3-642-35289-8_36},
	keywords = {Printed},
	pages = {659--686},
	file = {Lukoševičius - 2012 - A Practical Guide to Applying Echo State Networks.pdf:/home/thomaav/Zotero/storage/7MKK5B9H/Lukoševičius - 2012 - A Practical Guide to Applying Echo State Networks.pdf:application/pdf}
}

@article{jaeger_adaptive_2003,
	title = {Adaptive {Nonlinear} {System} {Identification} with {Echo} {State} {Networks}},
	abstract = {Echo state networks (ESN) are a novel approach to recurrent neural network training. An ESN consists of a large, fixed, recurrent "reservoir" network, from which the desired output is obtained by training suitable output connection weights. Determination of optimal output weights becomes a linear, uniquely solvable task of MSE minimization. This article reviews the basic ideas and describes an online adaptation scheme based on the RLS algorithm known from adaptive linear systems. As an example, a 10-th order NARMA system is adaptively identified. The known benefits of the RLS algorithms carryover from linear systems to nonlinear ones; specifically, the convergence rate and misadjustment can be determined at design time.},
	language = {en},
	journal = {NIPS'02 Proceedings of the 15th International Conference on Neural Information Processing Systems},
	author = {Jaeger, Herbert},
	year = {2003},
	keywords = {Printed},
	pages = {609--616},
	file = {Jaeger - Adaptive Nonlinear System Identification with Echo.pdf:/home/thomaav/Zotero/storage/4A4RN3K7/Jaeger - Adaptive Nonlinear System Identification with Echo.pdf:application/pdf}
}

@article{kubota_dynamical_2019,
	title = {Dynamical {Anatomy} of {NARMA10} {Benchmark} {Task}},
	url = {http://arxiv.org/abs/1906.04608},
	abstract = {The emulation task of a nonlinear autoregressive moving average model, i.e., the NARMA10 task, has been widely used as a benchmark task for recurrent neural networks, especially in reservoir computing. However, the type and quantity of computational capabilities required to emulate the NARMA10 model remain unclear, and, to date, the NARMA10 task has been utilized blindly. Therefore, in this study, we have investigated the properties of the NARMA10 model from a dynamical system perspective. We revealed its bifurcation structure and basin of attraction, as well as the system’s Lyapunov spectra. Furthermore, we have analyzed the computational capabilities required to emulate the NARMA10 model by decomposing it into multiple combinations of orthogonal nonlinear polynomials using Legendre polynomials, and we directly evaluated its information processing capacity together with its dependences on some system parameters. The result demonstrates that the NARMA10 model contains an unstable region in the phase space that makes the system diverge according to the selection of the input range and initial conditions. Furthermore, the information processing capacity of the model varies according to the input range. These properties prevent safe application of this model and fair comparisons among experiments, which are unfavorable for a benchmark task. As a result, we propose a benchmark model that can clearly evaluate equivalent computational capacity using NARMA10. Compared to the original NARMA10 model, the proposed model is highly stable and robust against the input range settings.},
	language = {en},
	urldate = {2019-10-14},
	journal = {arXiv:1906.04608 [cs, stat]},
	author = {Kubota, Tomoyuki and Nakajima, Kohei and Takahashi, Hirokazu},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.04608},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Kubota et al. - 2019 - Dynamical Anatomy of NARMA10 Benchmark Task.pdf:/home/thomaav/Zotero/storage/J5Y4H4CS/Kubota et al. - 2019 - Dynamical Anatomy of NARMA10 Benchmark Task.pdf:application/pdf}
}

@article{schrauwen_improving_2008,
	title = {Improving reservoirs using intrinsic plasticity},
	volume = {71},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231208000519},
	doi = {10.1016/j.neucom.2007.12.020},
	abstract = {The beneﬁts of using intrinsic plasticity (IP), an unsupervised, local, biologically inspired adaptation rule that tunes the probability density of a neuron’s output towards an exponential distribution—thereby realizing an information maximization—have already been demonstrated. In this work, we extend the ideas of this adaptation method to a more commonly used non-linearity and a Gaussian output distribution. After deriving the learning rules, we show the effects of the bounded output of the transfer function on the moments of the actual output distribution. This allows us to show that the rule converges to the expected distributions, even in random recurrent networks. The IP rule is evaluated in a reservoir computing setting, which is a temporal processing technique which uses random, untrained recurrent networks as excitable media, where the network’s state is fed to a linear regressor used to calculate the desired output. We present an experimental comparison of the different IP rules on three benchmark tasks with different characteristics. Furthermore, we show that this unsupervised reservoir adaptation is able to adapt networks with very constrained topologies, such as a 1D lattice which generally shows quite unsuitable dynamic behavior, to a reservoir that can be used to solve complex tasks. We clearly demonstrate that IP is able to make reservoir computing more robust: the internal dynamics can autonomously tune themselves—irrespective of initial weights or input scaling—to the dynamic regime which is optimal for a given task.},
	language = {en},
	number = {7-9},
	urldate = {2019-10-14},
	journal = {Neurocomputing},
	author = {Schrauwen, Benjamin and Wardermann, Marion and Verstraeten, David and Steil, Jochen J. and Stroobandt, Dirk},
	month = mar,
	year = {2008},
	keywords = {Printed},
	pages = {1159--1171},
	file = {Schrauwen et al. - 2008 - Improving reservoirs using intrinsic plasticity.pdf:/home/thomaav/Zotero/storage/7MXYL6WZ/Schrauwen et al. - 2008 - Improving reservoirs using intrinsic plasticity.pdf:application/pdf}
}

@article{bengio_learning_1994,
	title = {Learning long-term dependencies with gradient descent is difficult},
	volume = {5},
	issn = {1045-9227, 1941-0093},
	url = {https://ieeexplore.ieee.org/document/279181/},
	doi = {10.1109/72.279181},
	abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.},
	language = {en},
	number = {2},
	urldate = {2019-10-11},
	journal = {IEEE Transactions on Neural Networks},
	author = {Bengio, Y. and Simard, P. and Frasconi, P.},
	month = mar,
	year = {1994},
	pages = {157--166},
	file = {Bengio et al. - 1994 - Learning long-term dependencies with gradient desc.pdf:/home/thomaav/Zotero/storage/99YKVFUU/Bengio et al. - 1994 - Learning long-term dependencies with gradient desc.pdf:application/pdf}
}

@article{doya_bifurcations_nodate,
	title = {Bifurcations of {Recurrent} {Neural} {Networks} in {Gradient} {Descent} {Learning} 3},
	abstract = {Asymptotic behavior of a recurrent neural network changes qualitatively at certain points in the parameter space, which are known as {\textbackslash}bifurcation points". At bifurcation points, the output of a network can change discontinuously with the change of parameters and therefore convergence of gradient descent algorithms is not guaranteed. Furthermore, learning equations used for error gradient estimation can be unstable. However, some kinds of bifurcations are inevitable in training a recurrent network as an automaton or an oscillator. Some of the factors underlying successful training of recurrent networks are investigated, such as choice of initial connections, choice of input patterns, teacher forcing, and truncated learning equations.},
	language = {en},
	author = {Doya, Kenji},
	pages = {11},
	file = {Doya - Bifurcations of Recurrent Neural Networks in Gradi.pdf:/home/thomaav/Zotero/storage/5HZURWIF/Doya - Bifurcations of Recurrent Neural Networks in Gradi.pdf:application/pdf}
}

@article{jaeger_echo_2001,
	title = {The “echo state” approach to analysing and training recurrent neural networks},
	language = {en},
	journal = {GMD-Report 148, German National Research Institute for Computer Science},
	author = {Jaeger, Herbert},
	year = {2001},
	keywords = {Favorite},
	file = {Jaeger - The “echo state” approach to analysing and trainin.pdf:/home/thomaav/Zotero/storage/KEY7LPJ3/Jaeger - The “echo state” approach to analysing and trainin.pdf:application/pdf}
}

@inproceedings{steil_backpropagation-decorrelation:_2004,
	address = {Budapest, Hungary},
	title = {Backpropagation-decorrelation: online recurrent learning with {O}({N}) complexity},
	volume = {2},
	isbn = {978-0-7803-8359-3},
	shorttitle = {Backpropagation-decorrelation},
	url = {http://ieeexplore.ieee.org/document/1380039/},
	doi = {10.1109/IJCNN.2004.1380039},
	abstract = {We introduce a new learning rule for fully recurrent neural networks which we call Backpropagation-Decorrelation rule (BPDC). It combines important principles: one-step backpropagation of errors and the usage of temporal memory in the network dynamics by means of decorrelation of activations. The BPDC rule is derived and theoretically justiﬁed from regarding learning as a constraint optimization problem and applies uniformly in discrete and continuous time. It is very easy to implement, and has a minimal complexity of 2N multiplications per time-step in the single output case. Nevertheless we obtain fast tracking and excellent performance in some benchmark problems including the Mackey-Glass time-series.},
	language = {en},
	urldate = {2019-10-22},
	booktitle = {2004 {IEEE} {International} {Joint} {Conference} on {Neural} {Networks} ({IEEE} {Cat}. {No}.{04CH37541})},
	publisher = {IEEE},
	author = {Steil, J.J.},
	year = {2004},
	pages = {843--848},
	file = {Steil - 2004 - Backpropagation-decorrelation online recurrent le.pdf:/home/thomaav/Zotero/storage/HNNZPNBQ/Steil - 2004 - Backpropagation-decorrelation online recurrent le.pdf:application/pdf}
}

@inproceedings{kulkarni_memristor-based_2012,
	address = {Amsterdam, The Netherlands},
	title = {Memristor-based reservoir computing},
	isbn = {978-1-4503-1671-2},
	url = {http://dl.acm.org/citation.cfm?doid=2765491.2765531},
	doi = {10.1145/2765491.2765531},
	abstract = {As feature-size scaling and “Moore’s Law” in integrated CMOS circuits further slows down, attention is shifting to computing by non-von Neumann and non-Boolean computing models. Reservoir computing (RC) is a new computing paradigm that allows to harness the intrinsic dynamics of a “reservoir” to perform useful computations. The reservoir, or compute core, must only provide sufﬁciently rich dynamics that are then mapped onto a low-dimensional space by an readout layer. One of the key advantages of this approach is that only the readout layer needs to be adapted to perform the desired computation. The reservoir itself remains unchanged. In this paper we use for the ﬁrst time memristive components as reservoir building blocks that are assembled into device networks. Memristive components are particularly interesting for this purpose because of their non-linear and memory characteristics. In addition, they can be integrated very densely and provide rich dynamics with a few components only. We use pattern recognition and associative memory tasks to illustrate the memristive reservoir computing approach. For that purpose, we have built a software framework that allows to create valid memristor networks, to simulate and evaluate them in Ngspice, and to train the readout layer by means of a Genetic Algorithm (GA). Our results show that we can efﬁciently and robustly classify temporal patterns. The approach presents a promising new computing paradigm that harnesses the non-linear, time-dependent, and highly-variable properties of current memristive components for solving computational tasks.},
	language = {en},
	urldate = {2019-10-22},
	booktitle = {Proceedings of the 2012 {IEEE}/{ACM} {International} {Symposium} on {Nanoscale} {Architectures} - {NANOARCH} '12},
	publisher = {ACM Press},
	author = {Kulkarni, Manjari S. and Teuscher, Christof},
	year = {2012},
	pages = {226--232},
	file = {Kulkarni and Teuscher - 2012 - Memristor-based reservoir computing.pdf:/home/thomaav/Zotero/storage/WL3AC46C/Kulkarni and Teuscher - 2012 - Memristor-based reservoir computing.pdf:application/pdf}
}

@incollection{scholkopf_temporal_2007,
	title = {Temporal dynamics of information content carried by neurons in the primary visual cortex},
	isbn = {978-0-262-25691-9},
	url = {https://direct.mit.edu/books/book/3168/chapter/87523/temporal-dynamics-of-information-content-carried},
	abstract = {We use multi-electrode recordings from cat primary visual cortex and investigate whether a simple linear classifier can extract information about the presented stimuli. We find that information is extractable and that it even lasts for several hundred milliseconds after the stimulus has been removed. In a fast sequence of stimulus presentation, information about both new and old stimuli is present simultaneously and nonlinear relations between these stimuli can be extracted. These results suggest nonlinear properties of cortical representations. The important implications of these properties for the nonlinear brain theory are discussed.},
	language = {en},
	urldate = {2019-10-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 19},
	publisher = {The MIT Press},
	author = {Nikolic, Danko and Haeusler, Stefan and Singer, Wolf and Maass, Wolfgang},
	editor = {Schölkopf, Bernhard and Platt, John and Hofmann, Thomas},
	year = {2007},
	doi = {10.7551/mitpress/7503.003.0135},
	file = {Schölkopf et al. - 2007 - Temporal dynamics of information content carried b.pdf:/home/thomaav/Zotero/storage/PNC5UKQV/Schölkopf et al. - 2007 - Temporal dynamics of information content carried b.pdf:application/pdf}
}

@article{legenstein_edge_2007,
	title = {Edge of chaos and prediction of computational performance for neural circuit models},
	volume = {20},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608007000433},
	doi = {10.1016/j.neunet.2007.04.017},
	abstract = {We analyze in this article the signiﬁcance of the edge of chaos for real-time computations in neural microcircuit models consisting of spiking neurons and dynamic synapses. We ﬁnd that the edge of chaos predicts quite well those values of circuit parameters that yield maximal computational performance. But obviously it makes no prediction of their computational performance for other parameter values. Therefore, we propose a new method for predicting the computational performance of neural microcircuit models. The new measure estimates directly the kernel property and the generalization capability of a neural microcircuit. We validate the proposed measure by comparing its prediction with direct evaluations of the computational performance of various neural microcircuit models. The proposed method also allows us to quantify differences in the computational performance and generalization capability of neural circuits in different dynamic regimes (UP- and DOWN-states) that have been demonstrated through intracellular recordings in vivo.},
	language = {en},
	number = {3},
	urldate = {2019-10-29},
	journal = {Neural Networks},
	author = {Legenstein, Robert and Maass, Wolfgang},
	month = apr,
	year = {2007},
	pages = {323--334},
	file = {Legenstein and Maass - 2007 - Edge of chaos and prediction of computational perf.pdf:/home/thomaav/Zotero/storage/WDSSGS4P/Legenstein and Maass - 2007 - Edge of chaos and prediction of computational perf.pdf:application/pdf}
}

@article{jaeger_tutorial_2002,
	title = {A tutorial on training recurrent neural networks, covering {BPPT}, {RTRL}, {EKF} and the "echo state network" approach},
	abstract = {This tutorial is a worked-out version of a 5-hour course originally held at AIS in September/October 2002. It has two distinct components. First, it contains a mathematically-oriented crash course on traditional training methods for recurrent neural networks, covering back-propagation through time (BPTT), real-time recurrent learning (RTRL), and extended Kalman filtering approaches (EKF). This material is covered in Sections 2 – 5. The remaining sections 1 and 6 – 9 are much more gentle, more detailed, and illustrated with simple examples. They are intended to be useful as a stand-alone tutorial for the echo state network (ESN) approach to recurrent neural network training.},
	language = {en},
	journal = {GMD-Forschungszentrum Informationstechnik, 2002},
	author = {Jaeger, Herbert},
	year = {2002},
	pages = {46},
	file = {Jaeger - A tutorial on training recurrent neural networks, .pdf:/home/thomaav/Zotero/storage/5C7K855J/Jaeger - A tutorial on training recurrent neural networks, .pdf:application/pdf}
}

@article{verstraeten_isolated_2005,
	title = {Isolated word recognition with the {Liquid} {State} {Machine}: a case study},
	volume = {95},
	issn = {00200190},
	shorttitle = {Isolated word recognition with the {Liquid} {State} {Machine}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0020019005001523},
	doi = {10.1016/j.ipl.2005.05.019},
	abstract = {The Liquid State Machine (LSM) is a recently developed computational model with interesting properties. It can be used for pattern classiﬁcation, function approximation and other complex tasks. Contrary to most common computational models, the LSM does not require information to be stored in some stable state of the system: the inherent dynamics of the system are used by a memoryless readout function to compute the output. In this paper we present a case study of the performance of the Liquid State Machine based on a recurrent spiking neural network by applying it to a well known and well studied problem: speech recognition of isolated digits. We evaluate different ways of coding the speech into spike trains. In its optimal conﬁguration, the performance of the LSM approximates that of a state-of-the-art recognition system. Another interesting conclusion is the fact that the biologically most realistic encoding performs far better than more conventional methods.},
	language = {en},
	number = {6},
	urldate = {2019-11-09},
	journal = {Information Processing Letters},
	author = {Verstraeten, D. and Schrauwen, B. and Stroobandt, D. and Van Campenhout, J.},
	month = sep,
	year = {2005},
	pages = {521--528},
	file = {Verstraeten et al. - 2005 - Isolated word recognition with the Liquid State Ma.pdf:/home/thomaav/Zotero/storage/9RBQNIGU/Verstraeten et al. - 2005 - Isolated word recognition with the Liquid State Ma.pdf:application/pdf}
}

@incollection{kurkova_stable_2008,
	address = {Berlin, Heidelberg},
	title = {Stable {Output} {Feedback} in {Reservoir} {Computing} {Using} {Ridge} {Regression}},
	volume = {5163},
	isbn = {978-3-540-87535-2 978-3-540-87536-9},
	url = {http://link.springer.com/10.1007/978-3-540-87536-9_83},
	abstract = {An important property of Reservoir Computing, and signal processing techniques in general, is generalization and noise robustness. In trajectory generation tasks, we don’t want that a small deviation leads to an instability. For forecasting and system identiﬁcation we want to avoid over-ﬁtting. In prior work on Reservoir Computing, the addition of noise to the dynamic reservoir trajectory is generally used. In this work, we show that high-performing reservoirs can be trained using only the commonly used ridge regression. We experimentally validate these claims on two very diﬀerent tasks: long-term, robust trajectory generation and system identiﬁcation of a heating tank with variable dead-time.},
	language = {en},
	urldate = {2019-11-09},
	booktitle = {Artificial {Neural} {Networks} - {ICANN} 2008},
	publisher = {Springer Berlin Heidelberg},
	author = {Wyffels, Francis and Schrauwen, Benjamin and Stroobandt, Dirk},
	editor = {Kůrková, Véra and Neruda, Roman and Koutník, Jan},
	year = {2008},
	doi = {10.1007/978-3-540-87536-9_83},
	pages = {808--817},
	file = {Wyffels et al. - 2008 - Stable Output Feedback in Reservoir Computing Usin.pdf:/home/thomaav/Zotero/storage/FQWSDKGC/Wyffels et al. - 2008 - Stable Output Feedback in Reservoir Computing Usin.pdf:application/pdf}
}

@article{bishop_training_1995,
	title = {Training with {Noise} is {Equivalent} to {Tikhonov} {Regularization}},
	volume = {7},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/neco.1995.7.1.108},
	doi = {10.1162/neco.1995.7.1.108},
	abstract = {It is well known that the addition of noise to the input data of a neural network during training can, in some circumstances, lead to signiﬁcant improvements in generalization performance. Previous work has shown that such training with noise is equivalent to a form of regularization in which an extra term is added to the error function. However, the regularization term, which involves second derivatives of the error function, is not bounded below, and so can lead to diﬃculties if used directly in a learning algorithm based on error minimization. In this paper we show that, for the purposes of network training, the regularization term can be reduced to a positive deﬁnite form which involves only ﬁrst derivatives of the network mapping. For a sum-of-squares error function, the regularization term belongs to the class of generalized Tikhonov regularizers. Direct minimization of the regularized error function provides a practical alternative to training with noise.},
	language = {en},
	number = {1},
	urldate = {2019-11-09},
	journal = {Neural Computation},
	author = {Bishop, Chris M.},
	month = jan,
	year = {1995},
	pages = {108--116},
	file = {Bishop - 1995 - Training with Noise is Equivalent to Tikhonov Regu.pdf:/home/thomaav/Zotero/storage/U97NB98Y/Bishop - 1995 - Training with Noise is Equivalent to Tikhonov Regu.pdf:application/pdf}
}

@article{soriano_delay-based_2015,
	title = {Delay-{Based} {Reservoir} {Computing}: {Noise} {Effects} in a {Combined} {Analog} and {Digital} {Implementation}},
	volume = {26},
	issn = {2162-237X, 2162-2388},
	shorttitle = {Delay-{Based} {Reservoir} {Computing}},
	url = {http://ieeexplore.ieee.org/document/6782741/},
	doi = {10.1109/TNNLS.2014.2311855},
	abstract = {Reservoir computing is a paradigm in machine learning whose processing capabilities rely on the dynamical behavior of recurrent neural networks. We present a mixed analog and digital implementation of this concept with a nonlinear analog electronic circuit as a main computational unit. In our approach, the reservoir network can be replaced by a single nonlinear element with delay via time-multiplexing. We analyze the inﬂuence of noise on the performance of the system for two benchmark tasks: 1) a classiﬁcation problem and 2) a chaotic time-series prediction task. Special attention is given to the role of quantization noise, which is studied by varying the resolution in the conversion interface between the analog and digital worlds.},
	language = {en},
	number = {2},
	urldate = {2019-11-08},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Soriano, Miguel C. and Ortin, Silvia and Keuninckx, Lars and Appeltant, Lennert and Danckaert, Jan and Pesquera, Luis and van der Sande, Guy},
	month = feb,
	year = {2015},
	keywords = {Printed},
	pages = {388--393},
	file = {Soriano et al. - 2015 - Delay-Based Reservoir Computing Noise Effects in .pdf:/home/thomaav/Zotero/storage/7DIKTM58/Soriano et al. - 2015 - Delay-Based Reservoir Computing Noise Effects in .pdf:application/pdf}
}

@article{tong_learning_2007,
	title = {Learning grammatical structure with {Echo} {State} {Networks}},
	volume = {20},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608007000354},
	doi = {10.1016/j.neunet.2007.04.013},
	abstract = {Echo State Networks (ESNs) have been shown to be effective for a number of tasks, including motor control, dynamic time series prediction, and memorizing musical sequences. However, their performance on natural language tasks has been largely unexplored until now. Simple Recurrent Networks (SRNs) have a long history in language modeling and show a striking similarity in architecture to ESNs. A comparison of SRNs and ESNs on a natural language task is therefore a natural choice for experimentation. Elman applies SRNs to a standard task in statistical NLP: predicting the next word in a corpus, given the previous words. Using a simple context-free grammar and an SRN with backpropagation through time (BPTT), Elman showed that the network was able to learn internal representations that were sensitive to linguistic processes that were useful for the prediction task. Here, using ESNs, we show that training such internal representations is unnecessary to achieve levels of performance comparable to SRNs. We also compare the processing capabilities of ESNs to bigrams and trigrams. Due to some unexpected regularities of Elman’s grammar, these statistical techniques are capable of maintaining dependencies over greater distances than might be initially expected. However, we show that the memory of ESNs in this word-prediction task, although noisy, extends signiﬁcantly beyond that of bigrams and trigrams, enabling ESNs to make good predictions of verb agreement at distances over which these methods operate at chance. Overall, our results indicate a surprising ability of ESNs to learn a grammar, suggesting that they form useful internal representations without learning them.},
	language = {en},
	number = {3},
	urldate = {2019-11-07},
	journal = {Neural Networks},
	author = {Tong, Matthew H. and Bickett, Adam D. and Christiansen, Eric M. and Cottrell, Garrison W.},
	month = apr,
	year = {2007},
	pages = {424--432},
	file = {Tong et al. - 2007 - Learning grammatical structure with Echo State Net.pdf:/home/thomaav/Zotero/storage/DTYU6YQ4/Tong et al. - 2007 - Learning grammatical structure with Echo State Net.pdf:application/pdf}
}

@inproceedings{bush_modeling_2005,
	address = {Montreal, Que., Canada},
	title = {Modeling reward functions for incomplete state representations via echo state networks},
	isbn = {978-0-7803-9048-5},
	url = {http://ieeexplore.ieee.org/document/1556402/},
	doi = {10.1109/IJCNN.2005.1556402},
	abstract = {The Echo State Network (ESN) architecture is used as a recurrent network strategy for approximating the Q-function of the mass-spring-damper linear dynamical system when only partial state is observable. The ESN architecture’s approximation performance is compared against feed-forward neural networks given perfect state information (FNN) and a ﬁnite window of time-delayed partial state (TDNN), respectively. Both feed-forward representations are known to perform well in approximating the Q-function in this problem domain. We demonstrate that the ESN, given partial state, well-represents temporally dependent rewards and exhibits similar performance to FNN and TDNN architectures in approximating the Q-function during on-line learning.},
	language = {en},
	urldate = {2019-11-07},
	booktitle = {Proceedings. 2005 {IEEE} {International} {Joint} {Conference} on {Neural} {Networks}, 2005.},
	publisher = {IEEE},
	author = {Bush, K. and Anderson, C.},
	year = {2005},
	pages = {2995--3000 vol. 5},
	file = {Bush and Anderson - 2005 - Modeling reward functions for incomplete state rep.pdf:/home/thomaav/Zotero/storage/FSGSPKML/Bush and Anderson - 2005 - Modeling reward functions for incomplete state rep.pdf:application/pdf}
}

@article{aislan_antonelo_learning_2015,
	title = {On {Learning} {Navigation} {Behaviors} for {Small} {Mobile} {Robots} {With} {Reservoir} {Computing} {Architectures}},
	volume = {26},
	issn = {2162-237X, 2162-2388},
	url = {http://ieeexplore.ieee.org/document/6824836/},
	doi = {10.1109/TNNLS.2014.2323247},
	abstract = {This paper proposes a general reservoir computing (RC) learning framework that can be used to learn navigation behaviors for mobile robots in simple and complex unknown partially observable environments. RC provides an efﬁcient way to train recurrent neural networks by letting the recurrent part of the network (called reservoir) be ﬁxed while only a linear readout output layer is trained. The proposed RC framework builds upon the notion of navigation attractor or behavior that can be embedded in the high-dimensional space of the reservoir after learning. The learning of multiple behaviors is possible because the dynamic robot behavior, consisting of a sensorymotor sequence, can be linearly discriminated in the highdimensional nonlinear space of the dynamic reservoir. Three learning approaches for navigation behaviors are shown in this paper. The ﬁrst approach learns multiple behaviors based on the examples of navigation behaviors generated by a supervisor, while the second approach learns goal-directed navigation behaviors based only on rewards. The third approach learns complex goaldirected behaviors, in a supervised way, using a hierarchical architecture whose internal predictions of contextual switches guide the sequence of basic navigation behaviors toward the goal.},
	language = {en},
	number = {4},
	urldate = {2019-11-07},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Aislan Antonelo, Eric and Schrauwen, Benjamin},
	month = apr,
	year = {2015},
	pages = {763--780},
	file = {Aislan Antonelo and Schrauwen - 2015 - On Learning Navigation Behaviors for Small Mobile .pdf:/home/thomaav/Zotero/storage/7TSYEQPC/Aislan Antonelo and Schrauwen - 2015 - On Learning Navigation Behaviors for Small Mobile .pdf:application/pdf}
}

@incollection{krishnamachari_mote-based_2009,
	address = {Berlin, Heidelberg},
	title = {Mote-{Based} {Online} {Anomaly} {Detection} {Using} {Echo} {State} {Networks}},
	volume = {5516},
	isbn = {978-3-642-02084-1 978-3-642-02085-8},
	url = {http://link.springer.com/10.1007/978-3-642-02085-8_6},
	abstract = {Sensor network deployments are plagued with measurement faults due to hardware and software defects. At the same time, networks should autonomously adapt to events they sense, for example by increasing their sampling rate or raising alarms. In this work we unify fault and event detection under a general anomaly detection framework in which motes are trained to recognize data from a training set as normal and measurements that signiﬁcantly deviate from that set as anomalies. We implement an anomaly detection algorithm using Echo State Networks (ESN), a family of sparse neural networks, on a mote-class device and show that its accuracy is comparable to a PC-based implementation. Furthermore, we show that ESNs detect more faults and have fewer false positives than threshold-based fault detection mechanisms. More importantly, while rule-based fault detection algorithms generate false negatives and misclassiﬁcations when exposed to multiple faults and events, ESNs are general, correctly identifying a wide variety of anomalies.},
	language = {en},
	urldate = {2019-11-07},
	booktitle = {Distributed {Computing} in {Sensor} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Chang, Marcus and Terzis, Andreas and Bonnet, Philippe},
	editor = {Krishnamachari, Bhaskar and Suri, Subhash and Heinzelman, Wendi and Mitra, Urbashi},
	year = {2009},
	doi = {10.1007/978-3-642-02085-8_6},
	pages = {72--86},
	file = {Chang et al. - 2009 - Mote-Based Online Anomaly Detection Using Echo Sta.pdf:/home/thomaav/Zotero/storage/T5UQNDVW/Chang et al. - 2009 - Mote-Based Online Anomaly Detection Using Echo Sta.pdf:application/pdf}
}

@article{dan_deterministic_2014,
	title = {Deterministic {Echo} {State} {Networks} {Based} {Stock} {Price} {Forecasting}},
	volume = {2014},
	issn = {1085-3375, 1687-0409},
	url = {http://www.hindawi.com/journals/aaa/2014/137148/},
	doi = {10.1155/2014/137148},
	abstract = {Echo state networks (ESNs), as efficient and powerful computational models for approximating nonlinear dynamical systems, have been successfully applied in financial time series forecasting. Reservoir constructions in standard ESNs rely on trials and errors in real applications due to a series of randomized model building stages. A novel form of ESN with deterministically constructed reservoir is competitive with standard ESN by minimal complexity and possibility of optimizations for ESN specifications. In this paper, forecasting performances of deterministic ESNs are investigated in stock price prediction applications. The experiment results on two benchmark datasets (Shanghai Composite Index and S\&P500) demonstrate that deterministic ESNs outperform standard ESN in both accuracy and efficiency, which indicate the prospect of deterministic ESNs for financial prediction.},
	language = {en},
	urldate = {2019-11-06},
	journal = {Abstract and Applied Analysis},
	author = {Dan, Jingpei and Guo, Wenbo and Shi, Weiren and Fang, Bin and Zhang, Tingping},
	year = {2014},
	keywords = {Printed},
	pages = {1--6},
	file = {Dan et al. - 2014 - Deterministic Echo State Networks Based Stock Pric.pdf:/home/thomaav/Zotero/storage/DDLVUGVR/Dan et al. - 2014 - Deterministic Echo State Networks Based Stock Pric.pdf:application/pdf}
}

@article{lin_short-term_2009,
	title = {Short-term stock price prediction based on echo state networks},
	volume = {36},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417408006519},
	doi = {10.1016/j.eswa.2008.09.049},
	abstract = {Neural network has been popular in time series prediction in ﬁnancial areas because of their advantages in handling nonlinear systems. This paper presents a study of using a novel recurrent neural network–echo state network (ESN) to predict the next closing price in stock markets. The Hurst exponent is applied to adaptively determine initial transient and choose sub-series with greatest predictability during training. The experiment results on nearly all stocks of S\&P 500 demonstrate that ESN outperforms other conventional neural networks in most cases. Experiments also indicate that if we include principle component analysis (PCA) to ﬁlter noise in data pretreatment and choose appropriate parameters, we can effectively prevent coarse prediction performance. But in most cases PCA improves the prediction accuracy only a little.},
	language = {en},
	number = {3},
	urldate = {2019-11-06},
	journal = {Expert Systems with Applications},
	author = {Lin, Xiaowei and Yang, Zehong and Song, Yixu},
	month = apr,
	year = {2009},
	pages = {7313--7317},
	file = {Lin et al. - 2009 - Short-term stock price prediction based on echo st.pdf:/home/thomaav/Zotero/storage/ZYGSX48G/Lin et al. - 2009 - Short-term stock price prediction based on echo st.pdf:application/pdf}
}

@inproceedings{song_hourly_2011,
	address = {Mianyang, China},
	title = {Hourly electric load forecasting algorithm based on echo state neural network},
	isbn = {978-1-4244-8737-0},
	url = {http://ieeexplore.ieee.org/document/5968901/},
	doi = {10.1109/CCDC.2011.5968901},
	abstract = {An algorithm for hourly electric load forecasting based on echo state neural networks (ESN) is proposed in this paper. ESN is a new paradigm for using recurrent neural networks (RNNs) with a simpler training method. While the prediction, load patterns are treated as time series signals; no further information is used than the past load data records, such as weather, seasonal variations. The relation between key parameter of the ESN and the predicting performance is discussed; ESN and feedforward neural network (FNN) are compared with the same task also. Simulation experiment results demonstrate that the proposed ESN algorithm is valid and can obtain more accurate predicting results than the FNN for the short-term load prediction problem.},
	language = {en},
	urldate = {2019-11-06},
	booktitle = {2011 {Chinese} {Control} and {Decision} {Conference} ({CCDC})},
	publisher = {IEEE},
	author = {Song, Qingsong and Zhao, Xiangmo and Feng, Zuren and An, Yisheng and Song, Baohua},
	month = may,
	year = {2011},
	pages = {3893--3897},
	file = {Song et al. - 2011 - Hourly electric load forecasting algorithm based o.pdf:/home/thomaav/Zotero/storage/SDTM8L3A/Song et al. - 2011 - Hourly electric load forecasting algorithm based o.pdf:application/pdf}
}

@inproceedings{an_short-term_2011,
	address = {Shanghai, China},
	title = {Short-term traffic flow forecasting via echo state neural networks},
	isbn = {978-1-4244-9950-2},
	url = {http://ieeexplore.ieee.org/document/6022154/},
	doi = {10.1109/ICNC.2011.6022154},
	abstract = {An algorithm for short term traffic flaw prediction based on echo state neural networks (ESN) is proposed in this paper. ESN is a new paradigm for using recurrent neural networks (RNNs) with a simpler training method. While the prediction, traffic flow patterns are treated as time series signals; no further information is used than the past traffic flaw data records, such as weather, traffic accidents. The relation between key parameter of the ESN and the predicting performance is discussed; ESN and feed forward neural network (FNN) are compared with the same task also. Simulation experiment results demonstrate that the proposed ESN algorithm is valid and can obtain more accurate predicting results than the FNN for the short-term traffic flaw prediction problem.},
	language = {en},
	urldate = {2019-11-06},
	booktitle = {2011 {Seventh} {International} {Conference} on {Natural} {Computation}},
	publisher = {IEEE},
	author = {An, Yisheng and Song, Qingsong and Zhao, Xiangmo},
	month = jul,
	year = {2011},
	pages = {844--847},
	file = {An et al. - 2011 - Short-term traffic flow forecasting via echo state.pdf:/home/thomaav/Zotero/storage/YLLKRUUB/An et al. - 2011 - Short-term traffic flow forecasting via echo state.pdf:application/pdf}
}

@article{grigoryeva_optimal_2015,
	title = {Optimal nonlinear information processing capacity in delay-based reservoir computers},
	volume = {5},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/srep12858},
	doi = {10.1038/srep12858},
	language = {en},
	number = {1},
	urldate = {2019-11-10},
	journal = {Scientific Reports},
	author = {Grigoryeva, Lyudmila and Henriques, Julie and Larger, Laurent and Ortega, Juan-Pablo},
	month = oct,
	year = {2015},
	pages = {12858},
	file = {Grigoryeva et al. - 2015 - Optimal nonlinear information processing capacity .pdf:/home/thomaav/Zotero/storage/SCMU9DR9/Grigoryeva et al. - 2015 - Optimal nonlinear information processing capacity .pdf:application/pdf}
}

@inproceedings{aaser_towards_2017,
	address = {Lyon, France},
	title = {Towards making a cyborg: {A} closed-loop reservoir-neuro system},
	isbn = {978-0-262-34633-7},
	shorttitle = {Towards making a cyborg},
	url = {https://www.mitpressjournals.org/doi/abs/10.1162/isal_a_072},
	doi = {10.7551/ecal_a_072},
	language = {en},
	urldate = {2019-11-10},
	booktitle = {Proceedings of the 14th {European} {Conference} on {Artificial} {Life} {ECAL} 2017},
	publisher = {MIT Press},
	author = {Aaser, Peter and Knudsen, Martinius and Ramstad, Ola Huse and van de Wijdeven, Rosanne and Nichele, Stefano and Sandvig, Ioanna and Tufte, Gunnar and Stefan Bauer, Ulrich and Halaas, Øyvind and Hendseth, Sverre and Sandvig, Axel and Valderhaug, Vibeke},
	month = sep,
	year = {2017},
	pages = {430--437},
	file = {Aaser et al. - 2017 - Towards making a cyborg A closed-loop reservoir-n.pdf:/home/thomaav/Zotero/storage/C2QTIDF7/Aaser et al. - 2017 - Towards making a cyborg A closed-loop reservoir-n.pdf:application/pdf}
}

@article{noauthor_dynamics_nodate,
	title = {The {Dynamics} of {Complex} {Systems} —{Examples}, {Questions}, {Methods} and {Concepts}.pdf},
	keywords = {Printed},
	file = {The Dynamics of Complex Systems —Examples, Questions, Methods and Concepts.pdf:/home/thomaav/Zotero/storage/6SDWUS2M/The Dynamics of Complex Systems —Examples, Questions, Methods and Concepts.pdf:application/pdf}
}

@article{heylighen_science_nodate,
	title = {{THE} {SCIENCE} {OF} {SELF}- {ORGANIZATION} {AND} {ADAPTIVITY}},
	abstract = {The theory of self-organization and adaptivity has grown out of a variety of disciplines, including thermodynamics, cybernetics and computer modelling. The present article reviews its most important concepts and principles. It starts with an intuitive overview, illustrated by the examples of magnetization and Bénard convection, and concludes with the basics of mathematical modelling. Self-organization can be defined as the spontaneous creation of a globally coherent pattern out of local interactions. Because of its distributed character, this organization tends to be robust, resisting perturbations. The dynamics of a self-organizing system is typically non-linear, because of circular or feedback relations between the components. Positive feedback leads to an explosive growth, which ends when all components have been absorbed into the new configuration, leaving the system in a stable, negative feedback state. Non-linear systems have in general several stable states, and this number tends to increase (bifurcate) as an increasing input of energy pushes the system farther from its thermodynamic equilibrium. To adapt to a changing environment, the system needs a variety of stable states that is large enough to react to all perturbations but not so large as to make its evolution uncontrollably chaotic. The most adequate states are selected according to their fitness, either directly by the environment, or by subsystems that have adapted to the environment at an earlier stage. Formally, the basic mechanism underlying self-organization is the (often noise-driven) variation which explores different regions in the system’s state space until it enters an attractor. This precludes further variation outside the attractor, and thus restricts the freedom of the system’s components to behave independently. This is equivalent to the increase of coherence, or decrease of statistical entropy, that defines selforganization.},
	language = {en},
	author = {Heylighen, Francis},
	keywords = {Printed},
	pages = {26},
	file = {Heylighen - THE SCIENCE OF SELF- ORGANIZATION AND ADAPTIVITY.pdf:/home/thomaav/Zotero/storage/JUIAJCVY/Heylighen - THE SCIENCE OF SELF- ORGANIZATION AND ADAPTIVITY.pdf:application/pdf}
}

@article{sipper_emergence_1999-1,
	title = {The emergence of cellular computing},
	volume = {32},
	issn = {00189162},
	url = {http://ieeexplore.ieee.org/document/774914/},
	doi = {10.1109/2.774914},
	language = {en},
	number = {7},
	urldate = {2019-11-12},
	journal = {Computer},
	author = {Sipper, M.},
	month = jul,
	year = {1999},
	keywords = {Printed},
	pages = {18--26},
	file = {Sipper - 1999 - The emergence of cellular computing.pdf:/home/thomaav/Zotero/storage/9GMB7QWD/Sipper - 1999 - The emergence of cellular computing.pdf:application/pdf}
}

@article{skowronski_noise-robust_2007,
	title = {Noise-{Robust} {Automatic} {Speech} {Recognition} {Using} a {Predictive} {Echo} {State} {Network}},
	volume = {15},
	issn = {1558-7916},
	url = {http://ieeexplore.ieee.org/document/4244539/},
	doi = {10.1109/TASL.2007.896669},
	abstract = {Artiﬁcial neural networks have been shown to perform well in automatic speech recognition (ASR) tasks, although their complexity and excessive computational costs have limited their use. Recently, a recurrent neural network with simpliﬁed training, the echo state network (ESN), was introduced by Jaeger and shown to outperform conventional methods in time series prediction experiments. We created the predictive ESN classiﬁer by combining the ESN with a state machine framework. In small-vocabulary ASR experiments, we compared the noise-robust performance of the predictive ESN classiﬁer with a hidden Markov model (HMM) as a function of model size and signal-to-noise ratio (SNR). The predictive ESN classiﬁer outperformed an HMM by 8-dB SNR, and both models achieved maximum noise-robust accuracy for architectures with more states and fewer kernels per state. Using ten trials of random sets of training/validation/test speakers, accuracy for the predictive ESN classiﬁer, averaged between 0 and 20 dB SNR, was 81 3\%, compared to 61 2\% for an HMM. The closed-form regression training for the ESN signiﬁcantly reduced the computational cost of the network, and the reservoir of the ESN created a high-dimensional representation of the input with memory which led to increased noise-robust classiﬁcation.},
	language = {en},
	number = {5},
	urldate = {2019-11-16},
	journal = {IEEE Transactions on Audio, Speech and Language Processing},
	author = {Skowronski, Mark D. and Harris, John G.},
	month = jul,
	year = {2007},
	keywords = {Printed},
	pages = {1724--1730},
	file = {Skowronski and Harris - 2007 - Noise-Robust Automatic Speech Recognition Using a .pdf:/home/thomaav/Zotero/storage/B48324GX/Skowronski and Harris - 2007 - Noise-Robust Automatic Speech Recognition Using a .pdf:application/pdf}
}

@article{mitchell_life_nodate,
	title = {Life and {Evolution} in {Computers}},
	language = {en},
	author = {Mitchell, Melanie},
	keywords = {Printed},
	pages = {21},
	file = {Mitchell - Life and Evolution in Computers.pdf:/home/thomaav/Zotero/storage/FRLWBHYJ/Mitchell - Life and Evolution in Computers.pdf:application/pdf}
}

@article{barbosa_looking_2018,
	title = {Looking {Beyond} {Appearances}: {Synthetic} {Training} {Data} for {Deep} {CNNs} in {Re}-identification},
	volume = {167},
	issn = {10773142},
	shorttitle = {Looking {Beyond} {Appearances}},
	url = {http://arxiv.org/abs/1701.03153},
	doi = {10.1016/j.cviu.2017.12.002},
	abstract = {Re-identiﬁcation is generally carried out by encoding the appearance of a subject in terms of outﬁt, suggesting scenarios where people do not change their attire. In this paper we overcome this restriction, by proposing a framework based on a deep convolutional neural network, SOMAnet, that additionally models other discriminative aspects, namely, structural attributes of the human ﬁgure (e.g. height, obesity, gender). Our method is unique in many respects. First, SOMAnet is based on the Inception architecture, departing from the usual siamese framework. This spares expensive data preparation (pairing images across cameras) and allows the understanding of what the network learned. Second, and most notably, the training data consists of a synthetic 100K instance dataset, SOMAset, created by photorealistic human body generation software. Synthetic data represents a good compromise between realistic imagery, usually not required in re-identiﬁcation since surveillance cameras capture low-resolution silhouettes, and complete control of the samples, which is useful in order to customize the data w.r.t. the surveillance scenario at-hand, e.g. ethnicity. SOMAnet, trained on SOMAset and ﬁne-tuned on recent re-identiﬁcation benchmarks, outperforms all competitors, matching subjects even with different apparel. The combination of synthetic data with Inception architectures opens up new research avenues in re-identiﬁcation.},
	language = {en},
	urldate = {2019-11-24},
	journal = {Computer Vision and Image Understanding},
	author = {Barbosa, Igor Barros and Cristani, Marco and Caputo, Barbara and Rognhaugen, Aleksander and Theoharis, Theoharis},
	month = feb,
	year = {2018},
	note = {arXiv: 1701.03153},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Printed, I.2.10, I.4.8},
	pages = {50--62},
	file = {Barbosa et al. - 2018 - Looking Beyond Appearances Synthetic Training Dat.pdf:/home/thomaav/Zotero/storage/U4VR2BVC/Barbosa et al. - 2018 - Looking Beyond Appearances Synthetic Training Dat.pdf:application/pdf}
}

@article{kortylewski_analyzing_nodate,
	title = {Analyzing and {Reducing} the {Damage} of {Dataset} {Bias} to {Face} {Recognition} {With} {Synthetic} {Data}},
	abstract = {It is well known that deep learning approaches to face recognition suffer from various biases in the available training data. In this work, we demonstrate the large potential of synthetic data for analyzing and reducing the negative effects of dataset bias on deep face recognition systems. In particular we explore two complementary application areas for synthetic face images: 1) Using fully annotated synthetic face images we can study the face recognition rate as a function of interpretable parameters such as face pose. This enables us to systematically analyze the effect of different types of dataset biases on the generalization ability of neural network architectures. Our analysis reveals that deeper neural network architectures can generalize better to unseen face poses. Furthermore, our study shows that current neural network architectures cannot disentangle face pose and facial identity, which limits their generalization ability. 2) We pre-train neural networks with large-scale synthetic data that is highly variable in face pose and the number of facial identities. After a subsequent ﬁne-tuning with realworld data, we observe that the damage of dataset bias in the real-world data is largely reduced. Furthermore, we demonstrate that the size of real-world datasets can be reduced by 75\% while maintaining competitive face recognition performance. The data and software used in this work are publicly available 1.},
	language = {en},
	author = {Kortylewski, Adam and Egger, Bernhard and Schneider, Andreas and Gerig, Thomas and Morel-Forster, Andreas and Vetter, Thomas},
	keywords = {Printed},
	pages = {8},
	file = {Kortylewski et al. - Analyzing and Reducing the Damage of Dataset Bias .pdf:/home/thomaav/Zotero/storage/3G8ZEMSN/Kortylewski et al. - Analyzing and Reducing the Damage of Dataset Bias .pdf:application/pdf}
}

@article{sun_dissecting_nodate,
	title = {Dissecting {Person} {Re}-{Identification} {From} the {Viewpoint} of {Viewpoint}},
	abstract = {Variations in visual factors such as viewpoint, pose, illumination and background, are usually viewed as important challenges in person re-identiﬁcation (re-ID). In spite of acknowledging these factors to be inﬂuential, quantitative studies on how they affect a re-ID system are still lacking. To derive insights in this scientiﬁc campaign, this paper makes an early attempt in studying a particular factor, viewpoint. We narrow the viewpoint problem down to the pedestrian rotation angle to obtain focused conclusions. In this regard, this paper makes two contributions to the community. First, we introduce a large-scale synthetic data engine, PersonX. Composed of hand-crafted 3D person models, the salient characteristic of this engine is “controllable”. That is, we are able to synthesize pedestrians by setting the visual variables to arbitrary values. Second, on the 3D data engine, we quantitatively analyze the inﬂuence of pedestrian rotation angle on re-ID accuracy. Comprehensively, the person rotation angles are precisely customized from 0◦ to 360◦, allowing us to investigate its effect on the training, query, and gallery sets. Extensive experiment helps us have a deeper understanding of the fundamental problems in person re-ID. Our research also provides useful insights for dataset building and future practical usage, e.g., a person of a side view makes a better query.},
	language = {en},
	author = {Sun, Xiaoxiao and Zheng, Liang},
	keywords = {Printed},
	pages = {10},
	file = {Sun and Zheng - Dissecting Person Re-Identification From the Viewp.pdf:/home/thomaav/Zotero/storage/U83EI44Z/Sun and Zheng - Dissecting Person Re-Identification From the Viewp.pdf:application/pdf}
}

@article{murphy_use_2016,
	title = {Use of synthetic data to test biometric algorithms},
	volume = {25},
	issn = {1017-9909},
	url = {http://electronicimaging.spiedigitallibrary.org/article.aspx?doi=10.1117/1.JEI.25.4.043023},
	doi = {10.1117/1.JEI.25.4.043023},
	language = {en},
	number = {4},
	urldate = {2019-11-24},
	journal = {Journal of Electronic Imaging},
	author = {Murphy, Thomas M. and Broussard, Randy and Rakvic, Ryan and Ngo, Hau and Ives, Robert W. and Schultz, Robert and Aguayo, Joseph T.},
	month = aug,
	year = {2016},
	keywords = {Printed},
	pages = {043023},
	file = {Murphy et al. - 2016 - Use of synthetic data to test biometric algorithms.pdf:/home/thomaav/Zotero/storage/K723TMQ9/Murphy et al. - 2016 - Use of synthetic data to test biometric algorithms.pdf:application/pdf}
}

@incollection{riolo_survey_2011-1,
	address = {New York, NY},
	title = {A {Survey} of {Self} {Modifying} {Cartesian} {Genetic} {Programming}},
	volume = {8},
	isbn = {978-1-4419-7746-5 978-1-4419-7747-2},
	url = {http://link.springer.com/10.1007/978-1-4419-7747-2_6},
	abstract = {Self-Modifying Cartesian Genetic Programming (SMCGP) is a general purpose, graph-based, developmental form of Cartesian Genetic Programming. In addition to the usual computational functions found in CGP, SMCGP includes functions that can modify the evolved program at run time. This means that programs can be iterated to produce an inﬁnite sequence of phenotypes from a single evolved genotype. Here, we discuss the results of using SMCGP on a variety of diﬀerent problems, and see that SMCGP is able to solve tasks that require scalability and plasticity. We see how SMCGP is able to produce results that would be impossible for conventional, static Genetic Programming techniques.},
	language = {en},
	urldate = {2019-11-25},
	booktitle = {Genetic {Programming} {Theory} and {Practice} {VIII}},
	publisher = {Springer New York},
	author = {Harding, Simon and Banzhaf, Wolfgang and Miller, Julian F.},
	editor = {Riolo, Rick and McConaghy, Trent and Vladislavleva, Ekaterina},
	year = {2011},
	doi = {10.1007/978-1-4419-7747-2_6},
	keywords = {Printed},
	pages = {91--107},
	file = {Harding et al. - 2011 - A Survey of Self Modifying Cartesian Genetic Progr.pdf:/home/thomaav/Zotero/storage/YMHBHIRK/Harding et al. - 2011 - A Survey of Self Modifying Cartesian Genetic Progr.pdf:application/pdf}
}

@incollection{dos_santos_evo_2009,
	title = {From {Evo} to {EvoDevo}: {Mapping} and {Adaptation} in {Artificial} {Development}},
	isbn = {978-953-307-008-7},
	shorttitle = {From {Evo} to {EvoDevo}},
	url = {http://www.intechopen.com/books/evolutionary-computation/from-evo-to-evodevo-mapping-and-adaptation-in-artificial-development},
	language = {en},
	urldate = {2019-11-25},
	booktitle = {Evolutionary {Computation}},
	publisher = {InTech},
	author = {Tufte, Gunnar},
	editor = {dos Santos, Wellington Pinheiro},
	month = oct,
	year = {2009},
	doi = {10.5772/9603},
	keywords = {Printed},
	file = {Tufte - 2009 - From Evo to EvoDevo Mapping and Adaptation in Art.pdf:/home/thomaav/Zotero/storage/Z6Y75KRL/Tufte - 2009 - From Evo to EvoDevo Mapping and Adaptation in Art.pdf:application/pdf}
}

@book{wurtz_organic_2008,
	address = {Berlin, Heidelberg},
	series = {Understanding {Complex} {Systems}},
	title = {Organic {Computing}},
	isbn = {978-3-540-77656-7 978-3-540-77657-4},
	url = {http://link.springer.com/10.1007/978-3-540-77657-4},
	abstract = {There is growing interest in the use of analogies of biological development for problem solving in computer science. Nature is extremely intricate when compared to human designs, and incorporates features such as the ability to scale, adapt and self-repair that could be usefully incorporated into human-designed artifacts. In this chapter, we discuss how the metaphor of biological development can be used in artiﬁcial systems and highlight some of the challenges of this emerging ﬁeld.},
	language = {en},
	urldate = {2019-11-25},
	publisher = {Springer Berlin Heidelberg},
	author = {Würtz, Rolf P.},
	editor = {Kelso, J.A. Scott and Érdi, P. and Friston, K. and Haken, H. and Kacprzyk, J. and Kurths, J. and Reichl, L. and Schuster, P. and Schweitzer, F. and Sornette, D.},
	year = {2008},
	doi = {10.1007/978-3-540-77657-4},
	keywords = {Printed},
	file = {Würtz - 2008 - Organic Computing.pdf:/home/thomaav/Zotero/storage/6LZ3E4US/Würtz - 2008 - Organic Computing.pdf:application/pdf}
}

@article{kitano_designing_nodate,
	title = {Designing {Neural} {Networks} {Using} {Genetic} {Algorithms} with {Graph} {Generation} {System}},
	abstract = {We present a new method of designing neural networks using the genetic algorithm . Recently th ere have been several reports claiming attempts to design neural networks using genetic algorithms were successful. However, these methods have a problem in scalability, i.e., the convergence characteristic degrades significantly as the size of the network increases. This is because these methods employ direct mapp ing of chromosomes into network connectivities. As an alternative approach, we propose a graph grammatical encoding that will encode graph generation grammar to the chromosome so that it generates more regular connectivity patterns with shorter chromosome length . Experimental results support that our new scheme provides magnitude of speedup in convergence of neural network design and exhibits desirable scaling property.},
	language = {en},
	author = {Kitano, Hiroaki},
	keywords = {Printed},
	pages = {16},
	file = {Kitano - Designing Neural Networks Using Genetic Algorithms.pdf:/home/thomaav/Zotero/storage/KHTTWWQY/Kitano - Designing Neural Networks Using Genetic Algorithms.pdf:application/pdf}
}

@article{shannon_communication_1949,
	title = {Communication in the {Presence} of {Noise}},
	volume = {37},
	issn = {0096-8390},
	url = {http://ieeexplore.ieee.org/document/1697831/},
	doi = {10.1109/JRPROC.1949.232969},
	abstract = {A method is developed for representing any communication system geometrically. Messages and the corresponding signals are points in two "function spaces," and the modulation process is a mapping of one space into the other. Using this representation, a number of results in communication theory are deduced concerning expansion and compression of bandwidth and the threshold effect. Formulas are found for the maxmum rate of transmission of binary digits over a system when the signal is perturbed by various types of noise. Some of the properties of "ideal" systems which transmit at this maxmum rate are discussed. The equivalent number of binary digits per second for certain information sources is calculated.},
	language = {en},
	number = {1},
	urldate = {2019-12-01},
	journal = {Proceedings of the IRE},
	author = {Shannon, C.E.},
	month = jan,
	year = {1949},
	pages = {10--21},
	file = {Shannon - 1949 - Communication in the Presence of Noise.pdf:/home/thomaav/Zotero/storage/KYD8FWFU/Shannon - 1949 - Communication in the Presence of Noise.pdf:application/pdf}
}

@article{hauser_towards_2011,
	title = {Towards a theoretical foundation for morphological computation with compliant bodies},
	volume = {105},
	issn = {0340-1200, 1432-0770},
	url = {http://link.springer.com/10.1007/s00422-012-0471-0},
	doi = {10.1007/s00422-012-0471-0},
	language = {en},
	number = {5-6},
	urldate = {2019-12-01},
	journal = {Biological Cybernetics},
	author = {Hauser, Helmut and Ijspeert, Auke J. and Füchslin, Rudolf M. and Pfeifer, Rolf and Maass, Wolfgang},
	month = dec,
	year = {2011},
	pages = {355--370},
	file = {Hauser et al. - 2011 - Towards a theoretical foundation for morphological.pdf:/home/thomaav/Zotero/storage/JVDWJ3D6/Hauser et al. - 2011 - Towards a theoretical foundation for morphological.pdf:application/pdf}
}

@article{soriano_optoelectronic_2013,
	title = {Optoelectronic reservoir computing: tackling noise-induced performance degradation},
	volume = {21},
	issn = {1094-4087},
	shorttitle = {Optoelectronic reservoir computing},
	url = {https://www.osapublishing.org/oe/abstract.cfm?uri=oe-21-1-12},
	doi = {10.1364/OE.21.000012},
	abstract = {We present improved strategies to perform photonic information processing using an optoelectronic oscillator with delayed feedback. In particular, we study, via numerical simulations and experiments, the inﬂuence of a ﬁnite signal-to-noise ratio on the computing performance. We illustrate that the performance degradation induced by noise can be compensated for via multi-level pre-processing masks.},
	language = {en},
	number = {1},
	urldate = {2019-12-01},
	journal = {Optics Express},
	author = {Soriano, M. C. and Ortín, S. and Brunner, D. and Larger, L. and Mirasso, C. R. and Fischer, I. and Pesquera, L.},
	month = jan,
	year = {2013},
	pages = {12},
	file = {Soriano et al. - 2013 - Optoelectronic reservoir computing tackling noise.pdf:/home/thomaav/Zotero/storage/YDJ5ZGT4/Soriano et al. - 2013 - Optoelectronic reservoir computing tackling noise.pdf:application/pdf}
}

@article{busing_connectivity_2010-1,
	title = {Connectivity, {Dynamics}, and {Memory} in {Reservoir} {Computing} with {Binary} and {Analog} {Neurons}},
	volume = {22},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/neco.2009.01-09-947},
	doi = {10.1162/neco.2009.01-09-947},
	language = {en},
	number = {5},
	urldate = {2019-12-01},
	journal = {Neural Computation},
	author = {Büsing, Lars and Schrauwen, Benjamin and Legenstein, Robert},
	month = may,
	year = {2010},
	pages = {1272--1311},
	file = {Büsing et al. - 2010 - Connectivity, Dynamics, and Memory in Reservoir Co.pdf:/home/thomaav/Zotero/storage/U7VFUTSD/Büsing et al. - 2010 - Connectivity, Dynamics, and Memory in Reservoir Co.pdf:application/pdf}
}

@incollection{mcquillan_role_2019-1,
	address = {Cham},
	title = {The {Role} of the {Representational} {Entity} in {Physical} {Computing}},
	volume = {11493},
	isbn = {978-3-030-19310-2 978-3-030-19311-9},
	url = {http://link.springer.com/10.1007/978-3-030-19311-9_18},
	abstract = {We have developed abstraction/representation (AR) theory to answer the question “When does a physical system compute?” AR theory requires the existence of a representational entity (RE), but the vanilla theory does not explicitly include the RE in its deﬁnition of physical computing. Here we extend the theory by showing how the RE forms a linked complementary model to the physical computing model, and demonstrate its use in the case of intrinsic computing in a non-human RE: a bacterium.},
	language = {en},
	urldate = {2019-12-02},
	booktitle = {Unconventional {Computation} and {Natural} {Computation}},
	publisher = {Springer International Publishing},
	author = {Stepney, Susan and Kendon, Viv},
	editor = {McQuillan, Ian and Seki, Shinnosuke},
	year = {2019},
	doi = {10.1007/978-3-030-19311-9_18},
	keywords = {Printed},
	pages = {219--231},
	file = {Stepney and Kendon - 2019 - The Role of the Representational Entity in Physica.pdf:/home/thomaav/Zotero/storage/CVQ7IDVA/Stepney and Kendon - 2019 - The Role of the Representational Entity in Physica.pdf:application/pdf}
}

@incollection{rozenberg_nonclassical_2012,
	address = {Berlin, Heidelberg},
	title = {Nonclassical {Computation} — {A} {Dynamical} {Systems} {Perspective}},
	isbn = {978-3-540-92909-3 978-3-540-92910-9},
	url = {http://link.springer.com/10.1007/978-3-540-92910-9_59},
	abstract = {In this chapter, computation is investigated from a dynamical systems perspective. A dynamical system is described in terms of its abstract state space, the system’s current state within its state space, and a rule that determines its motion through its state space. In a classical computational system, that rule is given explicitly by the computer program; in a physical system, that rule is the underlying physical law governing the behavior of the system. Therefore, a dynamical systems approach to computation allows one to take a uniﬁed view of computation in classical discrete systems and in systems performing nonclassical computation. In particular, it gives a route to a computational interpretation of physical embodied systems exploiting the natural dynamics of their material substrates.},
	language = {en},
	urldate = {2019-12-02},
	booktitle = {Handbook of {Natural} {Computing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Stepney, Susan},
	editor = {Rozenberg, Grzegorz and Bäck, Thomas and Kok, Joost N.},
	year = {2012},
	doi = {10.1007/978-3-540-92910-9_59},
	pages = {1979--2025},
	file = {Stepney - 2012 - Nonclassical Computation — A Dynamical Systems Per.pdf:/home/thomaav/Zotero/storage/QA8R822P/Stepney - 2012 - Nonclassical Computation — A Dynamical Systems Per.pdf:application/pdf}
}

@article{stepney_neglected_2008,
	title = {The neglected pillar of material computation},
	volume = {237},
	issn = {01672789},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167278908000420},
	doi = {10.1016/j.physd.2008.01.028},
	abstract = {Many novel forms of computational material have been suggested, from using slime moulds to solve graph searching problems, to using packaging foam to solve differential equations. I argue that attempting to force such novel approaches into the conventional Universal Turing computational framework will provide neither insights into theoretical questions of computation, nor more powerful computational machines. Instead, we should be investigating matter from the perspective of its natural computational capabilities. I also argue that we should investigate nonbiological substrates, since these are less complex in that they have not been tuned by evolution to have their particular properties. Only then we will understand both aspects of computation (logical and physical) required to understand the computation occurring in biological systems.},
	language = {en},
	number = {9},
	urldate = {2019-12-02},
	journal = {Physica D: Nonlinear Phenomena},
	author = {Stepney, Susan},
	month = jul,
	year = {2008},
	keywords = {Printed},
	pages = {1157--1164},
	file = {Stepney - 2008 - The neglected pillar of material computation.pdf:/home/thomaav/Zotero/storage/2QH69I3U/Stepney - 2008 - The neglected pillar of material computation.pdf:application/pdf}
}

@article{markov_limits_2014,
	title = {Limits on fundamental limits to computation},
	volume = {512},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature13570},
	doi = {10.1038/nature13570},
	language = {en},
	number = {7513},
	urldate = {2019-12-02},
	journal = {Nature},
	author = {Markov, Igor L.},
	month = aug,
	year = {2014},
	keywords = {Printed},
	pages = {147--154},
	file = {Markov - 2014 - Limits on fundamental limits to computation.pdf:/home/thomaav/Zotero/storage/DVK6FKDT/Markov - 2014 - Limits on fundamental limits to computation.pdf:application/pdf}
}

@article{lloyd_ultimate_2000-1,
	title = {Ultimate physical limits to computation},
	volume = {406},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/35023282},
	doi = {10.1038/35023282},
	language = {en},
	number = {6799},
	urldate = {2019-12-02},
	journal = {Nature},
	author = {Lloyd, Seth},
	month = aug,
	year = {2000},
	pages = {1047--1054},
	file = {Lloyd - 2000 - Ultimate physical limits to computation.pdf:/home/thomaav/Zotero/storage/AU3C5ZK8/Lloyd - 2000 - Ultimate physical limits to computation.pdf:application/pdf}
}

@article{horsman_when_2014-1,
	title = {When does a physical system compute?},
	volume = {470},
	issn = {1364-5021, 1471-2946},
	url = {https://royalsocietypublishing.org/doi/10.1098/rspa.2014.0182},
	doi = {10.1098/rspa.2014.0182},
	language = {en},
	number = {2169},
	urldate = {2019-12-02},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Horsman, Clare and Stepney, Susan and Wagner, Rob C. and Kendon, Viv},
	month = sep,
	year = {2014},
	keywords = {Printed},
	pages = {20140182},
	file = {Horsman et al. - 2014 - When does a physical system compute.pdf:/home/thomaav/Zotero/storage/N3VL57P6/Horsman et al. - 2014 - When does a physical system compute.pdf:application/pdf}
}

@article{crutchfield_introduction_2010,
	title = {Introduction to {Focus} {Issue}: {Intrinsic} and {Designed} {Computation}: {Information} {Processing} in {Dynamical} {Systems}—{Beyond} the {Digital} {Hegemony}},
	volume = {20},
	issn = {1054-1500, 1089-7682},
	shorttitle = {Introduction to {Focus} {Issue}},
	url = {http://aip.scitation.org/doi/10.1063/1.3492712},
	doi = {10.1063/1.3492712},
	language = {en},
	number = {3},
	urldate = {2019-12-02},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {Crutchfield, James P. and Ditto, William L. and Sinha, Sudeshna},
	month = sep,
	year = {2010},
	keywords = {Printed},
	pages = {037101},
	file = {Crutchfield et al. - 2010 - Introduction to Focus Issue Intrinsic and Designe.pdf:/home/thomaav/Zotero/storage/QHYP29Q2/Crutchfield et al. - 2010 - Introduction to Focus Issue Intrinsic and Designe.pdf:application/pdf}
}

@article{dodson_hausdorff_2003,
	title = {Hausdorff {Dimension} and {Diophantine} {Approximation}},
	url = {http://arxiv.org/abs/math/0305399},
	abstract = {We begin with a brief treatment of Hausdorff measure and Hausdorff dimension. We then explain some of the principal results in Diophantine approximation and the Hausdorff dimension of related sets, originating in the pioneering work of Vojtech Jarnik. We conclude with some applications of these results to the metrical structure of exceptional sets associated with some famous problems. It is not intended that all the recent developments be covered but they can be found in the references cited.},
	urldate = {2019-12-03},
	journal = {arXiv:math/0305399},
	author = {Dodson, M. Maurice and Kristensen, Simon},
	month = jun,
	year = {2003},
	note = {arXiv: math/0305399},
	keywords = {Mathematics - Number Theory},
	file = {Dodson and Kristensen - 2003 - Hausdorff Dimension and Diophantine Approximation.pdf:/home/thomaav/Zotero/storage/MJFT6PII/Dodson and Kristensen - 2003 - Hausdorff Dimension and Diophantine Approximation.pdf:application/pdf}
}

@book{hurewicz_dimension_nodate,
	title = {Dimension {Theory}},
	author = {Hurewicz, Witold},
	file = {2015.84609.Dimension-Theory.pdf:/home/thomaav/Zotero/storage/GUEBRX4S/2015.84609.Dimension-Theory.pdf:application/pdf}
}

@article{wei_box-covering_2013,
	title = {Box-covering algorithm for fractal dimension of weighted networks},
	volume = {3},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/srep03049},
	doi = {10.1038/srep03049},
	language = {en},
	number = {1},
	urldate = {2019-12-03},
	journal = {Scientific Reports},
	author = {Wei, Dai-Jun and Liu, Qi and Zhang, Hai-Xin and Hu, Yong and Deng, Yong and Mahadevan, Sankaran},
	month = nov,
	year = {2013},
	pages = {3049},
	file = {Wei et al. - 2013 - Box-covering algorithm for fractal dimension of we.pdf:/home/thomaav/Zotero/storage/NCR3V94S/Wei et al. - 2013 - Box-covering algorithm for fractal dimension of we.pdf:application/pdf}
}

@article{wei_new_2014,
	title = {A new information dimension of complex networks},
	volume = {378},
	issn = {03759601},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0375960114001595},
	doi = {10.1016/j.physleta.2014.02.010},
	abstract = {The fractal and self-similarity properties are revealed in many complex networks. The classical information dimension is an important method to study fractal and self-similarity properties of planar networks. However, it is not practical for real complex networks. In this Letter, a new information dimension of complex networks is proposed. The nodes number in each box is considered by using the box-covering algorithm of complex networks. The proposed method is applied to calculate the fractal dimensions of some real networks. Our results show that the proposed method is eﬃcient when dealing with the fractal dimension problem of complex networks.},
	language = {en},
	number = {16-17},
	urldate = {2019-12-03},
	journal = {Physics Letters A},
	author = {Wei, Daijun and Wei, Bo and Hu, Yong and Zhang, Haixin and Deng, Yong},
	month = mar,
	year = {2014},
	pages = {1091--1094},
	file = {Wei et al. - 2014 - A new information dimension of complex networks.pdf:/home/thomaav/Zotero/storage/RF6IAXHZ/Wei et al. - 2014 - A new information dimension of complex networks.pdf:application/pdf}
}

@incollection{mcquillan_role_2019-2,
	address = {Cham},
	title = {The {Role} of {Structure} and {Complexity} on {Reservoir} {Computing} {Quality}},
	volume = {11493},
	isbn = {978-3-030-19310-2 978-3-030-19311-9},
	url = {http://link.springer.com/10.1007/978-3-030-19311-9_6},
	abstract = {We explore the eﬀect of structure and connection complexity on the dynamical behaviour of Reservoir Computers (RC). At present, considerable eﬀort is taken to design and hand-craft physical reservoir computers. Both structure and physical complexity are often pivotal to task performance, however, assessing their overall importance is challenging. Using a recently proposed framework, we evaluate and compare the dynamical freedom (referring to quality) of neural network structures, as an analogy for physical systems. The results quantify how structure aﬀects the range of behaviours exhibited by these networks. It highlights that high quality reached by more complex structures is often also achievable in simpler structures with greater network size. Alternatively, quality is often improved in smaller networks by adding greater connection complexity. This work demonstrates the beneﬁts of using abstract behaviour representation, rather than evaluation through benchmark tasks, to assess the quality of computing substrates, as the latter typically has biases, and often provides little insight into the complete computing quality of physical systems.},
	language = {en},
	urldate = {2019-12-04},
	booktitle = {Unconventional {Computation} and {Natural} {Computation}},
	publisher = {Springer International Publishing},
	author = {Dale, Matthew and Dewhirst, Jack and O’Keefe, Simon and Sebald, Angelika and Stepney, Susan and Trefzer, Martin A.},
	editor = {McQuillan, Ian and Seki, Shinnosuke},
	year = {2019},
	doi = {10.1007/978-3-030-19311-9_6},
	keywords = {Printed},
	pages = {52--64},
	file = {Dale et al. - 2019 - The Role of Structure and Complexity on Reservoir .pdf:/home/thomaav/Zotero/storage/XH4SVD3C/Dale et al. - 2019 - The Role of Structure and Complexity on Reservoir .pdf:application/pdf}
}

@article{ma_functional_2016,
	title = {Functional echo state network for time series classification},
	volume = {373},
	issn = {00200255},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0020025516306661},
	doi = {10.1016/j.ins.2016.08.081},
	abstract = {Echo state networks (ESNs) are a new approach to recurrent neural networks (RNNs) that have been successfully applied in many domains. Nevertheless, an ESN is a predictive model rather than a classiﬁer, and methods to employ ESNs in time series classiﬁcation (TSC) tasks have not yet been fully explored. In this paper, we propose a novel ESN approach named functional echo state network (FESN) for time series classiﬁcation. The basic idea behind FESN is to replace the numeric variable output weights of an ESN with timevarying output-weight functions and introduce a temporal aggregation operator to the output layer that can project temporal signals into discrete class labels, thereby transforming the ESN from a predictive model into a true classiﬁer. Subsequently, to learn the outputweight functions, a spatio-temporal aggregation learning algorithm is proposed based on orthogonal function basis expansion. By leveraging the nonlinear mapping capacity of a reservoir and the accumulation of temporal information in the time domain, FESN can not only enhance the separability of different classes in a high-dimensional functional space but can also consider the relative importance of temporal data at different time steps according to dynamic output-weight functions. Theoretical analyses and experiments on an extensive set of UCR data were conducted on FESN. The results show that FESN yields better performance than single-algorithm methods, has comparable accuracy with ensemblebased methods and exhibits acceptable computational complexity. Interestingly, for some time series datasets, we visualized some interpretable features extracted by FESN via speciﬁc patterns within the output-weight functions.},
	language = {en},
	urldate = {2019-12-09},
	journal = {Information Sciences},
	author = {Ma, Qianli and Shen, Lifeng and Chen, Weibiao and Wang, Jiabin and Wei, Jia and Yu, Zhiwen},
	month = dec,
	year = {2016},
	keywords = {Printed},
	pages = {1--20},
	file = {Ma et al. - 2016 - Functional echo state network for time series clas.pdf:/home/thomaav/Zotero/storage/JL52VGYR/Ma et al. - 2016 - Functional echo state network for time series clas.pdf:application/pdf}
}

@article{gallicchio_reservoir_2019,
	title = {Reservoir {Topology} in {Deep} {Echo} {State} {Networks}},
	volume = {11731},
	url = {http://arxiv.org/abs/1909.11022},
	doi = {10.1007/978-3-030-30493-5_6},
	abstract = {Deep Echo State Networks (DeepESNs) recently extended the applicability of Reservoir Computing (RC) methods towards the ﬁeld of deep learning. In this paper we study the impact of constrained reservoir topologies in the architectural design of deep reservoirs, through numerical experiments on several RC benchmarks. The major outcome of our investigation is to show the remarkable eﬀect, in terms of predictive performance gain, achieved by the synergy between a deep reservoir construction and a structured organization of the recurrent units in each layer. Our results also indicate that a particularly advantageous architectural setting is obtained in correspondence of DeepESNs where reservoir units are structured according to a permutation recurrent matrix.},
	language = {en},
	urldate = {2019-12-09},
	journal = {arXiv:1909.11022 [cs, stat]},
	author = {Gallicchio, Claudio and Micheli, Alessio},
	year = {2019},
	note = {arXiv: 1909.11022},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning, Statistics - Machine Learning, Printed},
	pages = {62--75},
	file = {Gallicchio and Micheli - 2019 - Reservoir Topology in Deep Echo State Networks.pdf:/home/thomaav/Zotero/storage/SXPQNLVF/Gallicchio and Micheli - 2019 - Reservoir Topology in Deep Echo State Networks.pdf:application/pdf}
}

@article{li_analysis_2019,
	title = {Analysis on the {Nonlinear} {Dynamics} of {Deep} {Neural} {Networks}: {Topological} {Entropy} and {Chaos}},
	shorttitle = {Analysis on the {Nonlinear} {Dynamics} of {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1804.03987},
	abstract = {The theoretical explanation for the great success of deep neural network (DNN) is still an open problem. In this paper DNN is considered as a discrete-time dynamical system due to its layered structure. The complexity provided by the nonlinearity in the DNN dynamics is analyzed in terms of topological entropy and chaos characterized by Lyapunov exponents. The properties revealed for the dynamics of DNN are applied to analyze the corresponding capabilities of classiﬁcation and generalization. In particular, for both the hyperbolic tangent function and the rectiﬁed linear units (ReLU), the Lyapunov exponents are both positive given proper DNN parameters, which implies the chaotic behavior of the dynamics. Moreover, the Vapnik-Chervonenkis (VC) dimension of DNN is also analyzed, based on the layered and recursive structure. The conclusions from the viewpoint of dynamical systems are expected to open a new dimension for the understanding of DNN.},
	language = {en},
	urldate = {2019-12-09},
	journal = {arXiv:1804.03987 [cs, stat]},
	author = {Li, Husheng},
	month = jan,
	year = {2019},
	note = {arXiv: 1804.03987},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Printed, To read},
	file = {Li - 2019 - Analysis on the Nonlinear Dynamics of Deep Neural .pdf:/home/thomaav/Zotero/storage/UT8NFRJX/Li - 2019 - Analysis on the Nonlinear Dynamics of Deep Neural .pdf:application/pdf}
}

@article{efroni_how_2019,
	title = {How to {Combine} {Tree}-{Search} {Methods} in {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1809.01843},
	abstract = {Finite-horizon lookahead policies are abundantly used in Reinforcement Learning and demonstrate impressive empirical success. Usually, the lookahead policies are implemented with speciﬁc planning methods such as Monte Carlo Tree Search (e.g. in AlphaZero (Silver et al. 2017b)). Referring to the planning problem as tree search, a reasonable practice in these implementations is to back up the value only at the leaves while the information obtained at the root is not leveraged other than for updating the policy. Here, we question the potency of this approach. Namely, the latter procedure is non-contractive in general, and its convergence is not guaranteed. Our proposed enhancement is straightforward and simple: use the return from the optimal tree path to back up the values at the descendants of the root. This leads to a γh-contracting procedure, where γ is the discount factor and h is the tree depth. To establish our results, we ﬁrst introduce a notion called multiple-step greedy consistency. We then provide convergence rates for two algorithmic instantiations of the above enhancement in the presence of noise injected to both the tree search stage and value estimation stage.},
	language = {en},
	urldate = {2020-01-01},
	journal = {arXiv:1809.01843 [cs, stat]},
	author = {Efroni, Yonathan and Dalal, Gal and Scherrer, Bruno and Mannor, Shie},
	month = feb,
	year = {2019},
	note = {arXiv: 1809.01843},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Efroni et al. - 2019 - How to Combine Tree-Search Methods in Reinforcemen.pdf:/home/thomaav/Zotero/storage/EA77ETUR/Efroni et al. - 2019 - How to Combine Tree-Search Methods in Reinforcemen.pdf:application/pdf}
}

@article{livi_determination_2018,
	title = {Determination of the edge of criticality in echo state networks through {Fisher} information maximization},
	volume = {29},
	issn = {2162-237X, 2162-2388},
	url = {http://arxiv.org/abs/1603.03685},
	doi = {10.1109/TNNLS.2016.2644268},
	abstract = {It is a widely accepted fact that the computational capability of recurrent neural networks is maximized on the socalled “edge of criticality”. Once the network operates in this conﬁguration, it performs efﬁciently on a speciﬁc application both in terms of (i) low prediction error and (ii) high shortterm memory capacity. Since the behavior of recurrent networks is strongly inﬂuenced by the particular input signal driving the dynamics, a universal, application-independent method for determining the edge of criticality is still missing. In this paper, we aim at addressing this issue by proposing a theoretically motivated, unsupervised method based on Fisher information for determining the edge of criticality in recurrent neural networks. It is proven that Fisher information is maximized for (ﬁnitesize) systems operating in such critical regions. However, Fisher information is notoriously difﬁcult to compute and either requires the probability density function or the conditional dependence of the system states with respect to the model parameters. The paper takes advantage of a recently-developed non-parametric estimator of the Fisher information matrix and provides a method to determine the critical region of echo state networks, a particular class of recurrent networks. The considered control parameters, which indirectly affect the echo state network performance, are explored to identify those conﬁgurations lying on the edge of criticality and, as such, maximizing Fisher information and computational performance. Experimental results on benchmarks and real-world data demonstrate the effectiveness of the proposed method.},
	language = {en},
	number = {3},
	urldate = {2020-01-09},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Livi, Lorenzo and Bianchi, Filippo Maria and Alippi, Cesare},
	month = mar,
	year = {2018},
	note = {arXiv: 1603.03685},
	keywords = {Computer Science - Neural and Evolutionary Computing, Physics - Data Analysis, Statistics and Probability, Computer Science - Machine Learning, To read},
	pages = {706--717},
	file = {Livi et al. - 2018 - Determination of the edge of criticality in echo s.pdf:/home/thomaav/Zotero/storage/D94VW7QX/Livi et al. - 2018 - Determination of the edge of criticality in echo s.pdf:application/pdf}
}

@inproceedings{koiran_vapnik-chervonenkis_nodate,
	title = {Vapnik-{Chervonenkis} dimension of recurrent neural networks},
	author = {Koiran, Pascal},
	keywords = {Read, Printed},
	file = {vapnik-chervonenkis.pdf:/home/thomaav/Zotero/storage/LS28W3YI/vapnik-chervonenkis.pdf:application/pdf}
}

@article{schmidhuber_discovering_1997,
	title = {Discovering {Neural} {Nets} with {Low} {Kolmogorov} {Complexity} and {High} {Generalization} {Capability}},
	volume = {10},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S089360809600127X},
	doi = {10.1016/S0893-6080(96)00127-X},
	abstract = {Many neural net learning algorithms aim at finding "simple" nets to explain training data. The expectation is that the "simpler" the networks, the better the generalization on test data ( --+ Occam's razor). Previous implementations, however, use measures for "simplicity" that lack the power, universality and elegance of those based on Kolmogorov complexity and Solomonoff's algorithmic probability. Likewise, most previous approaches (especially those ofthe "Bayesian" kind) sufferfrom the problem ofchoosing appropriate priors. This paper addresses both issues. It first reviews some basic concepts of algorithmic complexity theory relevant to machine learning, and how the Solomonoff-Levin distribution (or universal prior) deals with the prior problem. The universal prior leads to a probabilistic methodfor finding' 'algorithmically simple" problem solutions with high generalization capability. The method is based on Levin complexity (a time-bounded generalization ofKolmogorov complexity) and inspired by Levin's optimal universal search algorithm. For a given problem, solution candidates are computed by efficient' 'self-sizing" programs that influence their own runtime and storage size. The probabilistic search algorithm finds the "good" programs (the ones quickly computing algorithmically probable solutions fitting the training data). Simulations focus on the task of discovering "algorithmically simple" neural networks with low Kolmogorov complexity and high generalization capability. It is demonstrated that the method, at least with certain toy problems where it is computationally feasible, can lead to generalization results unmatchable by previous neural network algorithms. Much remains to be done, however, to make large scale applications and "incremental learning" feasible. © 1997 Elsevier Science Ltd.},
	language = {en},
	number = {5},
	urldate = {2020-01-09},
	journal = {Neural Networks},
	author = {Schmidhuber, Jürgen},
	month = jul,
	year = {1997},
	pages = {857--873},
	file = {Schmidhuber - 1997 - Discovering Neural Nets with Low Kolmogorov Comple.pdf:/home/thomaav/Zotero/storage/5RNVQWTV/Schmidhuber - 1997 - Discovering Neural Nets with Low Kolmogorov Comple.pdf:application/pdf}
}

@article{kallaugher_sketching_2018,
	title = {The {Sketching} {Complexity} of {Graph} and {Hypergraph} {Counting}},
	url = {http://arxiv.org/abs/1808.04995},
	abstract = {Subgraph counting is a fundamental primitive in graph processing, with applications in social network analysis (e.g., estimating the clustering coeﬃcient of a graph), database processing and other areas. The space complexity of subgraph counting has been studied extensively in the literature, but many natural settings are still not well understood. In this paper we revisit the subgraph (and hypergraph) counting problem in the sketching model, where the algorithm’s state as it processes a stream of updates to the graph is a linear function of the stream. This model has recently received a lot of attention in the literature, and has become a standard model for solving dynamic graph streaming problems.},
	language = {en},
	urldate = {2020-01-10},
	journal = {arXiv:1808.04995 [cs]},
	author = {Kallaugher, John and Kapralov, Michael and Price, Eric},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.04995},
	keywords = {To read, Computer Science - Data Structures and Algorithms},
	file = {Kallaugher et al. - 2018 - The Sketching Complexity of Graph and Hypergraph C.pdf:/home/thomaav/Zotero/storage/Z5SYBJKE/Kallaugher et al. - 2018 - The Sketching Complexity of Graph and Hypergraph C.pdf:application/pdf}
}

@article{hinzke_monte_1999,
	title = {Monte {Carlo} simulation of magnetization switching in a {Heisenberg} model for small ferromagnetic particles},
	volume = {121-122},
	issn = {00104655},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010465599003483},
	doi = {10.1016/S0010-4655(99)00348-3},
	abstract = {Using Monte Carlo methods we investigate the thermally activated magnetization switching of small ferromagnetic particles driven by an external magnetic ﬁeld. For low uniaxial anisotropy one expects that the spins rotate coherently while for sufﬁciently large anisotropy the reversal should be due to nucleation. The latter case has been investigated extensively by Monte Carlo simulation of corresponding Ising models. In order to study the crossover from coherent rotation to nucleation we use a specially adjusted update algorithm for the Monte Carlo simulation of a classical three-dimensional Heisenberg model with a ﬁnite uniaxial anisotropy. This special algorithm which uses a combined sampling can simulate different reversal mechanisms efﬁciently. It will be described in detail and its efﬁciency and physical validity will be discussed by a comparison with other common update-algorithms. © 1999 Elsevier Science B.V. All rights reserved.},
	language = {en},
	urldate = {2020-01-10},
	journal = {Computer Physics Communications},
	author = {Hinzke, D. and Nowak, U.},
	month = sep,
	year = {1999},
	pages = {334--337},
	file = {Hinzke and Nowak - 1999 - Monte Carlo simulation of magnetization switching .pdf:/home/thomaav/Zotero/storage/T5DHI7Z5/Hinzke and Nowak - 1999 - Monte Carlo simulation of magnetization switching .pdf:application/pdf}
}

@article{paterson_heisenberg_2019,
	title = {Heisenberg pseudo-exchange and emergent anisotropies in field-driven pinwheel artificial spin ice},
	volume = {100},
	issn = {2469-9950, 2469-9969},
	url = {http://arxiv.org/abs/1908.10626},
	doi = {10.1103/PhysRevB.100.174410},
	abstract = {Rotating all islands in square artificial spin ice (ASI) uniformly about their centres gives rise to the recently reported pinwheel ASI. At angles around 45\${\textasciicircum}{\textbackslash}mathrm\{o\}\$, the antiferromagnetic ordering changes to ferromagnetic and the magnetic configurations of the system exhibit near-degeneracy, making it particularly sensitive to small perturbations. We investigate through micromagnetic modelling the influence of dipolar fields produced by physically extended islands in field-driven magnetisation processes in pinwheel arrays, and compare the results to hysteresis experiments performed in-situ using Lorentz transmission electron microscopy. We find that magnetisation end-states induce a Heisenberg pseudo-exchange interaction that governs both the inter-island coupling and the resultant array reversal process. Symmetry reduction gives rise to anisotropies and array-corner mediated avalanche reversals through a cascade of nearest-neighbour (NN) islands. The symmetries of the anisotropy axes are related to those of the geometrical array but are misaligned to the array axes as a result of the correlated interactions between neighbouring islands. The NN dipolar coupling is reduced by decreasing the island size and, using this property, we track the transition from the strongly coupled regime towards the pure point dipole one and observe modification of the ferromagnetic array reversal process. Our results shed light on important aspects of the interactions in pinwheel ASI, and demonstrate a mechanism by which their properties may be tuned for use in a range of fundamental research and spintronic applications.},
	language = {en},
	number = {17},
	urldate = {2020-01-10},
	journal = {Physical Review B},
	author = {Paterson, Gary W. and Macauley, Gavin M. and Li, Yue and Macêdo, Rair and Ferguson, Ciaran and Morley, Sophie A. and Rosamond, Mark C. and Linfield, Edmund H. and Marrows, Christopher H. and Stamps, Robert L. and McVitie, Stephen},
	month = nov,
	year = {2019},
	note = {arXiv: 1908.10626},
	keywords = {Condensed Matter - Mesoscale and Nanoscale Physics},
	pages = {174410},
	file = {Paterson et al. - 2019 - Heisenberg pseudo-exchange and emergent anisotropi.pdf:/home/thomaav/Zotero/storage/MP92G7QA/Paterson et al. - 2019 - Heisenberg pseudo-exchange and emergent anisotropi.pdf:application/pdf}
}

@article{paterson_heisenberg_2019-1,
	title = {Heisenberg pseudo-exchange and emergent anisotropies in field-driven pinwheel artificial spin ice},
	volume = {100},
	issn = {2469-9950, 2469-9969},
	url = {http://arxiv.org/abs/1908.10626},
	doi = {10.1103/PhysRevB.100.174410},
	abstract = {Rotating all islands in square artificial spin ice (ASI) uniformly about their centres gives rise to the recently reported pinwheel ASI. At angles around 45\${\textasciicircum}{\textbackslash}mathrm\{o\}\$, the antiferromagnetic ordering changes to ferromagnetic and the magnetic configurations of the system exhibit near-degeneracy, making it particularly sensitive to small perturbations. We investigate through micromagnetic modelling the influence of dipolar fields produced by physically extended islands in field-driven magnetisation processes in pinwheel arrays, and compare the results to hysteresis experiments performed in-situ using Lorentz transmission electron microscopy. We find that magnetisation end-states induce a Heisenberg pseudo-exchange interaction that governs both the inter-island coupling and the resultant array reversal process. Symmetry reduction gives rise to anisotropies and array-corner mediated avalanche reversals through a cascade of nearest-neighbour (NN) islands. The symmetries of the anisotropy axes are related to those of the geometrical array but are misaligned to the array axes as a result of the correlated interactions between neighbouring islands. The NN dipolar coupling is reduced by decreasing the island size and, using this property, we track the transition from the strongly coupled regime towards the pure point dipole one and observe modification of the ferromagnetic array reversal process. Our results shed light on important aspects of the interactions in pinwheel ASI, and demonstrate a mechanism by which their properties may be tuned for use in a range of fundamental research and spintronic applications.},
	language = {en},
	number = {17},
	urldate = {2020-01-10},
	journal = {Physical Review B},
	author = {Paterson, Gary W. and Macauley, Gavin M. and Li, Yue and Macêdo, Rair and Ferguson, Ciaran and Morley, Sophie A. and Rosamond, Mark C. and Linfield, Edmund H. and Marrows, Christopher H. and Stamps, Robert L. and McVitie, Stephen},
	month = nov,
	year = {2019},
	note = {arXiv: 1908.10626},
	keywords = {Condensed Matter - Mesoscale and Nanoscale Physics},
	pages = {174410},
	file = {Paterson et al. - 2019 - Heisenberg pseudo-exchange and emergent anisotropi.pdf:/home/thomaav/Zotero/storage/QFG45C7H/Paterson et al. - 2019 - Heisenberg pseudo-exchange and emergent anisotropi.pdf:application/pdf}
}

@article{paterson_heisenberg_2019-2,
	title = {Heisenberg pseudo-exchange and emergent anisotropies in field-driven pinwheel artificial spin ice},
	volume = {100},
	issn = {2469-9950, 2469-9969},
	url = {http://arxiv.org/abs/1908.10626},
	doi = {10.1103/PhysRevB.100.174410},
	abstract = {Rotating all islands in square artificial spin ice (ASI) uniformly about their centres gives rise to the recently reported pinwheel ASI. At angles around 45\${\textasciicircum}{\textbackslash}mathrm\{o\}\$, the antiferromagnetic ordering changes to ferromagnetic and the magnetic configurations of the system exhibit near-degeneracy, making it particularly sensitive to small perturbations. We investigate through micromagnetic modelling the influence of dipolar fields produced by physically extended islands in field-driven magnetisation processes in pinwheel arrays, and compare the results to hysteresis experiments performed in-situ using Lorentz transmission electron microscopy. We find that magnetisation end-states induce a Heisenberg pseudo-exchange interaction that governs both the inter-island coupling and the resultant array reversal process. Symmetry reduction gives rise to anisotropies and array-corner mediated avalanche reversals through a cascade of nearest-neighbour (NN) islands. The symmetries of the anisotropy axes are related to those of the geometrical array but are misaligned to the array axes as a result of the correlated interactions between neighbouring islands. The NN dipolar coupling is reduced by decreasing the island size and, using this property, we track the transition from the strongly coupled regime towards the pure point dipole one and observe modification of the ferromagnetic array reversal process. Our results shed light on important aspects of the interactions in pinwheel ASI, and demonstrate a mechanism by which their properties may be tuned for use in a range of fundamental research and spintronic applications.},
	language = {en},
	number = {17},
	urldate = {2020-01-10},
	journal = {Physical Review B},
	author = {Paterson, Gary W. and Macauley, Gavin M. and Li, Yue and Macêdo, Rair and Ferguson, Ciaran and Morley, Sophie A. and Rosamond, Mark C. and Linfield, Edmund H. and Marrows, Christopher H. and Stamps, Robert L. and McVitie, Stephen},
	month = nov,
	year = {2019},
	note = {arXiv: 1908.10626},
	keywords = {Condensed Matter - Mesoscale and Nanoscale Physics},
	pages = {174410},
	file = {Paterson et al. - 2019 - Heisenberg pseudo-exchange and emergent anisotropi.pdf:/home/thomaav/Zotero/storage/97YEX5R8/Paterson et al. - 2019 - Heisenberg pseudo-exchange and emergent anisotropi.pdf:application/pdf}
}

@article{budrikis_athermal_nodate,
	title = {Athermal dynamics of artiﬁcial spin ice: disorder, edge and ﬁeld protocol eﬀects},
	language = {en},
	author = {Budrikis, Zoe},
	pages = {205},
	file = {Budrikis - Athermal dynamics of artiﬁcial spin ice disorder,.pdf:/home/thomaav/Zotero/storage/EXCQ9GYC/Budrikis - Athermal dynamics of artiﬁcial spin ice disorder,.pdf:application/pdf}
}

@article{farkas_computational_2016,
	title = {Computational analysis of memory capacity in echo state networks},
	volume = {83},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608016300946},
	doi = {10.1016/j.neunet.2016.07.012},
	abstract = {Reservoir computing became very popular due to its potential for efficient design of recurrent neural networks, exploiting the computational properties of the reservoir structure. Various approaches, ranging from appropriate reservoir initialization to its optimization by training have been proposed. In this paper, we extend our previous work and focus on short-term memory capacity, introduced by Jaeger in case of echo state networks. Memory capacity has been previously shown to peak at criticality, when the network switches from a stable regime to an unstable dynamic regime. Using computational experiments with nonlinear ESNs, we systematically analyze the memory capacity from the perspective of several parameters and their relationship, namely the input and reservoir weights scaling, reservoir size and its sparsity. We also derive and test two gradient descent based orthogonalization procedures for recurrent weights matrix, which considerably increase the memory capacity, approaching the upper bound, which is equal to the reservoir size, as proved for linear reservoirs. Orthogonalization procedures are discussed in the context of existing methods and their benefit is assessed.},
	language = {en},
	urldate = {2020-02-17},
	journal = {Neural Networks},
	author = {Farkaš, Igor and Bosák, Radomír and Gergeľ, Peter},
	month = nov,
	year = {2016},
	pages = {109--120},
	file = {Farkaš et al. - 2016 - Computational analysis of memory capacity in echo .pdf:/home/thomaav/Zotero/storage/AVHEXRIC/Farkaš et al. - 2016 - Computational analysis of memory capacity in echo .pdf:application/pdf}
}

@article{boedecker_information_2012,
	title = {Information processing in echo state networks at the edge of chaos},
	volume = {131},
	issn = {1431-7613, 1611-7530},
	url = {http://link.springer.com/10.1007/s12064-011-0146-8},
	doi = {10.1007/s12064-011-0146-8},
	language = {en},
	number = {3},
	urldate = {2020-02-17},
	journal = {Theory in Biosciences},
	author = {Boedecker, Joschka and Obst, Oliver and Lizier, Joseph T. and Mayer, N. Michael and Asada, Minoru},
	month = sep,
	year = {2012},
	pages = {205--213},
	file = {Boedecker et al. - 2012 - Information processing in echo state networks at t.pdf:/home/thomaav/Zotero/storage/T9ULC9KE/Boedecker et al. - 2012 - Information processing in echo state networks at t.pdf:application/pdf}
}

@article{millea_explorations_nodate,
	title = {Explorations in {Echo} {State} {Networks}},
	language = {en},
	author = {Millea, Adrian and Thesis, Master’s},
	pages = {90},
	file = {Millea and Thesis - Explorations in Echo State Networks.pdf:/home/thomaav/Zotero/storage/Z47HMIGX/Millea and Thesis - Explorations in Echo State Networks.pdf:application/pdf}
}

@article{mowshowitz_entropy_2012,
	title = {Entropy and the {Complexity} of {Graphs} {Revisited}},
	volume = {14},
	issn = {1099-4300},
	url = {http://www.mdpi.com/1099-4300/14/3/559},
	doi = {10.3390/e14030559},
	abstract = {This paper presents a taxonomy and overview of approaches to the measurement of graph and network complexity. The taxonomy distinguishes between deterministic (e.g., Kolmogorov complexity) and probabilistic approaches with a view to placing entropy-based probabilistic measurement in context. Entropy-based measurement is the main focus of the paper. Relationships between the different entropy functions used to measure complexity are examined; and intrinsic (e.g., classical measures) and extrinsic (e.g., Ko¨rner entropy) variants of entropy-based models are discussed in some detail.},
	language = {en},
	number = {3},
	urldate = {2020-02-18},
	journal = {Entropy},
	author = {Mowshowitz, Abbe and Dehmer, Matthias},
	month = mar,
	year = {2012},
	pages = {559--570},
	file = {Mowshowitz and Dehmer - 2012 - Entropy and the Complexity of Graphs Revisited.pdf:/home/thomaav/Zotero/storage/YE4UPZC7/Mowshowitz and Dehmer - 2012 - Entropy and the Complexity of Graphs Revisited.pdf:application/pdf}
}

@article{mowshowitz_entropy_2012-1,
	title = {Entropy and the {Complexity} of {Graphs} {Revisited}},
	volume = {14},
	issn = {1099-4300},
	url = {http://www.mdpi.com/1099-4300/14/3/559},
	doi = {10.3390/e14030559},
	abstract = {This paper presents a taxonomy and overview of approaches to the measurement of graph and network complexity. The taxonomy distinguishes between deterministic (e.g., Kolmogorov complexity) and probabilistic approaches with a view to placing entropy-based probabilistic measurement in context. Entropy-based measurement is the main focus of the paper. Relationships between the different entropy functions used to measure complexity are examined; and intrinsic (e.g., classical measures) and extrinsic (e.g., Ko¨rner entropy) variants of entropy-based models are discussed in some detail.},
	language = {en},
	number = {3},
	urldate = {2020-02-18},
	journal = {Entropy},
	author = {Mowshowitz, Abbe and Dehmer, Matthias},
	month = mar,
	year = {2012},
	pages = {559--570},
	file = {Mowshowitz and Dehmer - 2012 - Entropy and the Complexity of Graphs Revisited.pdf:/home/thomaav/Zotero/storage/P5VKY4UW/Mowshowitz and Dehmer - 2012 - Entropy and the Complexity of Graphs Revisited.pdf:application/pdf}
}

@article{watts_collective_1998,
	title = {Collective dynamics of ‘small-world’ networks},
	volume = {393},
	language = {en},
	author = {Watts, Duncan J and Strogatz, Steven H},
	year = {1998},
	pages = {3},
	file = {Watts and Strogatz - 1998 - Collective dynamics of ‘small-world’ networks.pdf:/home/thomaav/Zotero/storage/59J9KFTT/Watts and Strogatz - 1998 - Collective dynamics of ‘small-world’ networks.pdf:application/pdf}
}

@article{waxman_routing_1988,
	title = {Routing of multipoint connections},
	volume = {6},
	issn = {07338716},
	url = {http://ieeexplore.ieee.org/document/12889/},
	doi = {10.1109/49.12889},
	abstract = {This paper addresses the problem of routing connections in a large scale packet switched network supporting multipoint communications. We give a formal definition of several versions of the multipoint problem including both static and dynamic versions. We look at the Steiner tree problem as an example of the static problem and consider the experimental performance of two approximation algorithms for this problem. Then we consider a weighted greedy algorithm for a version of the dynamic problem which allows for endpoints to come and go during the life of a connection. One of the static algorithms serves as a reference to measure the performance of our weighted greedy algorithm in a series of experiments. In our conclusion, we consider areas for continued research including analysis of the probabilistic performance of multipoint approximation algorithms.},
	language = {en},
	number = {9},
	urldate = {2020-02-19},
	journal = {IEEE Journal on Selected Areas in Communications},
	author = {Waxman, B.M.},
	month = dec,
	year = {1988},
	pages = {1617--1622},
	file = {Waxman - 1988 - Routing of multipoint connections.pdf:/home/thomaav/Zotero/storage/TZ78SKG4/Waxman - 1988 - Routing of multipoint connections.pdf:application/pdf}
}

@inproceedings{dale_neuroevolution_2018,
	address = {Kyoto, Japan},
	title = {Neuroevolution of hierarchical reservoir computers},
	isbn = {978-1-4503-5618-3},
	url = {http://dl.acm.org/citation.cfm?doid=3205455.3205520},
	doi = {10.1145/3205455.3205520},
	abstract = {Reservoir Computers such as Echo State Networks (ESN) represent an alternative recurrent neural network model that provides fast training and state-of-the-art performances for supervised learning problems. Classic ESNs suffer from two limitations; hyperparameter selection and learning of multiple temporal and spatial scales. To learn multiple scales, hierarchies are proposed, and to overcome manual tuning, optimisation is used. However, the autonomous design of hierarchies and optimisation of multi-reservoir systems has not yet been demonstrated. In this work, an evolvable architecture is proposed called Reservoir-of-Reservoirs (RoR) where sub-networks of neurons (ESNs) are interconnected to form the reservoir. The design of each sub-network, hyperparameters, global connectivity and its hierarchical structure are evolved using a genetic algorithm (GA) called the Microbial GA.},
	language = {en},
	urldate = {2020-02-29},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference} on - {GECCO} '18},
	publisher = {ACM Press},
	author = {Dale, Matthew},
	year = {2018},
	pages = {410--417},
	file = {Dale - 2018 - Neuroevolution of hierarchical reservoir computers.pdf:/home/thomaav/Zotero/storage/2PH8LFTG/Dale - 2018 - Neuroevolution of hierarchical reservoir computers.pdf:application/pdf}
}

@misc{noauthor_httpsenwikipediaorgwikilong-term_memory_nodate,
	title = {https://en.wikipedia.org/wiki/{Long}-term\_memory}
}

@misc{noauthor_httpsenwikipediaorgwikineuroanatomy_of_memory_nodate,
	title = {https://en.wikipedia.org/wiki/{Neuroanatomy}\_of\_memory}
}

@misc{noauthor_httpsenwikipediaorgwikimemory_nodate,
	title = {https://en.wikipedia.org/wiki/{Memory}}
}

@article{kim_musical_2004,
	title = {Musical training-induced functional reorganization of the adult brain: {Functional} magnetic resonance imaging and transcranial magnetic stimulation study on amateur string players},
	volume = {23},
	issn = {1065-9471, 1097-0193},
	shorttitle = {Musical training-induced functional reorganization of the adult brain},
	url = {http://doi.wiley.com/10.1002/hbm.20058},
	doi = {10.1002/hbm.20058},
	abstract = {We used the combined technique of functional magnetic resonance imaging (fMRI) and transcranial magnetic stimulation (TMS) to observe changes that occur in adult brains after the practice of stringed musical instruments. We carried out fMRI on eight volunteers (aged 20 –22 years): ﬁve novices and three individuals who had discontinued practice for more than 5 years. The motor paradigm contained a repetitive lift-abduction/fall-adduction movement of the left/right little ﬁnger, carried out with maximum efforts without pacing. The sensory paradigm was to stimulate the same little ﬁnger using a string. In parallel to the fMRI acquisition, TMS motor maps for the little ﬁnger were obtained using a frameless stereotactic neuronavigation system. After the baseline study, each participant began to learn a stringed instrument. Newly developed fMRI activations for the left little ﬁnger were observed 6 months after practice at multiple brain regions including inferior parietal lobule, premotor area (PMA), left precuneus, right anterior superior temporal gyrus, and posterior middle temporal gyrus. In contrast, new activations were rarely observed for the right little ﬁnger. The TMS study revealed new motor representation sites for the left little ﬁnger in the PMA or supplementary motor area (SMA). Unexpectedly, TMS motor maps for the right little ﬁnger were reduced signiﬁcantly. Among new fMRI activations for sensory stimuli of the left little ﬁnger, the cluster of highest activation was located in the SMA. Collectively, these data provide insight into orchestrated reorganization of the sensorimotor and temporal association cortices contributing to the skillful ﬁngering and musical processing after the practice of playing stringed instruments. Hum Brain Mapp 23:188 –199, 2004. © 2004 Wiley-Liss, Inc.},
	language = {en},
	number = {4},
	urldate = {2020-05-02},
	journal = {Human Brain Mapping},
	author = {Kim, Dong-Eog and Shin, Min-Jung and Lee, Kyoung-Min and Chu, Kon and Woo, Sung Ho and Kim, Young Ro and Song, Eun-Cheol and Lee, Jun-Won and Park, Seong-Ho and Roh, Jae-Kyu},
	month = dec,
	year = {2004},
	pages = {188--199},
	file = {Kim et al. - 2004 - Musical training-induced functional reorganization.pdf:/home/thomaav/Zotero/storage/8BYXGFNR/Kim et al. - 2004 - Musical training-induced functional reorganization.pdf:application/pdf}
}

@article{haslinger_reduced_2004,
	title = {Reduced recruitment of motor association areas during bimanual coordination in concert pianists: {Bimanual} {Coordination} in {Pianists}},
	volume = {22},
	issn = {10659471},
	shorttitle = {Reduced recruitment of motor association areas during bimanual coordination in concert pianists},
	url = {http://doi.wiley.com/10.1002/hbm.20028},
	doi = {10.1002/hbm.20028},
	abstract = {Bimanual motor coordination is essential for piano playing. The functional neuronal substrate for high-level bimanual performance achieved by professional pianists is unclear. We compared professional pianists to musically na¨ıve controls while carrying out in-phase (mirror) and anti-phase (parallel) bimanual sequential ﬁnger movements during functional magnetic resonance imaging (fMRI). This task corresponds to bimanually playing scales practiced daily by pianists from the beginning of piano playing. Musicians and controls showed signiﬁcantly different functional activation patterns. When comparing performance of parallel movements to rest, musically na¨ıve controls showed stronger activations than did pianists within a network including anterior cingulate cortex, right dorsal premotor cortex, both cerebellar hemispheres, and right basal ganglia. The direct comparison of bimanual parallel to mirror movements between both groups revealed stronger signal increases in controls within mesial premotor cortex (SMA), bilateral cerebellar hemispheres and vermis, bilateral prefrontal cortex, left ventral premotor cortex, right anterior insula, and right basal ganglia. These ﬁndings suggest increased efﬁciency of cortical and subcortical systems for bimanual movement control in musicians. This may be fundamental to achieve high-level motor skills allowing the musician to focus on artistic aspects of musical performance. Hum. Brain Mapp. 22:206 –215, 2004.},
	language = {en},
	number = {3},
	urldate = {2020-05-02},
	journal = {Human Brain Mapping},
	author = {Haslinger, Bernhard and Erhard, Peter and Altenmüller, Eckart and Hennenlotter, Andreas and Schwaiger, Markus and Gräfin von Einsiedel, Helga and Rummeny, Ernst and Conrad, Bastian and Ceballos-Baumann, Andrés O.},
	month = jul,
	year = {2004},
	pages = {206--215},
	file = {Haslinger et al. - 2004 - Reduced recruitment of motor association areas dur.pdf:/home/thomaav/Zotero/storage/2M59U444/Haslinger et al. - 2004 - Reduced recruitment of motor association areas dur.pdf:application/pdf}
}

@article{gathercole_working_2019,
	title = {Working memory training involves learning new skills},
	volume = {105},
	issn = {0749596X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0749596X18300871},
	doi = {10.1016/j.jml.2018.10.003},
	abstract = {We present a new framework characterizing training-induced changes in WM as the acquisition of novel cognitive routines akin to learning a new skill. Predictions were tested in three studies analyzing the transfer between WM tasks following WM training. Study 1 reports a meta-analysis establishing substantial transfer when trained and untrained tasks shared either a serial recall, complex span or backward span paradigm. Transfer was weaker for serial recall of verbal than visuo-spatial material, suggesting that this paradigm is served by an existing verbal STM system and does not require a new routine. Re-analysis of published WM training data in Study 2 showed that transfer was restricted to tasks sharing properties proposed to require new routines. In a reanalysis of data from four studies, Study 3 demonstrated that transfer was greatest for children with higher fluid cognitive abilities. These findings suggest that development of new routines depends on general cognitive resources and that they can only be applied to other similarly-structured tasks.},
	language = {en},
	urldate = {2020-05-02},
	journal = {Journal of Memory and Language},
	author = {Gathercole, Susan E. and Dunning, Darren L. and Holmes, Joni and Norris, Dennis},
	month = apr,
	year = {2019},
	pages = {19--42},
	file = {Gathercole et al. - 2019 - Working memory training involves learning new skil.pdf:/home/thomaav/Zotero/storage/SK6CEKP7/Gathercole et al. - 2019 - Working memory training involves learning new skil.pdf:application/pdf}
}

@incollection{meister_memory_2017,
	title = {Memory},
	isbn = {978-0-12-802381-5},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780128023815000506},
	language = {en},
	urldate = {2020-05-02},
	booktitle = {Conn's {Translational} {Neuroscience}},
	publisher = {Elsevier},
	author = {Meister, M.L.R. and Buffalo, E.A.},
	year = {2017},
	doi = {10.1016/B978-0-12-802381-5.00050-6},
	pages = {693--708},
	file = {Meister and Buffalo - 2017 - Memory.pdf:/home/thomaav/Zotero/storage/H8Z8NXLM/Meister and Buffalo - 2017 - Memory.pdf:application/pdf}
}

@article{noauthor_notitle_nodate
}

@techreport{aven_exploring_2019,
	type = {Project report in {TDT4501}},
	title = {Exploring {Reservoir} {Computing} with physical limits using {Echo} {State} {Networks}},
	abstract = {Reservoir Computing (RC) emerged as an alternative framework to the traditional gradient descent methods for training Recurrent Neural Networks (RNNs). Interestingly, there is no need for the reservoir to be an artiﬁcial neural network, and in recent years a wide range of physical reservoirs have been realized, ranging from optical laser circuits and nanomagnetic assemblies to biological neural networks. The computational performance of such physical substrates is likely to be closely related to common physical limitations, e.g. noise, measurement accuracy, partially visible reservoir state, and physical morphology. Here we investigate fundamental properties of physical reservoirs using Echo State Network (ESN) simulations, offering insights into impairments resulting from such limitations, which in the wider context will help improve the design of further substrates.},
	language = {en},
	institution = {Department of Computer Science, NTNU -- Norwegian University of Science and Technology},
	author = {Aven, Thomas},
	month = dec,
	year = {2019},
	pages = {8},
	file = {Aven - Exploring Reservoir Computing with physical limits.pdf:/home/thomaav/Zotero/storage/N4ZQIP2V/Aven - Exploring Reservoir Computing with physical limits.pdf:application/pdf}
}

@article{mackey_oscillation_1977,
	title = {Oscillation and chaos in physiological control systems},
	volume = {197},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.267326},
	doi = {10.1126/science.267326},
	language = {en},
	number = {4300},
	urldate = {2020-05-06},
	journal = {Science},
	author = {Mackey, M. and Glass, L},
	month = jul,
	year = {1977},
	pages = {287--289},
	file = {Mackey and Glass - 1977 - Oscillation and chaos in physiological control sys.pdf:/home/thomaav/Zotero/storage/S69TFYK3/Mackey and Glass - 1977 - Oscillation and chaos in physiological control sys.pdf:application/pdf}
}

@article{chrol-cannon_correlation_2014,
	title = {On the {Correlation} between {Reservoir} {Metrics} and {Performance} for {Time} {Series} {Classification} under the {Influence} of {Synaptic} {Plasticity}},
	volume = {9},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0101792},
	doi = {10.1371/journal.pone.0101792},
	abstract = {Reservoir computing provides a simpler paradigm of training recurrent networks by initialising and adapting the recurrent connections separately to a supervised linear readout. This creates a problem, though. As the recurrent weights and topology are now separated from adapting to the task, there is a burden on the reservoir designer to construct an effective network that happens to produce state vectors that can be mapped linearly into the desired outputs. Guidance in forming a reservoir can be through the use of some established metrics which link a number of theoretical properties of the reservoir computing paradigm to quantitative measures that can be used to evaluate the effectiveness of a given design. We provide a comprehensive empirical study of four metrics; class separation, kernel quality, Lyapunov’s exponent and spectral radius. These metrics are each compared over a number of repeated runs, for different reservoir computing set-ups that include three types of network topology and three mechanisms of weight adaptation through synaptic plasticity. Each combination of these methods is tested on two time-series classification problems. We find that the two metrics that correlate most strongly with the classification performance are Lyapunov’s exponent and kernel quality. It is also evident in the comparisons that these two metrics both measure a similar property of the reservoir dynamics. We also find that class separation and spectral radius are both less reliable and less effective in predicting performance.},
	language = {en},
	number = {7},
	urldate = {2020-05-06},
	journal = {PLoS ONE},
	author = {Chrol-Cannon, Joseph and Jin, Yaochu},
	editor = {Lytton, William W.},
	month = jul,
	year = {2014},
	pages = {e101792},
	file = {Chrol-Cannon and Jin - 2014 - On the Correlation between Reservoir Metrics and P.PDF:/home/thomaav/Zotero/storage/U8F6YH92/Chrol-Cannon and Jin - 2014 - On the Correlation between Reservoir Metrics and P.PDF:application/pdf}
}

@article{haluszczynski_reservoir_2020,
	title = {Reservoir computing: {Reducing} network size and improving prediction stability},
	shorttitle = {Reservoir computing},
	url = {http://arxiv.org/abs/2003.03178},
	abstract = {Reservoir computing is a very promising approach for the prediction of complex nonlinear dynamical systems. Besides capturing the exact short-term trajectories of nonlinear systems it has also proved to reproduce its characteristic long-term properties very accurately. However, predictions do not always work equivalently well. It has been shown that both short- and long-term predictions vary significantly among different random realizations of the reservoir. In order to gain an understanding on when reservoir computing works best we investigate some differential properties of the respective realization of the reservoir in a systematic way. We find that removing nodes that correspond to the largest weights in the output regression matrix reduces outliers and improves overall prediction quality. Moreover this allows to effectively reduce the network size and therefore increase computational efficiency. In addition, we introduce a nonlinear scaling factor in the hyperbolic tangent of the activation function in order to adjust the response of the activation function to the range of values of the input variables of the nodes. This also significantly reduces the number of outliers and increases both the short and long-term prediction quality for the nonlinear systems investigated in this study. Our results demonstrate that a large optimization potential lies in the systematical refinement of the differential reservoir properties for a given data set.},
	language = {en},
	urldate = {2020-05-25},
	journal = {arXiv:2003.03178 [physics]},
	author = {Haluszczynski, Alexander and Aumeier, Jonas and Herteux, Joschka and Räth, Christoph},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.03178},
	keywords = {Physics - Data Analysis, Statistics and Probability},
	file = {Haluszczynski et al. - 2020 - Reservoir computing Reducing network size and imp.pdf:/home/thomaav/Zotero/storage/MRY9FJEA/Haluszczynski et al. - 2020 - Reservoir computing Reducing network size and imp.pdf:application/pdf}
}

@article{langton_computation_1990-1,
	title = {Computation at the edge of chaos: {Phase} transitions and emergent computation},
	volume = {42},
	issn = {01672789},
	shorttitle = {Computation at the edge of chaos},
	url = {https://linkinghub.elsevier.com/retrieve/pii/016727899090064V},
	doi = {10.1016/0167-2789(90)90064-V},
	language = {en},
	number = {1-3},
	urldate = {2020-05-28},
	journal = {Physica D: Nonlinear Phenomena},
	author = {Langton, Chris G.},
	month = jun,
	year = {1990},
	pages = {12--37},
	file = {Langton - 1990 - Computation at the edge of chaos Phase transition.pdf:/home/thomaav/Zotero/storage/WF8ZGT8R/Langton - 1990 - Computation at the edge of chaos Phase transition.pdf:application/pdf}
}

@article{heylighen_science_1999-1,
	title = {The {Science} of {Self}-{Organization} and {Adaptivity}},
	abstract = {The theory of self-organization and adaptivity has grown out of a variety of disciplines, including thermodynamics, cybernetics and computer modelling. The present article reviews its most important concepts and principles. It starts with an intuitive overview, illustrated by the examples of magnetization and Bénard convection, and concludes with the basics of mathematical modelling. Self-organization can be defined as the spontaneous creation of a globally coherent pattern out of local interactions. Because of its distributed character, this organization tends to be robust, resisting perturbations. The dynamics of a self-organizing system is typically non-linear, because of circular or feedback relations between the components. Positive feedback leads to an explosive growth, which ends when all components have been absorbed into the new configuration, leaving the system in a stable, negative feedback state. Non-linear systems have in general several stable states, and this number tends to increase (bifurcate) as an increasing input of energy pushes the system farther from its thermodynamic equilibrium. To adapt to a changing environment, the system needs a variety of stable states that is large enough to react to all perturbations but not so large as to make its evolution uncontrollably chaotic. The most adequate states are selected according to their fitness, either directly by the environment, or by subsystems that have adapted to the environment at an earlier stage. Formally, the basic mechanism underlying self-organization is the (often noise-driven) variation which explores different regions in the system’s state space until it enters an attractor. This precludes further variation outside the attractor, and thus restricts the freedom of the system’s components to behave independently. This is equivalent to the increase of coherence, or decrease of statistical entropy, that defines selforganization.},
	language = {en},
	author = {Heylighen, Francis},
	year = {1999},
	pages = {27},
	file = {Heylighen - THE SCIENCE OF SELF- ORGANIZATION AND ADAPTIVITY.pdf:/home/thomaav/Zotero/storage/WNL43XTE/Heylighen - THE SCIENCE OF SELF- ORGANIZATION AND ADAPTIVITY.pdf:application/pdf}
}

@incollection{jayne_information_2012,
	address = {Berlin, Heidelberg},
	title = {Information {Theoretic} {Self}-organised {Adaptation} in {Reservoirs} for {Temporal} {Memory} {Tasks}},
	volume = {311},
	isbn = {978-3-642-32908-1 978-3-642-32909-8},
	url = {http://link.springer.com/10.1007/978-3-642-32909-8_4},
	abstract = {Recurrent neural networks of the Reservoir Computing (RC) type have been found useful in various time-series processing tasks with inherent non-linearity and requirements of temporal memory. Here with the aim to obtain extended temporal memory in generic delayed response tasks, we combine a generalised intrinsic plasticity mechanism with an information storage based neuron leak adaptation rule in a self-organised manner. This results in adaptation of neuron local memory in terms of leakage along with inherent homeostatic stability. Experimental results on two benchmark tasks conﬁrm the extended performance of this system as compared to a static RC and RC with only intrinsic plasticity. Furthermore, we demonstrate the ability of the system to solve long temporal memory tasks via a simulated T-shaped maze navigation scenario.},
	language = {en},
	urldate = {2020-05-28},
	booktitle = {Engineering {Applications} of {Neural} {Networks}},
	publisher = {Springer Berlin Heidelberg},
	author = {Dasgupta, Sakyasingha and Wörgötter, Florentin and Manoonpong, Poramate},
	editor = {Jayne, Chrisina and Yue, Shigang and Iliadis, Lazaros},
	year = {2012},
	doi = {10.1007/978-3-642-32909-8_4},
	note = {Series Title: Communications in Computer and Information Science},
	pages = {31--40},
	file = {Dasgupta et al. - 2012 - Information Theoretic Self-organised Adaptation in.pdf:/home/thomaav/Zotero/storage/GQ58EEVV/Dasgupta et al. - 2012 - Information Theoretic Self-organised Adaptation in.pdf:application/pdf}
}

@inproceedings{harding_evolution_2005,
	address = {Washington, DC, USA},
	title = {Evolution {In} {Materio} : {A} {Real}-{Time} {Robot} {Controller} in {Liquid} {Crystal}},
	isbn = {978-0-7695-2399-6},
	shorttitle = {Evolution {In} {Materio}},
	url = {http://ieeexplore.ieee.org/document/1508505/},
	doi = {10.1109/EH.2005.22},
	abstract = {Although intrinsic evolution has been shown to be capable of exploiting the physical properties of materials to solve problems, most researchers have chosen to limit themselves to using standard electronic components. However, it has been previously argued that because such components are human designed and intentionally have predictable responses, they may not be the most suitable medium to use when trying to get a naturally inspired search technique to solve a problem. Indeed allowing computer controlled evolution (CCE) to manipulate novel physical media can allow much greater scope for the discovery of unconventional solutions. Last year the authors demonstrated, for the ﬁrst time, that CCE could manipulate liquid crystal to perform signal processing tasks (i.e frequency discrimination). In this paper we show that CCE can use liquid crystal to solve the much harder problem of controlling a robot in real time to navigate in an environment to reach an obstructed destination point.},
	language = {en},
	urldate = {2020-05-28},
	booktitle = {2005 {NASA}/{DoD} {Conference} on {Evolvable} {Hardware} ({EH}'05)},
	publisher = {IEEE},
	author = {Harding, S. and Miller, J.F.},
	year = {2005},
	pages = {229--238},
	file = {Harding and Miller - 2005 - Evolution In Materio  A Real-Time Robot Controlle.pdf:/home/thomaav/Zotero/storage/IMC5YUXS/Harding and Miller - 2005 - Evolution In Materio  A Real-Time Robot Controlle.pdf:application/pdf}
}

@article{antonelo_generative_2007,
	title = {Generative {Modeling} of {Autonomous} {Robots} and their {Environments} using {Reservoir} {Computing}},
	volume = {26},
	issn = {1370-4621, 1573-773X},
	url = {http://link.springer.com/10.1007/s11063-007-9054-9},
	doi = {10.1007/s11063-007-9054-9},
	abstract = {Autonomous mobile robots form an important research topic in the ﬁeld of robotics due to their near-term applicability in the real world as domestic service robots. These robots must be designed in an efﬁcient way using training sequences. They need to be aware of their position in the environment and also need to create models of it for deliberative planning. These tasks have to be performed using a limited number of sensors with low accuracy, as well as with a restricted amount of computational power. In this contribution we show that the recently emerged paradigm of Reservoir Computing (RC) is very well suited to solve all of the above mentioned problems, namely learning by example, robot localization, map and path generation. Reservoir Computing is a technique which enables a system to learn any time-invariant ﬁlter of the input by training a simple linear regressor that acts on the states of a high-dimensional but random dynamic system excited by the inputs. In addition, RC is a simple technique featuring ease of training, and low computational and memory demands.},
	language = {en},
	number = {3},
	urldate = {2020-05-28},
	journal = {Neural Processing Letters},
	author = {Antonelo, Eric. A. and Schrauwen, Benjamin and Van Campenhout, Jan},
	month = dec,
	year = {2007},
	pages = {233--249},
	file = {Antonelo et al. - 2007 - Generative Modeling of Autonomous Robots and their.pdf:/home/thomaav/Zotero/storage/W5JMWBI6/Antonelo et al. - 2007 - Generative Modeling of Autonomous Robots and their.pdf:application/pdf}
}

@article{burgsteiner_imitation_2006,
	title = {Imitation learning with spiking neural networks and real-world devices},
	volume = {19},
	issn = {09521976},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0952197606000959},
	doi = {10.1016/j.engappai.2006.05.007},
	abstract = {This article is about a new approach in robotic learning systems. It provides a method to use a real-world device that operates in realtime, controlled through a simulated recurrent spiking neural network for robotic experiments. A randomly generated network is used as the main computational unit. Only the weights of the output units of this network are changed during training. It will be shown, that this simple type of a biological realistic spiking neural network—also known as a neural microcircuit—is able to imitate robot controllers like that incorporated in Braitenberg vehicles. A more non-linear type controller is imitated in a further experiment. In a different series of experiments that involve temporal memory reported in Burgsteiner et al. [2005. In: Proceedings of the 18th International Conference IEA/AIE. Lecture Notes in Artiﬁcial Intelligence. Springer, Berlin, pp. 121–130.] this approach also provided a basis for a movement prediction task. The results suggest that a neural microcircuit with a simple learning rule can be used as a sustainable robot controller for experiments in computational motor control.},
	language = {en},
	number = {7},
	urldate = {2020-05-28},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Burgsteiner, Harald},
	month = oct,
	year = {2006},
	pages = {741--752},
	file = {Burgsteiner - 2006 - Imitation learning with spiking neural networks an.pdf:/home/thomaav/Zotero/storage/BFNUZZGF/Burgsteiner - 2006 - Imitation learning with spiking neural networks an.pdf:application/pdf}
}

@incollection{hutchison_movement_2004,
	address = {Berlin, Heidelberg},
	title = {Movement {Generation} and {Control} with {Generic} {Neural} {Microcircuits}},
	volume = {3141},
	isbn = {978-3-540-23339-8 978-3-540-27835-1},
	url = {http://link.springer.com/10.1007/978-3-540-27835-1_20},
	abstract = {Simple linear readouts from generic neural microcircuit models can be trained to generate and control basic movements, e.g., reaching with an arm to various target points. After suitable training of these readouts on a small number of target points; reaching movements to nearby points can also be generated. Sensory or proprioceptive feedback turns out to improve the performance of the neural microcircuit model, if it arrives with a signiﬁcant delay of 25 to 100 ms. Furthermore, additional feedbacks of “prediction of sensory variables” are shown to improve the performance signiﬁcantly. Existing control methods in robotics that take the particular dynamics of sensors and actuators into account(“embodiment of robot control”) are taken one step further with this approach which provides methods for also using the “embodiment of computation”, i.e. the inherent dynamics and spatial structure of neural circuits, for the design of robot movement controllers.},
	language = {en},
	urldate = {2020-05-28},
	booktitle = {Biologically {Inspired} {Approaches} to {Advanced} {Information} {Technology}},
	publisher = {Springer Berlin Heidelberg},
	author = {Joshi, Prashant and Maass, Wolfgang},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Ijspeert, Auke Jan and Murata, Masayuki and Wakamiya, Naoki},
	year = {2004},
	doi = {10.1007/978-3-540-27835-1_20},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {258--273},
	file = {Joshi and Maass - 2004 - Movement Generation and Control with Generic Neura.pdf:/home/thomaav/Zotero/storage/KJPXF9GC/Joshi and Maass - 2004 - Movement Generation and Control with Generic Neura.pdf:application/pdf}
}

@article{gallicchio_comparison_2019,
	title = {Comparison between {DeepESNs} and gated {RNNs} on multivariate time-series prediction},
	url = {http://arxiv.org/abs/1812.11527},
	abstract = {We propose an experimental comparison between Deep Echo State Networks (DeepESNs) and gated Recurrent Neural Networks (RNNs) on multivariate time-series prediction tasks. In particular, we compare reservoir and fully-trained RNNs able to represent signals featured by multiple time-scales dynamics. The analysis is performed in terms of eﬃciency and prediction accuracy on 4 polyphonic music tasks. Our results show that DeepESN is able to outperform ESN in terms of prediction accuracy and eﬃciency. Whereas, between fully-trained approaches, Gated Recurrent Units (GRU) outperforms Long Short-Term Memory (LSTM) and simple RNN models in most cases. Overall, DeepESN turned out to be extremely more eﬃcient than others RNN approaches and the best solution in terms of prediction accuracy on 3 out of 4 tasks.},
	language = {en},
	urldate = {2020-05-28},
	journal = {arXiv:1812.11527 [cs, stat]},
	author = {Gallicchio, Claudio and Micheli, Alessio and Pedrelli, Luca},
	month = nov,
	year = {2019},
	note = {arXiv: 1812.11527},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Gallicchio et al. - 2019 - Comparison between DeepESNs and gated RNNs on mult.pdf:/home/thomaav/Zotero/storage/YIEL6DT4/Gallicchio et al. - 2019 - Comparison between DeepESNs and gated RNNs on mult.pdf:application/pdf}
}

@article{cho_learning_2014,
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}-{Decoder} for {Statistical} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1406.1078},
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder–Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a ﬁxedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder–Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	language = {en},
	urldate = {2020-05-29},
	journal = {arXiv:1406.1078 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {arXiv: 1406.1078},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language},
	file = {Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-.pdf:/home/thomaav/Zotero/storage/79X3MBL7/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-.pdf:application/pdf}
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insu cient, decaying error back ow. We brie y review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, e cient, gradient-based method called {\textbackslash}Long Short-Term Memory" (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error ow through {\textbackslash}constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error ow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with arti cial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, arti cial long time lag tasks that have never been solved by previous recurrent network algorithms.},
	language = {en},
	number = {8},
	urldate = {2020-05-29},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
	file = {Hochreiter and Schmidhuber - 1997 - Long Short-Term Memory.pdf:/home/thomaav/Zotero/storage/HHTWGS99/Hochreiter and Schmidhuber - 1997 - Long Short-Term Memory.pdf:application/pdf}
}

@article{gallicchio_tree_2013,
	title = {Tree {Echo} {State} {Networks}},
	volume = {101},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231212006571},
	doi = {10.1016/j.neucom.2012.08.017},
	abstract = {In this paper we present the Tree Echo State Network (TreeESN) model, generalizing the paradigm of Reservoir Computing to tree structured data. TreeESNs exploit an untrained generalized recursive reservoir, exhibiting extreme efﬁciency for learning in structured domains. In addition, we highlight through the paper other characteristics of the approach: First, we discuss the Markovian characterization of reservoir dynamics, extended to the case of tree domains, that is implied by the contractive setting of the TreeESN state transition function. Second, we study two types of state mapping functions to map the tree structured state of TreeESN into a ﬁxed-size feature representation for classiﬁcation or regression tasks. The critical role of the relation between the choice of the state mapping function and the Markovian characterization of the task is analyzed and experimentally investigated on both artiﬁcial and real-world tasks. Finally, experimental results on benchmark and real-world tasks show that the TreeESN approach, in spite of its efﬁciency, can achieve comparable results with state-of-theart, although more complex, neural and kernel based models for tree structured data.},
	language = {en},
	urldate = {2020-05-29},
	journal = {Neurocomputing},
	author = {Gallicchio, Claudio and Micheli, Alessio},
	month = feb,
	year = {2013},
	pages = {319--337},
	file = {Gallicchio and Micheli - 2013 - Tree Echo State Networks.pdf:/home/thomaav/Zotero/storage/HQD6UCZ9/Gallicchio and Micheli - 2013 - Tree Echo State Networks.pdf:application/pdf}
}

@inproceedings{gallicchio_graph_2010,
	address = {Barcelona, Spain},
	title = {Graph {Echo} {State} {Networks}},
	isbn = {978-1-4244-6916-1},
	url = {http://ieeexplore.ieee.org/document/5596796/},
	doi = {10.1109/IJCNN.2010.5596796},
	abstract = {In this paper we introduce the Graph Echo State Network (GraphESN) model, a generalization of the Echo State Network (ESN) approach to graph domains. GraphESNs allow for an efﬁcient approach to Recursive Neural Networks (RecNNs) modeling extended to deal with cyclic/acyclic, directed/undirected, labeled graphs.},
	language = {en},
	urldate = {2020-05-29},
	booktitle = {The 2010 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Gallicchio, Claudio and Micheli, Alessio},
	month = jul,
	year = {2010},
	pages = {1--8},
	file = {Gallicchio and Micheli - 2010 - Graph Echo State Networks.pdf:/home/thomaav/Zotero/storage/3CI2D9DX/Gallicchio and Micheli - 2010 - Graph Echo State Networks.pdf:application/pdf}
}

@article{yilmaz_reservoir_2014,
	title = {Reservoir {Computing} using {Cellular} {Automata}},
	url = {http://arxiv.org/abs/1410.0162},
	abstract = {We introduce a novel framework of reservoir computing. Cellular automaton is used as the reservoir of dynamical systems. Input is randomly projected onto the initial conditions of automaton cells and nonlinear computation is performed on the input via application of a rule in the automaton for a period of time. The evolution of the automaton creates a space-time volume of the automaton state space, and it is used as the reservoir. The proposed framework is capable of long shortterm memory and it requires orders of magnitude less computation compared to Echo State Networks. Also, for additive cellular automaton rules, reservoir features can be combined using Boolean operations, which provides a direct way for concept building and symbolic processing, and it is much more efﬁcient compared to state-of-the-art approaches.},
	language = {en},
	urldate = {2020-05-29},
	journal = {arXiv:1410.0162 [cs]},
	author = {Yilmaz, Ozgur},
	month = oct,
	year = {2014},
	note = {arXiv: 1410.0162},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {Yilmaz - 2014 - Reservoir Computing using Cellular Automata.pdf:/home/thomaav/Zotero/storage/K5ESIPYW/Yilmaz - 2014 - Reservoir Computing using Cellular Automata.pdf:application/pdf}
}

@article{barthelemy_spatial_2011,
	title = {Spatial {Networks}},
	volume = {499},
	issn = {03701573},
	url = {http://arxiv.org/abs/1010.0302},
	doi = {10.1016/j.physrep.2010.11.002},
	abstract = {Complex systems are very often organized under the form of networks where nodes and edges are embedded in space. Transportation and mobility networks, Internet, mobile phone networks, power grids, social and contact networks, neural networks, are all examples where space is relevant and where topology alone does not contain all the information. Characterizing and understanding the structure and the evolution of spatial networks is thus crucial for many different fields ranging from urbanism to epidemiology. An important consequence of space on networks is that there is a cost associated to the length of edges which in turn has dramatic effects on the topological structure of these networks. We will expose thoroughly the current state of our understanding of how the spatial constraints affect the structure and properties of these networks. We will review the most recent empirical observations and the most important models of spatial networks. We will also discuss various processes which take place on these spatial networks, such as phase transitions, random walks, synchronization, navigation, resilience, and disease spread.},
	language = {en},
	number = {1-3},
	urldate = {2020-05-30},
	journal = {Physics Reports},
	author = {Barthelemy, Marc},
	month = feb,
	year = {2011},
	note = {arXiv: 1010.0302},
	keywords = {Condensed Matter - Statistical Mechanics, Condensed Matter - Disordered Systems and Neural Networks, Computer Science - Social and Information Networks, Physics - Physics and Society, Quantitative Biology - Neurons and Cognition},
	pages = {1--101},
	file = {Barthelemy - 2011 - Spatial Networks.pdf:/home/thomaav/Zotero/storage/BE64EKA7/Barthelemy - 2011 - Spatial Networks.pdf:application/pdf}
}

@article{erdos_evolution_nodate,
	title = {On the evolution of random graphs},
	language = {en},
	author = {Erdős, P and Rényi, A},
	pages = {45},
	file = {Erdős and Rényi - ON THE EVOLUTION OF RANDOM GRAPHS by.pdf:/home/thomaav/Zotero/storage/3QJH5LM7/Erdős and Rényi - ON THE EVOLUTION OF RANDOM GRAPHS by.pdf:application/pdf}
}

@article{erdos_random_1959,
	title = {On random graphs},
	journal = {Publicationes Mathematicae Debrecen},
	author = {Erdős, Paul and Rényi, Alfréd},
	year = {1959},
	pages = {290--297}
}

@article{rodan_simple_2012-1,
	title = {Simple {Deterministically} {Constructed} {Cycle} {Reservoirs} with {Regular} {Jumps}},
	volume = {24},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/NECO_a_00297},
	doi = {10.1162/NECO_a_00297},
	language = {en},
	number = {7},
	urldate = {2020-05-30},
	journal = {Neural Computation},
	author = {Rodan, Ali and Tiňo, Peter},
	month = jul,
	year = {2012},
	pages = {1822--1852},
	file = {Rodan and Tiňo - 2012 - Simple Deterministically Constructed Cycle Reservo.pdf:/home/thomaav/Zotero/storage/8WU267SJ/Rodan and Tiňo - 2012 - Simple Deterministically Constructed Cycle Reservo.pdf:application/pdf}
}

@inproceedings{miller_evolution_2002,
	address = {Alexandria, VA, USA},
	title = {Evolution in materio: looking beyond the silicon box},
	isbn = {978-0-7695-1718-6},
	shorttitle = {Evolution in materio},
	url = {http://ieeexplore.ieee.org/document/1029882/},
	doi = {10.1109/EH.2002.1029882},
	abstract = {It is argued that natural evolution is, par excellence, an algorithm that exploits the physical properties of materials. Such an exploitation of the physical characteristics has already been demonstrated in intrinsic evolution of electronic circuits. This paper is an attempt to point the way toward the exciting possibility of using artificial evolution to directly exploit the properties of materials, possibly at a molecular level. It is suggested that this may be best accomplished in materials not normally associated with electronic functions. Electronic components have been prefected by human designers to construct circuits using the traditional top-down methodology. Workers in artificial intrinsic hardware evolution have with the best of motives, been abusing such components. It is a tribute to the amazing resourcefulness of a blind evolutionary process that it has been possible to evolve new circuits in this way. Artificial evolution may be much more effective when the configurable medium has a rich and complicated physics. This idea is discussed and particular examples that look extremely promising are given. Ultimately it may be possible to evolve entirely new technologies and new sorts of computational systems may be devised that confer many advantages over conventional electronic technology.},
	language = {en},
	urldate = {2020-05-31},
	booktitle = {Proceedings 2002 {NASA}/{DoD} {Conference} on {Evolvable} {Hardware}},
	publisher = {IEEE Comput. Soc},
	author = {Miller, J.F. and Downing, K.},
	year = {2002},
	pages = {167--176},
	file = {Miller and Downing - 2002 - Evolution in materio looking beyond the silicon b.pdf:/home/thomaav/Zotero/storage/JD4CWDHG/Miller and Downing - 2002 - Evolution in materio looking beyond the silicon b.pdf:application/pdf}
}

@article{checinski_synchronization_2020,
	title = {Synchronization properties and reservoir computing capability of hexagonal spintronic oscillator arrays},
	url = {http://arxiv.org/abs/2002.07060},
	abstract = {The influence of array geometry on synchronization properties of a 2-D oscillator array is investigated based on a comparison between a rectangular and a hexagonal grid. The Kuramoto model is solved for a nearest-neighbor case with periodic boundary conditions and for a small-scale, realistic coupling case with 1/r{\textasciicircum}3 decay characteristic of spintronic oscillators. In both cases, it is found that the hexagonal grid choice leads to lower synchronization threshold and higher emission power than its rectangular counterpart, which results from increased connectivity, as well as, in the realistic-coupling case, from decreased contributions of the array edges. Additionally, a more general spin-torque oscillator model including both amplitude and phase as degrees of freedom is employed for reservoir computing simulations, showing that by using hexagonal grid one can increase the short-term memory capacity but not the parity-check capacity of the system.},
	language = {en},
	urldate = {2020-05-31},
	journal = {arXiv:2002.07060 [cond-mat, physics:nlin, physics:physics]},
	author = {Chęciński, Jakub},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.07060},
	keywords = {Nonlinear Sciences - Adaptation and Self-Organizing Systems, Condensed Matter - Disordered Systems and Neural Networks, Physics - Applied Physics},
	file = {Chęciński - 2020 - Synchronization properties and reservoir computing.pdf:/home/thomaav/Zotero/storage/J2JHG42E/Chęciński - 2020 - Synchronization properties and reservoir computing.pdf:application/pdf}
}

@book{lavis_equilibrium_2015,
	address = {Dordrecht},
	series = {Theoretical and {Mathematical} {Physics}},
	title = {Equilibrium {Statistical} {Mechanics} of {Lattice} {Models}},
	isbn = {978-94-017-9429-9 978-94-017-9430-5},
	url = {http://link.springer.com/10.1007/978-94-017-9430-5},
	language = {en},
	urldate = {2020-06-04},
	publisher = {Springer Netherlands},
	author = {Lavis, David A.},
	year = {2015},
	doi = {10.1007/978-94-017-9430-5},
	file = {Lavis - 2015 - Equilibrium Statistical Mechanics of Lattice Model.pdf:/home/thomaav/Zotero/storage/ILCT8VKE/Lavis - 2015 - Equilibrium Statistical Mechanics of Lattice Model.pdf:application/pdf}
}

@article{kanao_reservoir_2019,
	title = {Reservoir {Computing} on {Spin}-{Torque} {Oscillator} {Array}},
	volume = {12},
	issn = {2331-7019},
	url = {http://arxiv.org/abs/1905.07937},
	doi = {10.1103/PhysRevApplied.12.024052},
	abstract = {We numerically study reservoir computing on a spin-torque oscillator (STO) array, describing the magnetization dynamics of the STO array by a nonlinear oscillator model. The STOs exhibit synchronized oscillation due to coupling by magnetic dipolar fields. We show that reservoir computing can be performed using the synchronized oscillation state. The performance can be improved by increasing the number of STOs. The performance becomes highest at the boundary between the synchronized and disordered states. Using an STO array, we can achieve higher performance than that of an echo-state network with similar number of units. This result indicates that STO arrays are promising for hardware implementation of reservoir computing.},
	language = {en},
	number = {2},
	urldate = {2020-06-04},
	journal = {Physical Review Applied},
	author = {Kanao, Taro and Suto, Hirofumi and Mizushima, Koichi and Goto, Hayato and Tanamoto, Tetsufumi and Nagasawa, Tazumi},
	month = aug,
	year = {2019},
	note = {arXiv: 1905.07937},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Mesoscale and Nanoscale Physics, Physics - Applied Physics},
	pages = {024052},
	file = {Kanao et al. - 2019 - Reservoir Computing on Spin-Torque Oscillator Arra.pdf:/home/thomaav/Zotero/storage/JQDB6ZZ8/Kanao et al. - 2019 - Reservoir Computing on Spin-Torque Oscillator Arra.pdf:application/pdf}
}

@article{opala_neuromorphic_2019,
	title = {Neuromorphic {Computing} in {Ginzburg}-{Landau} {Polariton}-{Lattice} {Systems}},
	volume = {11},
	issn = {2331-7019},
	url = {https://link.aps.org/doi/10.1103/PhysRevApplied.11.064029},
	doi = {10.1103/PhysRevApplied.11.064029},
	language = {en},
	number = {6},
	urldate = {2020-06-04},
	journal = {Physical Review Applied},
	author = {Opala, Andrzej and Ghosh, Sanjib and Liew, Timothy C.H. and Matuszewski, Michał},
	month = jun,
	year = {2019},
	pages = {064029},
	file = {Opala et al. - 2019 - Neuromorphic Computing in Ginzburg-Landau Polarito.pdf:/home/thomaav/Zotero/storage/AVET97BG/Opala et al. - 2019 - Neuromorphic Computing in Ginzburg-Landau Polarito.pdf:application/pdf}
}
