{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Imports.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Important for multiprocessing.\n",
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "# General plotting things.\n",
    "from plot import get_3d_subplot_axs\n",
    "from plot import get_figsize, set_figsize\n",
    "from plot import plot_df_trisurf\n",
    "\n",
    "default_w, default_h = get_figsize()\n",
    "\n",
    "# Experiment imports.\n",
    "from collections import OrderedDict\n",
    "from gridsearch import experiment, load_experiment\n",
    "\n",
    "# Dataset.\n",
    "import dataset as ds\n",
    "\n",
    "u_train, y_train = ds.NARMA(sample_len = 2000)\n",
    "u_test, y_test = ds.NARMA(sample_len = 3000)\n",
    "dataset = [u_train, y_train, u_test, y_test]\n",
    "ds.dataset = dataset\n",
    "\n",
    "# Distance functions.\n",
    "from matrix import euclidean\n",
    "\n",
    "euc = euclidean\n",
    "def inv(x, y): return 1/euclidean(x, y)\n",
    "def inv_squared(x, y): return 1/euclidean(x, y)**2\n",
    "def inv_cubed(x, y): return 1/euclidean(x, y)**3\n",
    "\n",
    "# Oftentimes for debugging purposes.\n",
    "from ESN import ESN, Distribution\n",
    "from metric import evaluate_esn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Experiments: Random Geometric Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Echo State Networks with nodes in metric space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Undirected geometric graphs with nodes randomly sampled uniformly in the  \n",
    "underlying space [0, 1)^d.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from notebook import rgg_dist_performance\n",
    "rgg_dist_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_rgg_dist_performance\n",
    "plot_rgg_dist_performance(agg='mean')\n",
    "plot_rgg_dist_performance(agg='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "(TODO): add the default echo state network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The inv^2 distance function seems to work the best. There is a diminishing  \n",
    "return from a squared to a cubed distance function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Add some stuff about distribution of weights, and original spectral radius of  \n",
    "the matrix requiring huge scalings.  \n",
    "\n",
    "Perhaps also memory capacity and/or QGU.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Experiments: Regular Tilings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Performance of standard regular tilings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Regular tilings/Bravais lattices. Lattices that were mostly used are the square,  \n",
    "hexagonal and triangular lattices/tilings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_regular_tilings\n",
    "plot_regular_tilings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Default performance of such lattices, with a standard uniform input distribution  \n",
    "in the interval [-0.5, 0.5], i.e. mostly the same as echo state networks.  \n",
    "\n",
    "Note that these networks are all *undirected*, and have *no negative weights*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from notebook import regular_tilings_performance\n",
    "regular_tilings_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_regular_tilings_performance\n",
    "plot_regular_tilings_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The performances shown are surprisingly good, if we consider the cutoff for  \n",
    "being \"unable to predict the time series\" at an NRMSE of 1.0.  \n",
    "\n",
    "An example for the predicted output of a square lattice vs. the expected output  \n",
    "is shown below for the NARMA 10 dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "esn = ESN(hidden_nodes=81, w_res_type='tetragonal')\n",
    "evaluate_esn(ds.dataset, esn, plot=True, plot_range=[0, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Regular tilings with inhibitory connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Hardly interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Regular tilings with directed connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What happens if we make a fraction of the edges of the lattice directed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "esn = ESN(hidden_nodes=25, w_res_type='tetragonal', dir_frac=0.5)\n",
    "plot_lattice(esn.G.reverse(), color_directed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Performance-wise, we should, according to previous work, expect at least some  \n",
    "improvement.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_directed_regular_tilings_performance\n",
    "plot_directed_regular_tilings_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It would seem that, the performance of the lattices match that of the standard  \n",
    "echo state network for the NARMA 10 task. What about memory capacity and QGU?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Physical perspective: global input scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The input scheme of the previous experiment was still achieved by scaling the  \n",
    "input of each hidden node with a value in the interval [-0.5, 0.5], as in all  \n",
    "previous experiments. Is there some simpler scheme that works?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import global_input_scheme_performance\n",
    "global_input_scheme_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Every input weight of every node is the same, but has been scaled as to fit the  \n",
    "memory requirements of the task better.  \n",
    "\n",
    "Additionally, every reservoir weight (i.e. the spacing between nodes in the  \n",
    "lattice) is uniquely determined. This has been scaled to a specific reservoir  \n",
    "weight that fits, but as the spacing is fixed in all cases, this amounts to a  \n",
    "simple scalar scaling in all cases.  \n",
    "\n",
    "Thus, what remains in terms of parameterization for the reservoir? One single  \n",
    "parameter: determining which direction each edge should point. With a completely  \n",
    "random scheme, the mean reservoir performs equally as well to that of the echo  \n",
    "state network.  \n",
    "\n",
    "Note that this is a quite similar approach to that of the minimal complexity  \n",
    "echo state network, i.e. cyclic reservoirs with regular jumps. In CRJs, all  \n",
    "reservoir weights are fixed to the same, predetermined value. However, with  \n",
    "CRJs, the input scheme fixes all input values to 1, but includes a random  \n",
    "distribution scheme of the signedness of these inputs, thus not employing a  \n",
    "single global input.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What about the activations of a lattice compared to that of standard echo state  \n",
    "networks?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_global_input_activations\n",
    "plot_global_input_activations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Reservoir robustness: removing nodes gradually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Usually one thinks of robustness in terms of two different scenarios: dead nodes  \n",
    "and noisy nodes. Here we look at the impact of dead nodes, i.e. what happens  \n",
    "when single nodes disappear completely from the network.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "name": "thesis.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
