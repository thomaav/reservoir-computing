{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Imports.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Important for multiprocessing.\n",
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "# General plotting things.\n",
    "from plot import get_3d_subplot_axs\n",
    "from plot import get_figsize, set_figsize\n",
    "from plot import plot_df_trisurf\n",
    "\n",
    "default_w, default_h = get_figsize()\n",
    "\n",
    "# Experiment imports.\n",
    "from collections import OrderedDict\n",
    "from gridsearch import experiment, load_experiment\n",
    "\n",
    "# Dataset.\n",
    "import dataset as ds\n",
    "\n",
    "u_train, y_train = ds.NARMA(sample_len = 2000)\n",
    "u_test, y_test = ds.NARMA(sample_len = 3000)\n",
    "dataset = [u_train, y_train, u_test, y_test]\n",
    "ds.dataset = dataset\n",
    "\n",
    "# Distance functions.\n",
    "from matrix import euclidean\n",
    "\n",
    "euc = euclidean\n",
    "def inv(x, y): return 1/euclidean(x, y)\n",
    "def inv_squared(x, y): return 1/euclidean(x, y)**2\n",
    "def inv_cubed(x, y): return 1/euclidean(x, y)**3\n",
    "\n",
    "# Oftentimes for debugging purposes.\n",
    "from ESN import ESN, Distribution\n",
    "from metric import evaluate_esn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Experiments: Random Geometric Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Echo State Networks with nodes in metric space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Undirected geometric graphs with nodes randomly sampled uniformly in the  \n",
    "underlying space [0, 1)^d.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from notebook import rgg_dist_performance\n",
    "rgg_dist_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_rgg_dist_performance\n",
    "plot_rgg_dist_performance(agg='mean')\n",
    "plot_rgg_dist_performance(agg='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "(TODO): add the default echo state network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The inv^2 distance function seems to work the best. There is a diminishing  \n",
    "return from a squared to a cubed distance function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Add some stuff about distribution of weights, and original spectral radius of  \n",
    "the matrix requiring huge scalings.  \n",
    "\n",
    "Perhaps also memory capacity and/or QGU.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Experiments: Regular Tilings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Performance of standard regular tilings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Regular tilings/Bravais lattices. Lattices that were mostly used are the square,  \n",
    "hexagonal and triangular lattices/tilings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_regular_tilings\n",
    "plot_regular_tilings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Default performance of such lattices, with a standard uniform input distribution  \n",
    "in the interval [-0.5, 0.5], i.e. mostly the same as echo state networks.  \n",
    "\n",
    "Note that these networks are all *undirected*, and have *no negative weights*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from notebook import regular_tilings_performance\n",
    "regular_tilings_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_regular_tilings_performance\n",
    "plot_regular_tilings_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The performances shown are surprisingly good, if we consider the cutoff for  \n",
    "being \"unable to predict the time series\" at an NRMSE of 1.0.  \n",
    "\n",
    "An example for the predicted output of a square lattice vs. the expected output  \n",
    "is shown below for the NARMA 10 dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "esn = ESN(hidden_nodes=81, w_res_type='tetragonal')\n",
    "evaluate_esn(ds.dataset, esn, plot=True, plot_range=[0, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Regular tilings with inhibitory connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Hardly interesting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Regular tilings with directed connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What happens if we make a fraction of the edges of the lattice directed?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "esn = ESN(hidden_nodes=25, w_res_type='tetragonal', dir_frac=0.5)\n",
    "plot_lattice(esn.G.reverse(), color_directed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Performance-wise, we should, according to previous work, expect at least some  \n",
    "improvement.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_directed_regular_tilings_performance\n",
    "plot_directed_regular_tilings_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It would seem that, the performance of the lattices match that of the standard  \n",
    "echo state network for the NARMA 10 task. What about memory capacity and QGU?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Physical perspective: global input scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The input scheme of the previous experiment was still achieved by scaling the  \n",
    "input of each hidden node with a value in the interval [-0.5, 0.5], as in all  \n",
    "previous experiments. Is there some simpler scheme that works?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import global_input_scheme_performance\n",
    "global_input_scheme_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Every input weight of every node is the same, but has been scaled as to fit the  \n",
    "memory requirements of the task better.  \n",
    "\n",
    "Additionally, every reservoir weight (i.e. the spacing between nodes in the  \n",
    "lattice) is uniquely determined. This has been scaled to a specific reservoir  \n",
    "weight that fits, but as the spacing is fixed in all cases, this amounts to a  \n",
    "simple scalar scaling in all cases.  \n",
    "\n",
    "Thus, what remains in terms of parameterization for the reservoir? One single  \n",
    "parameter: determining which direction each edge should point. With a completely  \n",
    "random scheme, the mean reservoir performs equally as well to that of the echo  \n",
    "state network.  \n",
    "\n",
    "Note that this is a quite similar approach to that of the minimal complexity  \n",
    "echo state network, i.e. cyclic reservoirs with regular jumps. In CRJs, all  \n",
    "reservoir weights are fixed to the same, predetermined value. However, with  \n",
    "CRJs, the input scheme fixes all input values to 1, but includes a random  \n",
    "distribution scheme of the signedness of these inputs, thus not employing a  \n",
    "single global input.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What about the activations of a lattice compared to that of standard echo state  \n",
    "networks?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_global_input_activations\n",
    "plot_global_input_activations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can clearly see the input sequence in the activations of the nodes of the  \n",
    "lattice network, as all nodes see exactly the same input.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Reservoir robustness: removing nodes gradually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Usually one thinks of robustness in terms of two different scenarios: dead nodes,  \n",
    "and noisy nodes. Here we look at the impact of dead nodes, i.e. what happens  \n",
    "when single nodes disappear completely from the network.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First: we will remove nodes to perform well specifically for the NARMA 10  \n",
    "benchmark. The chosen nodes may (and should) prove different if we optimize for  \n",
    "e.g. memory capacity or kernel quality.  \n",
    "\n",
    "Thus, which nodes cause the biggest performance hits from removal?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import node_removal_impact\n",
    "node_removal_impact()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We see that the majority of the nodes cause a very small difference in  \n",
    "NRMSE. **I also suspect that the node with a big value is due to a bug I had  \n",
    "with the Torch implementation of SVD (pinverse), but I have yet to re-run the  \n",
    "experiment with the sklearn implementation of the same algorithm, which causes  \n",
    "no such instabilities.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Since the network proves quite resilient to removal of nodes, let's remove them  \n",
    "all. One by one, and greedily.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import remove_nodes_performance\n",
    "remove_nodes_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This is, of course, an unfair comparison. As we are comparing a specific  \n",
    "instance of a lattice that we are incrementally removing single nodes to cause  \n",
    "as little damage as possible.  \n",
    "\n",
    "However, we still see that the lattice is robust, one may remove quite a lot of  \n",
    "nodes before the performance collapses completely. Additionally, removing the  \n",
    "worst nodes initially will actually improve the network for the specific task.  \n",
    "\n",
    "To make a more fair comparison, I also did the same with the standard echo state  \n",
    "network, as to determine whether the method is only applicable to the lattice  \n",
    "(which it was not, it turns out to be a reasonable approach to the ESN as well).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import remove_esn_nodes_performance\n",
    "remove_esn_nodes_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, how do these lattices look?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_node_removal\n",
    "plot_node_removal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "How does the ESN look?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_esn_node_removal\n",
    "plot_esn_node_removal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Key takeaways:\n",
    "\n",
    "- It is quite difficult to see what has happened with the ESN, even if the  \n",
    "  density of the reservoir matrix is originally set to only 10%.  \n",
    "- OTOH, it seems to be quite possible to look at the progress of node removals  \n",
    "  in the lattice, and be able to get a clearer picture of what is happening.  \n",
    "- There seems to develop a \"main\" pathway in the reservoir, likely for memory  \n",
    "  purposes, that is \"augmented\" from outside connections.  \n",
    "- Quite interestingly, when the NRMSE drops to 0.4, which is the theoretical  \n",
    "  value achieved by a shift register, the network has seemingly turn into just  \n",
    "  that.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This also works similarly with periodic lattices, but they looked to be  \n",
    "*slightly*, perhaps simply due to the fact that are more randomly directed edges  \n",
    "that may turn out valuable.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Growing reservoirs: adding nodes incrementally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Interestingly, there is only a handful amount of ways to add new nodes to the  \n",
    "network: only along the frontier of the lattice, and for each such node, there  \n",
    "is only a set amount of ways to direct edges.  \n",
    "\n",
    "We first create the default 144 node lattice, remove nodes to its minimal value,  \n",
    "and then grow to a set size.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_growth\n",
    "plot_growth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Increasing the size seems to eventually run into some diminishing return, which  \n",
    "seems to gradually stop around 250 nodes or so from other experiments.  \n",
    "\n",
    "It could be interesting to hunt heuristics here: *could we add nodes in a less  \n",
    "greedy manner, yet deterministic by some heuristic?*. I have not looked too far  \n",
    "into this, as I've not found it too easy to look at the graph growth and see  \n",
    "what, concretely, is happening.  \n",
    "\n",
    "A viable approach could be to start with a shift register and/or appropriate  \n",
    "loops, and from there grow a suitable lattice.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Gradually making edges undirected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_making_edges_undirected_performance\n",
    "plot_making_edges_undirected_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We see something similar to the previous experiments. Some edges actually  \n",
    "improve performance with bidirection. Generally, though, it seems as if the  \n",
    "edges that are made bidirectional are edges that perhaps \"did not matter much\"  \n",
    "originally, while the more important, directed, are still intact. Once these  \n",
    "important edges must disappear, so must the performance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_making_edges_undirected\n",
    "plot_making_edges_undirected()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Example of great performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "An example of the performance achieved with the best network from growing until  \n",
    "some set amount of nodes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_good_performance\n",
    "plot_good_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Short-term memory, kernel quality, Mackey-Glass and other metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Perhaps less relevant. A bunch of general metrics and other tests, mostly  \n",
    "contained within the NARMA 10 benchmark.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "name": "thesis.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
