{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Imports.\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Important for multiprocessing.\n",
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "# General plotting things.\n",
    "from plot import get_3d_subplot_axs\n",
    "from plot import get_figsize, set_figsize\n",
    "from plot import plot_df_trisurf\n",
    "from plot import plot_lattice\n",
    "\n",
    "default_w, default_h = get_figsize()\n",
    "\n",
    "# Experiment imports.\n",
    "from collections import OrderedDict\n",
    "from gridsearch import experiment, load_experiment\n",
    "\n",
    "# Dataset.\n",
    "import dataset as ds\n",
    "\n",
    "u_train, y_train = ds.NARMA(sample_len = 2000)\n",
    "u_test, y_test = ds.NARMA(sample_len = 3000)\n",
    "dataset = [u_train, y_train, u_test, y_test]\n",
    "ds.dataset = dataset\n",
    "\n",
    "# Distance functions.\n",
    "from matrix import euclidean\n",
    "\n",
    "euc = euclidean\n",
    "def inv(x, y): return 1/euclidean(x, y)\n",
    "def inv_squared(x, y): return 1/euclidean(x, y)**2\n",
    "def inv_cubed(x, y): return 1/euclidean(x, y)**3\n",
    "\n",
    "# Oftentimes for debugging purposes.\n",
    "from ESN import ESN, Distribution\n",
    "from metric import evaluate_esn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## The NARMA10 time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from notebook import plot_NARMA10\n",
    "plot_NARMA10(range=[50, 200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## NARMA nonlinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_NARMA_nonlinearity\n",
    "plot_NARMA_nonlinearity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Experiments: Random Geometric Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Example RGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_rgg_example\n",
    "plot_rgg_example(save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Size of the Graph Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from notebook import rgg_volume_size\n",
    "rgg_volume_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_rgg_volume_size\n",
    "plot_rgg_volume_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Echo State Networks with nodes in metric space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Undirected geometric graphs with nodes randomly sampled uniformly in the  \n",
    "underlying space [0, 1)^d.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from notebook import rgg_dist_performance\n",
    "rgg_dist_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_rgg_dist_performance\n",
    "plot_rgg_dist_performance(agg='mean', file_name='RGG-dist-performance-mean.png')\n",
    "plot_rgg_dist_performance(agg='min', file_name='RGG-dist-performance-min.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from notebook import rgg_dist_mc\n",
    "rgg_dist_mc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_rgg_dist_mc\n",
    "plot_rgg_dist_mc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from notebook import rgg_dist_performance_is\n",
    "rgg_dist_performance_is()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_rgg_dist_performance_is\n",
    "plot_rgg_dist_performance_is()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from notebook import performance_restoration\n",
    "performance_restoration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_performance_restoration\n",
    "plot_performance_restoration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_performance_restoration_comparison\n",
    "plot_performance_restoration_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_new_weight_distribution\n",
    "plot_new_weight_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Experiments: Regular Tilings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Lattice networks/models are common in computational physics, condensed matter  \n",
    "physics and beyond, modelling physical interactions, phase transitions and  \n",
    "structure [15]. Examples include: discrete lattices like the Ising model with  \n",
    "variables representing magnetic dipole moments of atomic spins, and the Gray-  \n",
    "Scott reaction-diffusion model to simulate chemical systems [22]. Also, physical  \n",
    "substrates often have a regular grid of connections. Lattice networks are  \n",
    "therefore more realistic representations of many physical systems that would be  \n",
    "considered for reservoir computing.  \n",
    "\n",
    "From «Role of Structure and Complexity..».  \n",
    "\n",
    "I think this is a good point, but we can take this even further. A lattice will  \n",
    "necessarily be a subset of what the ESN can produce: but the big question is  \n",
    "whether is is *sufficient*. Yes, of course the CHARC model will show that the  \n",
    "ESN will reach a bigger area, that should be obvious.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Performance of standard regular tilings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Regular tilings/Bravais lattices. Lattices that were mostly used are the square,  \n",
    "hexagonal and triangular lattices/tilings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_regular_tilings\n",
    "plot_regular_tilings(save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Default performance of such lattices, with a standard uniform input distribution  \n",
    "in the interval [-0.5, 0.5], i.e. mostly the same as echo state networks.  \n",
    "\n",
    "Note that these networks are all *undirected*, and have *no negative weights*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from notebook import regular_tilings_performance\n",
    "regular_tilings_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_regular_tilings_performance\n",
    "plot_regular_tilings_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from notebook import regular_tilings_performance_is\n",
    "regular_tilings_performance_is()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_regular_tilings_performance_is\n",
    "plot_regular_tilings_performance_is()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The performances shown are surprisingly good, if we consider the cutoff for  \n",
    "being \"unable to predict the time series\" at an NRMSE of 1.0.  \n",
    "\n",
    "An example for the predicted output of a square lattice vs. the expected output  \n",
    "is shown below for the NARMA 10 dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "esn = ESN(hidden_nodes=81, w_res_type='tetragonal')\n",
    "evaluate_esn(ds.dataset, esn, plot=True, plot_range=[0, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Regular tilings with inhibitory connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Hardly interesting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Regular tilings with directed connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What happens if we make a fraction of the edges of the lattice directed?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_directed_lattice\n",
    "plot_directed_lattice()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Performance-wise, we should, according to previous work, expect at least some  \n",
    "improvement.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from notebook import directed_regular_tilings_performance\n",
    "directed_regular_tilings_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_directed_regular_tilings_performance\n",
    "plot_directed_regular_tilings_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It would seem that, the performance of the lattices match that of the standard  \n",
    "echo state network for the NARMA 10 task. What about memory capacity and QGU?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Physical perspective: global input scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The input scheme of the previous experiment was still achieved by scaling the  \n",
    "input of each hidden node with a value in the interval [-0.5, 0.5], as in all  \n",
    "previous experiments. Is there some simpler scheme that works?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import global_input_scheme_performance\n",
    "global_input_scheme_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Every input weight of every node is the same, but has been scaled as to fit the  \n",
    "memory requirements of the task better.  \n",
    "\n",
    "Additionally, every reservoir weight (i.e. the spacing between nodes in the  \n",
    "lattice) is uniquely determined. This has been scaled to a specific reservoir  \n",
    "weight that fits, but as the spacing is fixed in all cases, this amounts to a  \n",
    "simple scalar scaling in all cases.  \n",
    "\n",
    "Thus, what remains in terms of parameterization for the reservoir? One single  \n",
    "parameter: determining which direction each edge should point. With a completely  \n",
    "random scheme, the mean reservoir performs equally as well to that of the echo  \n",
    "state network.  \n",
    "\n",
    "Note that this is a quite similar approach to that of the minimal complexity  \n",
    "echo state network, i.e. cyclic reservoirs with regular jumps. In CRJs, all  \n",
    "reservoir weights are fixed to the same, predetermined value. However, with  \n",
    "CRJs, the input scheme fixes all input values to 1, but includes a random  \n",
    "distribution scheme of the signedness of these inputs, thus not employing a  \n",
    "single global input.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What about the activations of a lattice compared to that of standard echo state  \n",
    "networks?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_global_input_activations\n",
    "plot_global_input_activations(save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can clearly see the input sequence in the activations of the nodes of the  \n",
    "lattice network, as all nodes see exactly the same input.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Performance comparison with ESN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from notebook import directed_lattice_performance\n",
    "directed_lattice_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_directed_lattice_performance\n",
    "plot_directed_lattice_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from notebook import unique_weights\n",
    "unique_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import print_unique_weights\n",
    "print_unique_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Reservoir robustness: removing nodes gradually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Usually one thinks of robustness in terms of two different scenarios: dead nodes,  \n",
    "and noisy nodes. Here we look at the impact of dead nodes, i.e. what happens  \n",
    "when single nodes disappear completely from the network.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First: we will remove nodes to perform well specifically for the NARMA 10  \n",
    "benchmark. The chosen nodes may (and should) prove different if we optimize for  \n",
    "e.g. memory capacity or kernel quality.  \n",
    "\n",
    "Thus, which nodes cause the biggest performance hits from removal?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from notebook import node_removal_impact\n",
    "node_removal_impact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_node_removal_impact\n",
    "plot_node_removal_impact()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We see that the majority of the nodes cause a very small difference in  \n",
    "NRMSE. **I also suspect that the node with a big value is due to a bug I had  \n",
    "with the Torch implementation of SVD (pinverse), but I have yet to re-run the  \n",
    "experiment with the sklearn implementation of the same algorithm, which causes  \n",
    "no such instabilities.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Since the network proves quite resilient to removal of nodes, let's remove them  \n",
    "all. One by one, and greedily.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import remove_nodes_performance\n",
    "remove_nodes_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The sudden drop in performance at some size is due to the memory degrading such\n",
    "that the third term in the NARMA, which is quite high weighted, is not solvable\n",
    "anymore.\n",
    "\n",
    "This is, of course, an unfair comparison. As we are comparing a specific  \n",
    "instance of a lattice that we are incrementally removing single nodes to cause  \n",
    "as little damage as possible.  \n",
    "\n",
    "However, we still see that the lattice is robust, one may remove quite a lot of  \n",
    "nodes before the performance collapses completely. Additionally, removing the  \n",
    "worst nodes initially will actually improve the network for the specific task.  \n",
    "\n",
    "To make a more fair comparison, I also did the same with the standard echo state  \n",
    "network, as to determine whether the method is only applicable to the lattice  \n",
    "(which it was not, it turns out to be a reasonable approach to the ESN as well).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from notebook import general_esn_shrink\n",
    "general_esn_shrink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import remove_esn_nodes_performance\n",
    "remove_esn_nodes_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, how do these lattices look?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_node_removal\n",
    "plot_node_removal(save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "How does the ESN look?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_esn_node_removal\n",
    "plot_esn_node_removal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Key takeaways:\n",
    "\n",
    "- It is quite difficult to see what has happened with the ESN, even if the  \n",
    "  density of the reservoir matrix is originally set to only 10%.  \n",
    "- OTOH, it seems to be quite possible to look at the progress of node removals  \n",
    "  in the lattice, and be able to get a clearer picture of what is happening.  \n",
    "- There seems to develop a \"main\" pathway in the reservoir, likely for memory  \n",
    "  purposes, that is \"augmented\" from outside connections.  \n",
    "- Quite interestingly, when the NRMSE drops to 0.4, which is the theoretical  \n",
    "  value achieved by a shift register, the network has seemingly turn into just  \n",
    "  that.  \n",
    "\n",
    "Thought: looking at inubushi_reservoir_2017, is it perhaps reasonable to believe\n",
    "that we can uncover which nodes are very important for memory here, and make\n",
    "such nodes linear instead? Thus beyond the trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This also works similarly with periodic lattices, but they looked to be  \n",
    "*slightly*, perhaps simply due to the fact that are more randomly directed edges  \n",
    "that may turn out valuable.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Growing reservoirs: adding nodes incrementally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Interestingly, there is only a handful amount of ways to add new nodes to the  \n",
    "network: only along the frontier of the lattice, and for each such node, there  \n",
    "is only a set amount of ways to direct edges.  \n",
    "\n",
    "We first create the default 144 node lattice, remove nodes to its minimal value,  \n",
    "and then grow to a set size.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_growth\n",
    "plot_growth(save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_growth_performance\n",
    "plot_growth_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Increasing the size seems to eventually run into some diminishing return, which  \n",
    "seems to gradually stop around 250 nodes or so from other experiments.  \n",
    "\n",
    "It could be interesting to hunt heuristics here: *could we add nodes in a less  \n",
    "greedy manner, yet deterministic by some heuristic?*. I have not looked too far  \n",
    "into this, as I've not found it too easy to look at the graph growth and see  \n",
    "what, concretely, is happening.  \n",
    "\n",
    "A viable approach could be to start with a shift register and/or appropriate  \n",
    "loops, and from there grow a suitable lattice.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Gradually making edges undirected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from notebook import making_edges_undirected_performance\n",
    "making_edges_undirected_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_making_edges_undirected_performance\n",
    "plot_making_edges_undirected_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We see something similar to the previous experiments. Some edges actually  \n",
    "improve performance with bidirection. Generally, though, it seems as if the  \n",
    "edges that are made bidirectional are edges that perhaps \"did not matter much\"  \n",
    "originally, while the more important, directed, are still intact. Once these  \n",
    "important edges must disappear, so must the performance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_making_edges_undirected\n",
    "plot_making_edges_undirected()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Example of great performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "An example of the performance achieved with the best network from growing until  \n",
    "some set amount of nodes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_good_performance\n",
    "plot_good_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Short-term memory, kernel quality, Mackey-Glass and other metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Perhaps less relevant. A bunch of general metrics and other tests, mostly  \n",
    "contained within the NARMA 10 benchmark.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Harder benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from notebook import harder_benchmarks_performance\n",
    "harder_benchmarks_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_harder_benchmarks_performance\n",
    "plot_harder_benchmarks_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The lower points is *about* the same performance as that of an untuned ESN of  \n",
    "the same size. What we are seeing, is likely to be that we are hitting the  \n",
    "required memory capacity for the task, and that lower input scaling do not  \n",
    "improve the memory much further, thus capping the performance for the network.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A further example for the square lattice could be to start with a shift register  \n",
    "with the required memory capacity, and then grow from there.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from notebook import grow_shift_register_narma30\n",
    "grow_shift_register_narma30()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook import plot_shift_register_lattices\n",
    "plot_shift_register_lattices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Perhaps, in short fashion, mention the fact that we are turning a dot product  \n",
    "and a matrix multiplication into a single multiplication and a scalar  \n",
    "multiplication + row sum for the input and reservoir matrices, respectively.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Experimens: Towards Artificial Spin Ice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ESN import Distribution\n",
    "\n",
    "params = OrderedDict()\n",
    "params['w_res_type'] = 'tetragonal'\n",
    "params['hidden_nodes'] = 20*20\n",
    "params['readout'] = 'rr'\n",
    "params['input_scaling'] = 1/10\n",
    "params['w_in_distrib'] = Distribution.fixed\n",
    "params['dir_frac'] = 1.0\n",
    "params['w_in_density'] = 0.5\n",
    "esn = ESN(**params)\n",
    "print(evaluate_esn(ds.dataset, esn))\n",
    "\n",
    "params = OrderedDict()\n",
    "params['hidden_nodes'] = 20*20\n",
    "params['w_res_density'] = 0.5\n",
    "esn = ESN(**params)\n",
    "print(evaluate_esn(ds.dataset, esn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from ESN import Distribution\n",
    "\n",
    "params = OrderedDict()\n",
    "params['w_res_type'] = 'tetragonal'\n",
    "params['hidden_nodes'] = 20*20\n",
    "params['readout'] = 'rr'\n",
    "params['input_scaling'] = 1/10\n",
    "params['w_in_distrib'] = Distribution.fixed\n",
    "params['dir_frac'] = 1.0\n",
    "params['w_in_density'] = 0.5\n",
    "esn = ESN(**params)\n",
    "\n",
    "from experiment import remove_nodes_incrementally\n",
    "remove_nodes_incrementally(ds.dataset, esn, 'experiments/debug_test_debug.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Debug w_in_density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from ESN import ESN, Distribution\n",
    "from metric import esn_nrmse\n",
    "\n",
    "params = OrderedDict()\n",
    "params['w_res_type'] = ['tetragonal']\n",
    "params['hidden_nodes'] = [12*12]\n",
    "params['readout'] = ['rr']\n",
    "params['input_scaling'] = np.arange(0.05, 1.05, 0.05)\n",
    "params['w_in_distrib'] = [Distribution.fixed]\n",
    "params['dir_frac'] = [1.0]\n",
    "params['w_in_density'] = np.arange(0.05, 1.05, 0.05)\n",
    "\n",
    "df = experiment(esn_nrmse, params, runs=10)\n",
    "df.to_pickle('experiments/lattice_input_density.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = load_experiment('experiments/lattice_input_density.pkl')\n",
    "\n",
    "groupby = ['input_scaling', 'w_in_density']\n",
    "axes    = ['input_scaling', 'w_in_density', 'esn_nrmse']\n",
    "agg     = ['mean', 'min']\n",
    "zlim    = (0.25, 0.7)\n",
    "azim    = 140\n",
    "titles  = ['tetragonal', 'hexagonal', 'triangular']\n",
    "\n",
    "plot_df_trisurf(df=df, groupby=groupby, axes=axes, azim=azim, agg=agg,\n",
    "                zlim=zlim, title='My title')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "name": "thesis.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
