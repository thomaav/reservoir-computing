{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Standard imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Important for multiprocessing.\n",
    "import torch\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plot import get_3d_subplot_axs\n",
    "from plot import get_figsize, set_figsize\n",
    "\n",
    "default_w, default_h = get_figsize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Lattice imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from gridsearch import experiment, load_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Create NARMA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dataset as ds\n",
    "\n",
    "u_train, y_train = ds.NARMA(sample_len = 2000)\n",
    "u_test, y_test = ds.NARMA(sample_len = 3000)\n",
    "dataset = [u_train, y_train, u_test, y_test]\n",
    "ds.dataset = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Waxman graph generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matrix import waxman\n",
    "from plot import scatter_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matrix import euclidean\n",
    "\n",
    "# We need names for the distance functions to select them from a pandas\n",
    "# dataframe later.\n",
    "euc = euclidean\n",
    "def inv(x, y): return 1/euclidean(x, y)\n",
    "def inv_squared(x, y): return 1/euclidean(x, y)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Waxman visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for z_frac in [0.0, 0.5, 1.0]:\n",
    "    G = waxman(n=200, alpha=1.0, beta=1.0, z_frac=z_frac)\n",
    "    scatter_3d(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Waxman performance with increasing fraction of 3d nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from metric import esn_nrmse\n",
    "\n",
    "params = OrderedDict()\n",
    "params['w_res_type'] = ['waxman']\n",
    "params['dist_function'] = [inv]\n",
    "params['hidden_nodes'] = np.arange(20, 90, 10)\n",
    "params['z_frac'] = np.arange(0.0, 1.2, 0.2)\n",
    "params['directed'] = [True, False]\n",
    "waxman_nrmse_df = experiment(esn_nrmse, params, runs=20)\n",
    "waxman_nrmse_df.to_pickle('experiments/waxman_nrmse.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plot import plot_df_trisurf\n",
    "\n",
    "if 'waxman_nrmse_df' not in locals():\n",
    "    waxman_nrmse_df = load_experiment('experiments/waxman_nrmse.pkl')\n",
    "\n",
    "df = waxman_nrmse_df.loc[waxman_nrmse_df['directed'] == False]\n",
    "groupby = ['hidden_nodes', 'z_frac']\n",
    "\n",
    "axes    = ['hidden_nodes', 'z_frac', 'esn_nrmse']\n",
    "agg     = ['mean', 'min']\n",
    "zlim    = [(0.40, 0.80), (0.40, 0.80)]\n",
    "azim    = [160, 160]\n",
    "title   = 'Increasing fraction of 3d nodes'\n",
    "\n",
    "plot_df_trisurf(df=df, groupby=groupby, axes=axes, agg=agg, azim=azim, zlim=zlim, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A hypothesis as to what we're seeing from the *min* plot is that there is little  \n",
    "noticeable difference with increasing *z_frac*. Additionally, we do not see the  \n",
    "expected performance increase when adding more hidden nodes to the reservoir.  \n",
    "\n",
    "An initial for why more nodes does not matter is that there is a cap on what the  \n",
    "reservoir may be capable of learning with only positive weights, especially when  \n",
    "the network is fully connected. In fact, more nodes makes it more likely for the  \n",
    "network to be \"unstable\", explaining the higher performance variance seen in the  \n",
    "leftmost plot. Since all weights are positive: does the scaling for spectral  \n",
    "radius purposes have to be tighter?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Waxman distance functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Distribution of reservoir weights with different distance functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The first thing we want to address is how the distribution of weights differ  \n",
    "with different distance functions for the generated Waxman graph.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ESN import ESN\n",
    "from plot import plot_esn_weight_hist\n",
    "\n",
    "bins = 40\n",
    "params = {\n",
    "    'hidden_nodes': 150,\n",
    "}\n",
    "\n",
    "plt.title('Reference ESN')\n",
    "plot_esn_weight_hist(params, n_bins=bins, show=True)\n",
    "\n",
    "set_figsize(14, 6)\n",
    "\n",
    "params = {\n",
    "    'directed': False,\n",
    "    'w_res_type': 'waxman',\n",
    "}\n",
    "\n",
    "hidden_nodes = [50, 150, 150, 150]\n",
    "dist_functions = [euc, euc, inv, inv_squared]\n",
    "titles = ['50 nodes euc', '150 nodes euc', '150 nodes inv', '150 nodes inv^2']\n",
    "\n",
    "for z_frac in [0.0, 1.0]:\n",
    "    fig, axs = plt.subplots(1, 4)\n",
    "\n",
    "    params['z_frac'] = z_frac\n",
    "    plt.suptitle('z_frac:' + str(z_frac))\n",
    "\n",
    "    for i in range(len(axs)):\n",
    "        params['hidden_nodes'] = hidden_nodes[i]\n",
    "        params['dist_function'] = dist_functions[i]\n",
    "\n",
    "        ax = axs[i]\n",
    "        ax.set_title(titles[i])\n",
    "        plot_esn_weight_hist(params, n_bins=bins, ax=ax, show=False)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "set_figsize(default_w, default_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We see the clear difference between the functions *d*, *1/d* and *1/d^*; we can  \n",
    "almost see the functions in the distributions themselves.  \n",
    "\n",
    "When the weight function moves from d to 1/d to 1/d, we see that we are shifting  \n",
    "the probability distribution towards many lower weights, with just a few big  \n",
    "ones, which is as expected.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Waxman performance with changing weight/distance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from metric import esn_nrmse\n",
    "\n",
    "params = OrderedDict()\n",
    "params['w_res_type'] = ['waxman']\n",
    "params['z_frac'] = np.arange(0.0, 1.2, 0.2)\n",
    "params['hidden_nodes'] = np.arange(20, 90, 10)\n",
    "params['dist_function'] = [euc, inv, inv_squared]\n",
    "waxman_dist_df = experiment(esn_nrmse, params, runs=20)\n",
    "waxman_dist_df.to_pickle('experiments/waxman_dist.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from plot import plot_df_trisurf\n",
    "\n",
    "if 'waxman_dist_df' not in locals():\n",
    "    waxman_dist_df = load_experiment('experiments/waxman_dist.pkl')\n",
    "\n",
    "waxman_dist_df['dist_function'] = waxman_dist_df['dist_function'].apply(\n",
    "    lambda f: f.__name__ if not isinstance(f, str) else f\n",
    ")\n",
    "\n",
    "euc_df = waxman_dist_df.loc[waxman_dist_df['dist_function'] == euc.__name__]\n",
    "inv_df = waxman_dist_df.loc[waxman_dist_df['dist_function'] == inv.__name__]\n",
    "inv_squared_df = waxman_dist_df.loc[waxman_dist_df['dist_function'] == inv_squared.__name__]\n",
    "\n",
    "groupby = ['hidden_nodes', 'z_frac']\n",
    "axes    = ['hidden_nodes', 'z_frac', 'esn_nrmse']\n",
    "agg     = ['mean', 'min']\n",
    "titles  = ['euc', 'inv', 'inv^2']\n",
    "labels  = ['euc', 'inv', 'inv^2']\n",
    "zlim    = (0.3, 1.0)\n",
    "azim    = 110\n",
    "\n",
    "for i, df in enumerate([euc_df, inv_df, inv_squared_df]):\n",
    "    plot_df_trisurf(df=df, groupby=groupby, axes=axes, agg=agg, azim=azim, zlim=zlim, title=titles[i])\n",
    "\n",
    "ax = get_3d_subplot_axs(1)[0]\n",
    "show = [False, False, True]\n",
    "for i, df in enumerate([euc_df, inv_df, inv_squared_df]):\n",
    "    plot_df_trisurf(df=df, groupby=groupby, axes=axes, azim=azim, zlim=zlim,\n",
    "                    title='min for all dist functions', agg=['min'], ax=ax, label=labels[i],\n",
    "                    show=show[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It does seem like the different distance functions may all achieve about the  \n",
    "same performance in terms of error rate on NARMA10, giving a slight edge to the  \n",
    "inverse functions. However, the latter ones also introduce the instability we  \n",
    "see disappear when we use a minimum aggregation instead. The weights in general  \n",
    "are _smaller_, as can be seen in the weight distribution plots. First thoughts:  \n",
    "can this be remedied with input scalings? It is clear that _z\\_frac_ has little  \n",
    "impact on performance, so let's change it to input scaling.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Further experiments using inv distance function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Waxman plotting the original spectral radius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "An initial thought was that there is some sort of saturation in the  \n",
    "network. However, since the input distribution is uniform [-0.5, 0.5], I find  \n",
    "this hard to believe in hindsight.  \n",
    "\n",
    "Instead, the problem could be that the resulting *w_res* with the Waxman model  \n",
    "has an initial spectral radius that is high, resulting in a normalization that  \n",
    "is very harsh, leading to very low activations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from metric import esn_nrmse, evaluate_esn\n",
    "from ESN import ESN\n",
    "\n",
    "params = OrderedDict()\n",
    "params['w_res_type'] = 'waxman'\n",
    "params['z_frac'] = 1.0\n",
    "params['hidden_nodes'] = 50\n",
    "params['dist_function'] = inv\n",
    "\n",
    "nrmses = []\n",
    "srs = []\n",
    "for i in range(100):\n",
    "    esn = ESN(**params)\n",
    "    nrmse = evaluate_esn(ds.dataset, esn)\n",
    "    nrmses.append(nrmse)\n",
    "    srs.append(esn.org_spectral_radius)\n",
    "\n",
    "pickle.dump(nrmses, open('models/waxman_nrmses.pkl', 'wb'))\n",
    "pickle.dump(srs, open('models/waxman_srs.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nrmses = np.array(pickle.load(open('models/waxman_nrmses.pkl', 'rb')))\n",
    "srs = np.array(pickle.load(open('models/waxman_srs.pkl', 'rb')))\n",
    "\n",
    "np.clip(nrmses, 0, 1, out=nrmses)\n",
    "np.clip(srs, 0, 100, out=srs)\n",
    "\n",
    "plt.title('Original spectral radius vs. NRMSE after scaling (xmax&ymax clipped)')\n",
    "plt.xlabel('Original spectral radius')\n",
    "plt.ylabel('Evaluated NRMSE')\n",
    "\n",
    "plt.scatter(srs, nrmses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The spectral radius of the resulting matrices are huge, perhaps suggesting that  \n",
    "the Waxman model is unsuited.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Effect of input scaling in Waxman networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from metric import esn_nrmse\n",
    "\n",
    "params = OrderedDict()\n",
    "params['w_res_type'] = ['waxman']\n",
    "params['input_scaling'] = np.arange(0.1, 2.1, 0.1)\n",
    "params['hidden_nodes'] = np.arange(20, 90, 10)\n",
    "params['dist_function'] = [euc, inv, inv_squared]\n",
    "waxman_is_df = experiment(esn_nrmse, params, runs=20)\n",
    "waxman_is_df.to_pickle('experiments/waxman_is.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from metric import esn_mc\n",
    "\n",
    "params = OrderedDict()\n",
    "params['w_res_type'] = ['waxman']\n",
    "params['input_scaling'] = np.arange(0.1, 2.1, 0.1)\n",
    "params['hidden_nodes'] = [80]\n",
    "params['dist_function'] = [euc, inv, inv_squared]\n",
    "waxman_is_mc_df = experiment(esn_mc, params, runs=20)\n",
    "waxman_is_mc_df.to_pickle('experiments/waxman_is_mc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plot import plot_df_trisurf\n",
    "\n",
    "if 'waxman_is_df' not in locals():\n",
    "    waxman_is_df = load_experiment('experiments/waxman_is.pkl')\n",
    "\n",
    "waxman_is_df['dist_function'] = waxman_is_df['dist_function'].apply(\n",
    "    lambda f: f.__name__ if not isinstance(f, str) else f\n",
    ")\n",
    "\n",
    "euc_df = waxman_is_df.loc[waxman_is_df['dist_function'] == euc.__name__]\n",
    "inv_df = waxman_is_df.loc[waxman_is_df['dist_function'] == inv.__name__]\n",
    "inv_squared_df = waxman_is_df.loc[waxman_is_df['dist_function'] == inv_squared.__name__]\n",
    "\n",
    "groupby = ['hidden_nodes', 'input_scaling']\n",
    "axes    = ['hidden_nodes', 'input_scaling', 'esn_nrmse']\n",
    "agg     = ['min']\n",
    "titles  = ['euc', 'inv', 'inv^2']\n",
    "zlim    = (0.5, 0.7)\n",
    "azim    = 20\n",
    "\n",
    "for i, df in enumerate([euc_df, inv_df, inv_squared_df]):\n",
    "    plot_df_trisurf(df=df, groupby=groupby, axes=axes, agg=agg, azim=azim, zlim=zlim, title=titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plot import plot_df_trisurf\n",
    "\n",
    "if 'waxman_is_mc_df' not in locals():\n",
    "    waxman_is_mc_df = load_experiment('experiments/waxman_is_mc.pkl')\n",
    "\n",
    "waxman_is_mc_df['dist_function'] = waxman_is_mc_df['dist_function'].apply(\n",
    "    lambda f: f.__name__ if not isinstance(f, str) else f\n",
    ")\n",
    "\n",
    "euc_df = waxman_is_mc_df.loc[waxman_is_mc_df['dist_function'] == euc.__name__]\n",
    "inv_df = waxman_is_mc_df.loc[waxman_is_mc_df['dist_function'] == inv.__name__]\n",
    "inv_squared_df = waxman_is_mc_df.loc[waxman_is_mc_df['dist_function'] == inv_squared.__name__]\n",
    "\n",
    "labels = ['euc', 'inv', 'inv^2']\n",
    "plt.title('STM (MC) - 80 hidden nodes')\n",
    "plt.xlabel('Input scaling')\n",
    "plt.ylabel('STM (MC)')\n",
    "\n",
    "for i, df in enumerate([euc_df, inv_df, inv_squared_df]):\n",
    "    grouped_df = df.groupby(['input_scaling']).mean().reset_index()\n",
    "    plt.plot(grouped_df['input_scaling'], grouped_df['esn_mc'], label=labels[i])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Input scaling seems to have a quite big impact on the different distance  \n",
    "functions. In fact, 1/d^2 has a minimum NRMSE with correct input scaling of 0.5,  \n",
    "which is quite a lot lower than the original distance function d.  \n",
    "\n",
    "In the STM plot, we see that 1d^2 consistently has better memory performance  \n",
    "than the other distance functions, even before changing the input scaling. That  \n",
    "much. An analysis of the lyapunov spectrum could prove interesting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Can we see any patterns if we look the output weights of the network for tests  \n",
    "of increasing memory capacity?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from metric import memory_capacity\n",
    "from plot import scatter_3d\n",
    "\n",
    "params = {\n",
    "    'w_res_type': 'waxman',\n",
    "    'hidden_nodes': 80,\n",
    "    'dist_function': inv_squared,\n",
    "    'input_scaling': 0.1,\n",
    "}\n",
    "\n",
    "esn = ESN(**params)\n",
    "mc = memory_capacity(esn)\n",
    "print('Memory capacity:', mc)\n",
    "\n",
    "for i in range(3, 9):\n",
    "    weights = esn.w_outs[i].abs().data.numpy()\n",
    "    weights /= weights.max()\n",
    "    title = 'Hidden weights for STM length: ' + str(i)\n",
    "    scatter_3d(esn.G, title=title, cols=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "I see no concernable pattern, which is perhaps as expected from a fully  \n",
    "connected network.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The question remains: what is the difference that makes the inverted functions  \n",
    "increasingly performant on NARMA/memory? Are the internal node activations  \n",
    "relevant?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from metric import evaluate_esn\n",
    "\n",
    "# For comparison to a default network.\n",
    "params = { 'hidden_nodes': 80 }\n",
    "esn = ESN(**params)\n",
    "nrmse = evaluate_esn(ds.dataset, esn)\n",
    "plt.title('Reference NRMSE: ' + str(nrmse))\n",
    "plt.plot(esn.X)\n",
    "plt.show()\n",
    "\n",
    "# With different distance functions.\n",
    "dist_functions = [euc, inv, inv_squared]\n",
    "params = {\n",
    "    'w_res_type': 'waxman',\n",
    "    'hidden_nodes': 80,\n",
    "    'input_scaling': 1.0,\n",
    "}\n",
    "\n",
    "for f in dist_functions:\n",
    "    params['dist_function'] = f\n",
    "    esn = ESN(**params)\n",
    "    nrmse = evaluate_esn(ds.dataset, esn)\n",
    "\n",
    "    plt.title(f.__name__ + ' NRMSE: ' + str(nrmse))\n",
    "    plt.plot(esn.X)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Note: this creates a whole new network every time instead of re-using the  \n",
    "existing network but changing the weighting scheme, as to try to understand the  \n",
    "whole picture.  \n",
    "\n",
    "From running this a few times, I notice that the times when the network simply  \n",
    "does not work at all, the root cause is that a big majority of the network nodes  \n",
    "are either *all positive* or *all negative*, there is no balance.  \n",
    "\n",
    "There is a tendency here: the networks that seem to perform well also seem to  \n",
    "resemble the default first ESN network in terms of \"look\", but not so in the  \n",
    "magnitude of the activations -- which is lowered by the spectral radius. This  \n",
    "seems to indicate that the initial spectral radius of the waxman networks are  \n",
    "quite a lot higher than that of default initializations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Moving waxman performance towards Echo State Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Recall the results we got for the original Waxman graph for the 1/d distance  \n",
    "function:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 'waxman_dist_df' not in locals():\n",
    "    waxman_dist_df = load_experiment('experiments/waxman_dist.pkl')\n",
    "\n",
    "waxman_dist_df['dist_function'] = waxman_dist_df['dist_function'].apply(\n",
    "    lambda f: f.__name__ if not isinstance(f, str) else f\n",
    ")\n",
    "\n",
    "inv_df = waxman_dist_df.loc[waxman_dist_df['dist_function'] == inv.__name__]\n",
    "\n",
    "groupby = ['hidden_nodes', 'z_frac']\n",
    "axes    = ['hidden_nodes', 'z_frac', 'esn_nrmse']\n",
    "agg     = ['mean', 'min']\n",
    "labels  = ['euc', 'inv', 'inv^2']\n",
    "zlim    = (0.3, 1.0)\n",
    "azim    = 110\n",
    "\n",
    "plot_df_trisurf(df=inv_df, groupby=groupby, axes=axes, agg=agg, azim=azim, zlim=zlim, title='inv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We will attempt to make the performance of the Waxman graph equivalent to that  \n",
    "of the Echo State Network by:  \n",
    "\n",
    "* Introducing negative weights\n",
    "* Making the graph directed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Increasing fraction of negative weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from metric import esn_nrmse\n",
    "\n",
    "params = OrderedDict()\n",
    "params['w_res_type'] = ['waxman']\n",
    "params['dist_function'] = [inv]\n",
    "params['z_frac'] = [1.0]\n",
    "params['hidden_nodes'] = np.arange(20, 90, 10)\n",
    "params['sign_frac'] = np.arange(0.0, 0.55, 0.05)\n",
    "params['directed'] = [True, False]\n",
    "waxman_sign_df = experiment(esn_nrmse, params, runs=20)\n",
    "waxman_sign_df.to_pickle('experiments/waxman_sign.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plot import plot_df_trisurf\n",
    "\n",
    "if 'waxman_sign_df' not in locals():\n",
    "    waxman_sign_df = load_experiment('experiments/waxman_sign.pkl')\n",
    "\n",
    "undirected_df = waxman_sign_df.loc[waxman_sign_df['directed'] == False]\n",
    "directed_df = waxman_sign_df.loc[waxman_sign_df['directed'] == True]\n",
    "\n",
    "groupby = ['hidden_nodes', 'sign_frac']\n",
    "axes    = ['hidden_nodes', 'sign_frac', 'esn_nrmse']\n",
    "agg     = ['mean', 'min']\n",
    "title   = 'Increasing fraction of negative weights'\n",
    "zlim    = (0.3, 1.0)\n",
    "azim    = 70\n",
    "\n",
    "plot_df_trisurf(df=undirected_df, groupby=groupby, axes=axes,\n",
    "                agg=agg, azim=azim, zlim=zlim, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Directed Waxman graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title = 'Effect of directedness'\n",
    "\n",
    "plot_df_trisurf(df=directed_df, groupby=groupby, axes=axes,\n",
    "                agg=agg, azim=azim, zlim=zlim, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Comparison to Echo State Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params = OrderedDict()\n",
    "params['hidden_nodes'] = np.arange(20, 90, 10)\n",
    "waxman_esn_df = experiment(esn_nrmse, params, runs=20)\n",
    "waxman_esn_df.to_pickle('experiments/waxman_default_esn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 'waxman_esn_df' not in locals():\n",
    "    waxman_esn_df = load_experiment('experiments/waxman_default_esn.pkl')\n",
    "\n",
    "waxman_esn_df_max = waxman_esn_df.copy()\n",
    "waxman_esn_df_max['sign_frac'] = 0.5\n",
    "waxman_esn_df_min = waxman_esn_df.copy()\n",
    "waxman_esn_df_min['sign_frac'] = 0.0\n",
    "waxman_esn_df = pd.concat([waxman_esn_df_max, waxman_esn_df_min])\n",
    "\n",
    "labels  = ['+Negative weights', '+Directed', 'Echo State Network']\n",
    "show    = [False, False, True]\n",
    "azim    = 45\n",
    "elev    = 20\n",
    "\n",
    "ax = get_3d_subplot_axs(1)[0]\n",
    "ax.invert_yaxis()\n",
    "for i, df in enumerate([undirected_df, directed_df, waxman_esn_df]):\n",
    "    plot_df_trisurf(df=df, groupby=groupby, axes=axes, azim=azim, elev=elev,\n",
    "                    zlim=zlim, title='min', agg=['min'], ax=ax, label=labels[i],\n",
    "                    show=show[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Adding the possibility of having negative weights makes for a big difference,  \n",
    "especially when also combined with a directed network. In fact, we end up with  \n",
    "performance that very closely resembles that of ESNs at the end:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "undirected = undirected_df.loc[undirected_df['sign_frac'] == 0.5]\n",
    "directed = directed_df.loc[directed_df['sign_frac'] == 0.5]\n",
    "esn = waxman_esn_df.loc[waxman_esn_df['sign_frac'] == 0.5]\n",
    "\n",
    "labels = ['Undirected', 'Directed', 'Echo State Network']\n",
    "\n",
    "for i, df in enumerate([undirected, directed, esn]):\n",
    "    df = df.groupby(['hidden_nodes', 'sign_frac']).min().reset_index()\n",
    "    plt.plot(df['hidden_nodes'], df['esn_nrmse'], label=labels[i])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Dale et al. investigates this a bit in their paper: «It then becomes clear  \n",
    "that how weights are structured and directed, controlling information flow,  \n",
    "has a greater affect on quality of the network. This supports similar results  \n",
    "using hierarchical networks, where structure and number of parameters also  \n",
    "significantly impact performance [6].»  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Lastly, for the similarly performing networks in the previous graph, their  \n",
    "weight distributions look like (exemplified with the undirected graph, as their  \n",
    "distributions will be the same, except a directed graph would be symmetric):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plot import plot_vector_hist, plot_esn_weight_hist\n",
    "from ESN import ESN\n",
    "\n",
    "bins = 200\n",
    "hidden_nodes = 400\n",
    "\n",
    "params = OrderedDict()\n",
    "params['hidden_nodes'] = hidden_nodes\n",
    "plt.title('Echo State Network reservoir weight distribution')\n",
    "plot_esn_weight_hist(params, n_bins=bins, m=20.0, show=True)\n",
    "\n",
    "params = OrderedDict()\n",
    "params['w_res_type'] = 'waxman'\n",
    "params['dist_function'] = inv\n",
    "params['z_frac'] = 1.0\n",
    "params['hidden_nodes'] = hidden_nodes\n",
    "params['sign_frac'] = 0.5\n",
    "params['directed'] = False\n",
    "esn = ESN(**params)\n",
    "M = esn.w_res.data.numpy()\n",
    "\n",
    "# Masking to remove diagonal (i.e. 0-elements) of reservoir weights.\n",
    "M = M[~np.eye(M.shape[0], dtype=bool)].reshape(M.shape[0], -1)\n",
    "\n",
    "weights = M.flatten()\n",
    "plt.title('Waxman graph reservoir weight distribution (network)')\n",
    "plot_vector_hist(weights, n_bins=bins, m=4.0, show=True)\n",
    "\n",
    "weights = M[0].flatten()\n",
    "plt.title('Waxman graph reservoir weight distribution (single node)')\n",
    "plot_vector_hist(weights, n_bins=bins, m=4.0, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First thoughts: does this perhaps fit some known distribution (perhaps something  \n",
    "similar to a power law)? 1/d could probably be fit.  \n",
    "\n",
    "Now: does this distribution also scale as well with increasing hidden nodes as  \n",
    "the Echo State Network?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Waxman scaling with hidden nodes vs. ESN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from metric import esn_nrmse\n",
    "\n",
    "hidden_nodes = np.arange(20, 260, 10)\n",
    "\n",
    "params = OrderedDict()\n",
    "params['w_res_type'] = ['waxman']\n",
    "params['dist_function'] = [inv]\n",
    "params['z_frac'] = [1.0]\n",
    "params['hidden_nodes'] = hidden_nodes\n",
    "params['sign_frac'] = [0.5]\n",
    "params['directed'] = [True]\n",
    "waxman_dist_hn_df = experiment(esn_nrmse, params, runs=20)\n",
    "waxman_dist_hn_df.to_pickle('experiments/waxman_dist_hn.pkl')\n",
    "\n",
    "params = OrderedDict()\n",
    "params['hidden_nodes'] = hidden_nodes\n",
    "esn_hn_df = experiment(esn_nrmse, params, runs=20)\n",
    "esn_hn_df.to_pickle('experiments/esn_hn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 'waxman_dist_hn_df' not in locals():\n",
    "    waxman_dist_hn_df = load_experiment('experiments/waxman_dist_hn.pkl')\n",
    "\n",
    "if 'esn_hn_df' not in locals():\n",
    "    esn_hn_df = load_experiment('experiments/esn_hn.pkl')\n",
    "\n",
    "gdf = waxman_dist_hn_df.groupby(['hidden_nodes', 'w_res_type']).mean().reset_index()\n",
    "plt.plot(gdf['hidden_nodes'], gdf['esn_nrmse'], label='Modified Waxman graph')\n",
    "\n",
    "gdf = esn_hn_df.groupby(['hidden_nodes']).mean().reset_index()\n",
    "plt.plot(gdf['hidden_nodes'], gdf['esn_nrmse'], label='Echo State Network')\n",
    "\n",
    "plt.xlabel('Hidden nodes')\n",
    "plt.ylabel('NRMSE')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "So it seems that this symmetric weight distribution performs reasonably  \n",
    "equivalent to the standard ESN. Does it provide an additional robustness? \n",
    "\n",
    "It should not for dead nodes, as with a global connectivity, this is equivalent  \n",
    "to simply removing the node (i.e. decrementing the amount of hidden nodes).  \n",
    "\n",
    "What about sporadically firing nodes, or nodes that constantly produce noise?  \n",
    "\n",
    "I found little interesting in an introductory study of the difference in  \n",
    "robustness attributed to the difference in weight distributions. I might look  \n",
    "deeper into it later.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Did the activation footprint change with these additions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ESN import ESN\n",
    "from metric import evaluate_esn\n",
    "\n",
    "l = 50\n",
    "\n",
    "params = OrderedDict()\n",
    "params['w_res_type'] = 'waxman'\n",
    "params['dist_function'] = inv\n",
    "params['z_frac'] = 1.0\n",
    "params['hidden_nodes'] = 200\n",
    "\n",
    "params['sign_frac'] = 0.5\n",
    "params['directed'] = True\n",
    "\n",
    "exp = {\n",
    "    'sign_frac': [0.0, 0.5, 0.0, 0.5],\n",
    "    'directed':  [False, False, True, True],\n",
    "    'title':     ['default', '+neg', '+dir', '+neg+dir'],\n",
    "}\n",
    "\n",
    "for i in range(4):\n",
    "    params['sign_frac'] = exp['sign_frac'][i]\n",
    "    params['directed'] = exp['directed'][i]\n",
    "\n",
    "    esn = ESN(**params)\n",
    "    nrmse = evaluate_esn(ds.dataset, esn)\n",
    "\n",
    "    plt.title(f'{exp[\"title\"][i]} with NRMSE: {nrmse}')\n",
    "    plt.plot(esn.X[esn.washout:esn.washout+l])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "So there does seem to be a significant change. Just making the graph directed  \n",
    "helps little, as the activations are highly symmetrical until we also add  \n",
    "negative weights.  \n",
    "\n",
    "This is quite interesting, considering the fact that we, using lattices, later  \n",
    "will see that a lattice network of only positive weights perform well.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Lattice/tiling experiments (sq, rect, hex, tri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Plots of lattices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ESN import ESN\n",
    "from plot import plot_lattice\n",
    "\n",
    "set_figsize(14, 6)\n",
    "\n",
    "esn_square = ESN(hidden_nodes=25, w_res_type='tetragonal')\n",
    "esn_hex = ESN(hidden_nodes=25, w_res_type='hexagonal')\n",
    "esn_tri = ESN(hidden_nodes=25, w_res_type='triangular')\n",
    "esn_rect = ESN(hidden_nodes=25, w_res_type='rectangular', rect_ratio=2.0)\n",
    "\n",
    "G_square = esn_square.G\n",
    "G_hex = esn_hex.G\n",
    "G_tri = esn_tri.G\n",
    "G_rect = esn_rect.G\n",
    "\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "ax1, ax2, ax3, ax4 = axs[0, 0], axs[0, 1], axs[1, 0], axs[1, 1]\n",
    "plot_lattice(G_square, title='Square', ax=ax1, show=False)\n",
    "plot_lattice(G_hex, title='Hexagonal', ax=ax2, show=False)\n",
    "plot_lattice(G_tri, title='Triangular', ax=ax3, show=False)\n",
    "plot_lattice(G_rect, title='Rectangular', ax=ax4, show=True)\n",
    "\n",
    "set_figsize(default_w, default_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Growing neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ESN import ESN\n",
    "from plot import plot_lattice\n",
    "from matrix import grow_neighborhoods\n",
    "\n",
    "set_figsize(14, 6)\n",
    "\n",
    "esn_sq = ESN(hidden_nodes=25, w_res_type='tetragonal')\n",
    "G_sq = esn_sq.G\n",
    "titles = ['\"Exponential\" growth', 'Default growth']\n",
    "\n",
    "for n_plt in [0, 1]:\n",
    "    _G_sq = G_sq.copy()\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2)\n",
    "    ax1, ax2, ax3, ax4 = axs[0, 0], axs[0, 1], axs[1, 0], axs[1, 1]\n",
    "    axs = [ax1, ax2, ax3, ax4]\n",
    "\n",
    "    for i, ax in enumerate(axs):\n",
    "        plot_lattice(_G_sq, title='Square', ax=ax, neigh_color=True, show=False)\n",
    "\n",
    "        # The second time around we don't \"exponentially\" grow the neighborhood,\n",
    "        # but instead only use the original adjacency matrix to grow the\n",
    "        # neighborhood `l` times.\n",
    "        if n_plt == 0:\n",
    "            grow_neighborhoods(_G_sq, dist_function=inv)\n",
    "        else:\n",
    "            _G_sq = G_sq.copy()\n",
    "            grow_neighborhoods(_G_sq, dist_function=inv, l=i+1)\n",
    "\n",
    "    plt.suptitle(titles[n_plt])\n",
    "\n",
    "set_figsize(default_w, default_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from metric import esn_nrmse\n",
    "\n",
    "params = OrderedDict()\n",
    "params['hidden_nodes'] = [9, 16, 25, 36, 49, 64, 81]\n",
    "params['w_res_type'] = ['tetragonal', 'hexagonal', 'triangular']\n",
    "params['grow_neigh'] = list(range(1, 9))\n",
    "params['dist_function'] = [inv]\n",
    "lattice_grow_df = experiment(esn_nrmse, params)\n",
    "lattice_grow_df.to_pickle('experiments/lattice_grow_neigh.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plot import plot_df_trisurf\n",
    "\n",
    "if 'lattice_grow_df' not in locals():\n",
    "    lattice_grow_df = load_experiment('experiments/lattice_grow_neigh.pkl')\n",
    "\n",
    "groupby = ['hidden_nodes', 'grow_neigh']\n",
    "axes    = ['hidden_nodes', 'grow_neigh', 'esn_nrmse']\n",
    "agg     = ['mean', 'min']\n",
    "titles  = ['Square', 'Hexagonal', 'Triangular']\n",
    "zlim    = (0.5, 0.8)\n",
    "azim    = -60\n",
    "elev    = 20\n",
    "\n",
    "for i, w_res_type in enumerate(['tetragonal', 'hexagonal', 'triangular']):\n",
    "    df = lattice_grow_df.loc[lattice_grow_df['w_res_type'] == w_res_type]\n",
    "    plot_df_trisurf(df=df, groupby=groupby, axes=axes, agg=agg, azim=azim, elev=elev,\n",
    "                    zlim=zlim, title=titles[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There is a definite performance penalty with increasing sizes of node  \n",
    "neighborhood.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set_figsize(14, 6)\n",
    "\n",
    "params = {\n",
    "    'w_res_type': 'tetragonal',\n",
    "    'hidden_nodes': 14*14,\n",
    "    'dist_function': inv,\n",
    "    'grow_neigh': 0,\n",
    "    'readout': 'pinv',\n",
    "}\n",
    "\n",
    "esn = ESN(**params)\n",
    "nrmse = evaluate_esn(ds.dataset, esn, plot=True)\n",
    "print('NRMSE', nrmse )\n",
    "\n",
    "set_figsize(default_w, default_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Lattice NRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from metric import esn_nrmse\n",
    "\n",
    "params = OrderedDict()\n",
    "params['hidden_nodes'] = [9, 16, 25, 36, 49, 64, 81]\n",
    "params['w_res_type'] = ['tetragonal', 'hexagonal', 'triangular']\n",
    "nrmse_df = experiment(esn_nrmse, params)\n",
    "nrmse_df.to_pickle('experiments/lattice_nrmse.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 'nrmse_df' not in locals():\n",
    "    nrmse_df = load_experiment('experiments/lattice_nrmse.pkl')\n",
    "\n",
    "grouped_df = nrmse_df.groupby(['hidden_nodes', 'w_res_type']).mean().reset_index()\n",
    "\n",
    "tetragonal = grouped_df.loc[grouped_df['w_res_type'] == 'tetragonal']\n",
    "hexagonal = grouped_df.loc[grouped_df['w_res_type'] == 'hexagonal']\n",
    "triangular = grouped_df.loc[grouped_df['w_res_type'] == 'triangular']\n",
    "\n",
    "plt.plot(tetragonal['hidden_nodes'], tetragonal['esn_nrmse'], label='sq')\n",
    "plt.plot(hexagonal['hidden_nodes'], hexagonal['esn_nrmse'], label='hex')\n",
    "plt.plot(triangular['hidden_nodes'], triangular['esn_nrmse'], label='tri')\n",
    "\n",
    "plt.legend(fancybox=False, loc='upper right', bbox_to_anchor=(1.0, 1.0))\n",
    "plt.ylabel('NRMSE')\n",
    "plt.xlabel('Hidden nodes')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Lattice STM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Measuring the STM (MC) of lattices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from metric import esn_mc\n",
    "\n",
    "params = OrderedDict()\n",
    "params['hidden_nodes'] = [9, 16, 25, 36, 49, 64, 81]\n",
    "params['w_res_type'] = ['tetragonal', 'hexagonal', 'triangular']\n",
    "mc_df = experiment(esn_mc, params)\n",
    "mc_df.to_pickle('experiments/lattice_mc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 'mc_df' not in locals():\n",
    "    mc_df = load_experiment('experiments/lattice_mc.pkl')\n",
    "\n",
    "grouped_df = mc_df.groupby(['hidden_nodes', 'w_res_type']).mean().reset_index()\n",
    "\n",
    "tetragonal = grouped_df.loc[grouped_df['w_res_type'] == 'tetragonal']\n",
    "hexagonal = grouped_df.loc[grouped_df['w_res_type'] == 'hexagonal']\n",
    "triangular = grouped_df.loc[grouped_df['w_res_type'] == 'triangular']\n",
    "\n",
    "plt.plot(tetragonal['hidden_nodes'], tetragonal['esn_mc'], label='sq')\n",
    "plt.plot(hexagonal['hidden_nodes'], hexagonal['esn_mc'], label='hex')\n",
    "plt.plot(triangular['hidden_nodes'], triangular['esn_mc'], label='tri')\n",
    "\n",
    "plt.legend(fancybox=False, loc='upper right', bbox_to_anchor=(1.0, 1.0))\n",
    "plt.ylabel('MC')\n",
    "plt.xlabel('Hidden nodes')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Is there some pattern in the STM in lattices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First: how does the recall of the network look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from metric import memory_capacity\n",
    "from plot import plot_lattice\n",
    "\n",
    "params = {\n",
    "    'w_res_type': 'hexagonal',\n",
    "    'hidden_nodes': 49,\n",
    "    'input_scaling': 0.1,\n",
    "    'periodic': False,\n",
    "}\n",
    "\n",
    "esn = ESN(**params)\n",
    "mc = memory_capacity(esn)\n",
    "print('Memory capacity:', mc)\n",
    "\n",
    "plt.title(f'Hexagonal lattice memory capacity for {esn.hidden_nodes} hidden nodes')\n",
    "plt.xlabel('Signal delay')\n",
    "plt.ylabel('Recall ability')\n",
    "plt.plot(esn.mcs, marker='*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There was no pattern for the positions of high weighted hidden nodes for the  \n",
    "Waxman graphs, what about highly regular structures/networks?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(3, int(mc)):\n",
    "    weights = esn.w_outs[i].abs().data.numpy()\n",
    "    weights /= weights.max()\n",
    "    title = 'Hidden weights for STM length: ' + str(i)\n",
    "    plot_lattice(esn.G, title=title, cols=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "*An important note here: this works quite a bit worse if the input distribution  \n",
    "is fixed; the uniform distribution is crucial here.*  \n",
    "\n",
    "Quite interesting! A lot of the memory capacity seems to be due to the edges of  \n",
    "the lattice, seemingly because it is aperiodic. Making the graph periodic tanks  \n",
    "the memory capacity quite a little bit, which is investigated a little bit  \n",
    "further below.  \n",
    "\n",
    "Additionally, big weights in the output layer are often surrounded by other big  \n",
    "weights, which is an interesting indication of the flow of information in the  \n",
    "network.  \n",
    "\n",
    "Lastly, there is not necessarsily anything indicating that clusters of weights  \n",
    "used to recall n is close to weights used to recall n-1, which is slightly  \n",
    "surprising.  \n",
    "\n",
    "\n",
    "Second thoughts: the edges are not \"more important\", but simply have a smaller  \n",
    "in-degree, thus resulting in some of the edges having a smaller absolute value  \n",
    "for activations, requiring a bigger scaling by the w_out weight to achieve the  \n",
    "relevant order of magnitude. This is also probably what is seen below for  \n",
    "activations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What about the activations of the seemingly most important hidden nodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set_figsize(12, 5)\n",
    "\n",
    "length = 200\n",
    "n_max = 3\n",
    "n_min = 3\n",
    "\n",
    "for i in range(3, 6):\n",
    "    weights = esn.w_outs[i].abs().data.numpy()\n",
    "    min_elems = np.argpartition(weights, n_min)[:n_min]\n",
    "    max_elems = np.argpartition(weights, -n_max)[-n_max:]\n",
    "\n",
    "    min_activations = esn.X_train[:length, min_elems]\n",
    "    max_activations = esn.X_train[:length, max_elems]\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    plt.suptitle('Activations for nodes of biggest w_out weights for STM length: ' + str(i))\n",
    "    for ax in [ax1, ax2]:\n",
    "        ax.set_ylim(-0.08, 0.08)\n",
    "\n",
    "    ax1.plot(min_activations)\n",
    "    ax2.plot(max_activations)\n",
    "\n",
    "set_figsize(default_w, default_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Lattice input scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from metric import esn_mc\n",
    "\n",
    "params = OrderedDict()\n",
    "params['hidden_nodes'] = [25, 36, 49, 64, 81]\n",
    "params['input_scaling'] = np.insert(np.arange(0.05, 2.05, 0.05), 0, 0.01, axis=0)\n",
    "params['w_res_type'] = [None, 'tetragonal', 'hexagonal', 'triangular']\n",
    "waxman_is_df = experiment(esn_mc, params)\n",
    "is_df.to_pickle('experiments/lattice_input_scaling.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plot import plot_df_trisurf\n",
    "\n",
    "if 'is_df' not in locals():\n",
    "    is_df = load_experiment('experiments/lattice_input_scaling.pkl')\n",
    "\n",
    "reference_esn = is_df.loc[is_df['w_res_type'].isnull()]\n",
    "tetragonal = is_df.loc[is_df['w_res_type'] == 'tetragonal']\n",
    "hexagonal = is_df.loc[is_df['w_res_type'] == 'hexagonal']\n",
    "triangular = is_df.loc[is_df['w_res_type'] == 'triangular']\n",
    "\n",
    "groupby = ['hidden_nodes', 'input_scaling']\n",
    "axes    = ['input_scaling', 'hidden_nodes', 'esn_mc']\n",
    "agg     = ['mean', 'max']\n",
    "titles  = ['Reference ESN', 'Square', 'Hexagonal', 'Triangular']\n",
    "zlims   = [(5, 50)] + [(5, 20)]*3\n",
    "azim    = 70\n",
    "\n",
    "for i, df in enumerate([reference_esn, tetragonal, hexagonal, triangular]):\n",
    "    plot_df_trisurf(df=df, groupby=groupby, axes=axes, agg=agg, zlim=zlims[i], title=titles[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Using a rectangular lattice instead of square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from metric import esn_nrmse\n",
    "\n",
    "params = OrderedDict()\n",
    "params['hidden_nodes'] = [25, 36, 49, 64, 81]\n",
    "params['w_res_type'] = ['rectangular']\n",
    "params['rect_ratio'] = np.arange(0.1, 3.1, 0.1)\n",
    "rect_df = experiment(esn_nrmse, params)\n",
    "rect_df.to_pickle('experiments/lattice_rect.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plot import plot_df_trisurf\n",
    "\n",
    "if 'rect_df' not in locals():\n",
    "    rect_df = load_experiment('experiments/lattice_rect.pkl')\n",
    "\n",
    "groupby = ['hidden_nodes', 'rect_ratio']\n",
    "axes    = ['hidden_nodes', 'rect_ratio', 'esn_nrmse']\n",
    "agg     = ['mean', 'min']\n",
    "titles  = title = 'Effect of rectangular lattice'\n",
    "zlim    = (0.5, 0.62)\n",
    "azim    = 40\n",
    "\n",
    "plot_df_trisurf(df=rect_df, groupby=groupby, axes=axes, agg=agg, azim=azim, zlim=zlim, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Periodic lattice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from metric import esn_mc\n",
    "\n",
    "params = OrderedDict()\n",
    "params['hidden_nodes'] = [25, 36, 49, 64, 81]\n",
    "params['periodic'] = [True, False]\n",
    "params['w_res_type'] = ['tetragonal', 'hexagonal', 'triangular']\n",
    "periodic_df = experiment(esn_mc, params)\n",
    "periodic_df.to_pickle('experiments/periodic_lattice.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 'periodic_df' not in locals():\n",
    "    periodic_df = load_experiment('experiments/periodic_lattice.pkl')\n",
    "\n",
    "gdf = periodic_df.groupby(['hidden_nodes', 'periodic', 'w_res_type']).mean().reset_index()\n",
    "\n",
    "from collections import defaultdict\n",
    "dfs = defaultdict(lambda: [])\n",
    "\n",
    "plt.title('Periodic lattices')\n",
    "plt.ylabel('MC')\n",
    "plt.xlabel('Hidden nodes')\n",
    "colors = ['green', 'red', 'blue']\n",
    "\n",
    "for c, l in enumerate(['tetragonal' , 'hexagonal', 'triangular']):\n",
    "    dfs[l].append(gdf.loc[(gdf['w_res_type'] == l) & (gdf['periodic'] == False)])\n",
    "    dfs[l].append(gdf.loc[(gdf['w_res_type'] == l) & (gdf['periodic'] == True)])\n",
    "\n",
    "    for i, p in enumerate(['aperiodic', 'periodic']):\n",
    "        x = dfs[l][i]['hidden_nodes']\n",
    "        y = dfs[l][i]['esn_mc']\n",
    "        label = l + ' ' + p\n",
    "        linestyle = '-' if i == 0 else '--'\n",
    "        plt.plot(x, y, label=label, linestyle=linestyle, color=colors[c])\n",
    "\n",
    "plt.legend(fancybox=False, loc='lower right', bbox_to_anchor=(1.0, 0.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Lattice with a fraction of negative weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "esn = ESN(hidden_nodes=25, w_res_type='hexagonal', sign_frac=0.5)\n",
    "plot_lattice(esn.G, edge_color=True, directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from metric import esn_nrmse\n",
    "\n",
    "params = OrderedDict()\n",
    "params['hidden_nodes'] = [25, 36, 49, 64, 81]\n",
    "params['w_res_type'] = ['tetragonal', 'hexagonal', 'triangular']\n",
    "params['sign_frac'] = np.arange(0.0, 0.55, 0.05)\n",
    "lattice_sign_df = experiment(esn_nrmse, params)\n",
    "lattice_sign_df.to_pickle('experiments/lattice_sign.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 'lattice_sign_df' not in locals():\n",
    "    lattice_sign_df = load_experiment('experiments/lattice_sign.pkl')\n",
    "\n",
    "tetragonal = lattice_sign_df.loc[lattice_sign_df['w_res_type'] == 'tetragonal']\n",
    "hexagonal = lattice_sign_df.loc[lattice_sign_df['w_res_type'] == 'hexagonal']\n",
    "triangular = lattice_sign_df.loc[lattice_sign_df['w_res_type'] == 'triangular']\n",
    "\n",
    "groupby = ['hidden_nodes', 'sign_frac']\n",
    "axes    = ['hidden_nodes', 'sign_frac', 'esn_nrmse']\n",
    "agg     = ['mean', 'min']\n",
    "zlim    = (0.55, 0.7)\n",
    "azim    = -50\n",
    "titles  = ['tetragonal', 'hexagonal', 'triangular']\n",
    "\n",
    "for i, df in enumerate([tetragonal, hexagonal, triangular]):\n",
    "    plot_df_trisurf(df=df, groupby=groupby, axes=axes, azim=azim, agg=agg, zlim=zlim, title=titles[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "No difference is obtained by setting some edges to have negative weights.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Lattice with a fraction of directed edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "esn = ESN(hidden_nodes=25, w_res_type='hexagonal', dir_frac=0.5)\n",
    "plot_lattice(esn.G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from metric import esn_nrmse\n",
    "\n",
    "params = OrderedDict()\n",
    "params['w_res_type'] = ['tetragonal', 'hexagonal', 'triangular']\n",
    "params['hidden_nodes'] = [25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225]\n",
    "params['dir_frac'] = np.arange(0.0, 1.1, 0.1)\n",
    "lattice_dir_df = experiment(esn_nrmse, params, runs=50)\n",
    "lattice_dir_df.to_pickle('experiments/lattice_dir.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plot import plot_df_trisurf\n",
    "\n",
    "if 'lattice_dir_df' not in locals():\n",
    "    lattice_dir_df = load_experiment('experiments/lattice_dir.pkl')\n",
    "\n",
    "tetragonal = lattice_dir_df.loc[lattice_dir_df['w_res_type'] == 'tetragonal']\n",
    "hexagonal = lattice_dir_df.loc[lattice_dir_df['w_res_type'] == 'hexagonal']\n",
    "triangular = lattice_dir_df.loc[lattice_dir_df['w_res_type'] == 'triangular']\n",
    "\n",
    "groupby = ['hidden_nodes', 'dir_frac']\n",
    "axes    = ['hidden_nodes', 'dir_frac', 'esn_nrmse']\n",
    "agg     = ['mean', 'min']\n",
    "zlim    = (0.25, 0.7)\n",
    "azim    = 45\n",
    "titles  = ['tetragonal', 'hexagonal', 'triangular']\n",
    "\n",
    "for i, df in enumerate([tetragonal, hexagonal, triangular]):\n",
    "    plot_df_trisurf(df=df, groupby=groupby, axes=axes, azim=azim, agg=agg, zlim=zlim, title=titles[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We are below the strictly linear regime of NARMA10! That is interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Lattice negative weights+directed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from metric import esn_nrmse\n",
    "\n",
    "params = OrderedDict()\n",
    "params['w_res_type'] = ['tetragonal', 'hexagonal', 'triangular']\n",
    "params['hidden_nodes'] = [100]\n",
    "params['dir_frac'] = np.arange(0.0, 1.1, 0.1)\n",
    "params['sign_frac'] = np.arange(0.0, 0.55, 0.05)\n",
    "lattice_neg_dir_df = experiment(esn_nrmse, params, runs=20)\n",
    "lattice_neg_dir_df.to_pickle('experiments/lattice_neg_dir.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 'lattice_neg_dir_df' not in locals():\n",
    "    lattice_neg_dir_df = load_experiment('experiments/lattice_neg_dir.pkl')\n",
    "\n",
    "tetragonal = lattice_neg_dir_df.loc[lattice_neg_dir_df['w_res_type'] == 'tetragonal']\n",
    "hexagonal = lattice_neg_dir_df.loc[lattice_neg_dir_df['w_res_type'] == 'hexagonal']\n",
    "triangular = lattice_neg_dir_df.loc[lattice_neg_dir_df['w_res_type'] == 'triangular']\n",
    "\n",
    "groupby = ['sign_frac', 'dir_frac']\n",
    "axes    = ['sign_frac', 'dir_frac', 'esn_nrmse']\n",
    "agg     = ['mean', 'min']\n",
    "zlim    = (0.5, 0.65)\n",
    "azim    = 45\n",
    "titles  = ['tetragonal', 'hexagonal', 'triangular']\n",
    "\n",
    "for i, df in enumerate([tetragonal, hexagonal, triangular]):\n",
    "    plot_df_trisurf(df=df, groupby=groupby, axes=axes, azim=azim, agg=agg, zlim=zlim, title=titles[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Spectral radius with directed lattice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from metric import evaluate_esn\n",
    "from ESN import ESN\n",
    "\n",
    "params = OrderedDict()\n",
    "params['w_res_type'] = 'tetragonal'\n",
    "params['hidden_nodes'] = 144\n",
    "\n",
    "params['dir_frac'] = 0.0\n",
    "while True:\n",
    "    undir_esn = ESN(**params)\n",
    "    undir_nrmse = evaluate_esn(ds.dataset, undir_esn)\n",
    "    if undir_nrmse < 0.55:\n",
    "        break\n",
    "\n",
    "params['dir_frac'] = 1.0\n",
    "while True:\n",
    "    dir_esn = ESN(**params)\n",
    "    dir_nrmse = evaluate_esn(ds.dataset, dir_esn)\n",
    "    if dir_nrmse < 0.35:\n",
    "        break\n",
    "\n",
    "pickle.dump(dir_esn, open('models/dir_esn.pkl', 'wb'))\n",
    "pickle.dump(undir_esn, open('models/undir_esn.pkl', 'wb'))\n",
    "pickle.dump(ds.dataset, open('dataset/ds_narma.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir_esn = pickle.load(open('models/dir_esn.pkl', 'rb'))\n",
    "undir_esn = pickle.load(open('models/undir_esn.pkl', 'rb'))\n",
    "ds.dataset = pickle.load(open('dataset/ds_narma.pkl', 'rb'))\n",
    "\n",
    "dir_nrmse = evaluate_esn(ds.dataset, dir_esn)\n",
    "undir_nrmse = evaluate_esn(ds.dataset, undir_esn)\n",
    "\n",
    "print('NRMSE:')\n",
    "print(' undirected:', undir_nrmse)\n",
    "print(' directed:  ', dir_nrmse)\n",
    "\n",
    "print()\n",
    "print('Original spectral radius')\n",
    "print(' undirected:', undir_esn.org_spectral_radius)\n",
    "print(' directed:  ', dir_esn.org_spectral_radius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Huh! The directed square lattice with 144 hidden nodes is as good as the Echo  \n",
    "State Networks of the same size. This is surprising and promising. Perhaps it is  \n",
    "necessary to design physical reservoirs in simulation first, if one is to  \n",
    "consider the flow of information as such?  \n",
    "\n",
    "Do the activations look wildly different between directed and undirected graphs?  \n",
    "For example, is the directed graph perhaps able to reach the nonlinear part of  \n",
    "tanh?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = 100\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "plt.suptitle('Activations for an undirected vs. directed lattice')\n",
    "\n",
    "ax1.plot(undir_esn.X[undir_esn.washout:undir_esn.washout+l])\n",
    "ax1.set_title('Undirected')\n",
    "\n",
    "ax2.plot(dir_esn.X[dir_esn.washout:dir_esn.washout+l])\n",
    "ax2.set_title('Directed')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It is difficult to differentiate the two: this seems to be a common occurrence.  \n",
    "\n",
    "Is there perhaps a correlation between the spectral radius of the original  \n",
    "adjacency matrix (before scaling), and the NRMSE performance of the lattice?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from metric import evaluate_esn\n",
    "from ESN import ESN\n",
    "\n",
    "params = OrderedDict()\n",
    "params['w_res_type'] = 'tetragonal'\n",
    "params['hidden_nodes'] = 144\n",
    "params['dir_frac'] = 1.0\n",
    "\n",
    "nrmses = []\n",
    "srs = []\n",
    "for i in range(100):\n",
    "    esn = ESN(**params)\n",
    "    nrmse = evaluate_esn(ds.dataset, esn)\n",
    "    nrmses.append(nrmse)\n",
    "    srs.append(esn.org_spectral_radius)\n",
    "\n",
    "pickle.dump(nrmses, open('models/dir_nrmses.pkl', 'wb'))\n",
    "pickle.dump(srs, open('models/dir_srs.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nrmses = pickle.load(open('models/dir_nrmses.pkl', 'rb'))\n",
    "srs = pickle.load(open('models/dir_srs.pkl', 'rb'))\n",
    "\n",
    "plt.xlabel('Original spectral radius')\n",
    "plt.ylabel('Evaluated NRMSE')\n",
    "\n",
    "plt.scatter(srs, nrmses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "I see little/no correlation between the original spectral radius and the  \n",
    "evaluated NRMSE, so this is unlikely to be the culprit.  \n",
    "\n",
    "What about how the well-performing lattice _looks_?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 0,11 .. 11,11\n",
    "#  ..       ..\n",
    "# 0,0  .. 11,0\n",
    "\n",
    "# An important thing to notice here is that we must reverse the directions when\n",
    "# we plot the graph, as the way we use the produced matrix is the opposite of\n",
    "# the adjacency matrix we get from G.to_numpy_matrix().\n",
    "\n",
    "from plot import plot_lattice\n",
    "plot_lattice(dir_esn.G.reverse())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "With directedness: what is the distributions of in-degree/out-degree with  \n",
    "directed lattices? Can we color the lattices in some way to give insights?  \n",
    "\n",
    "An idea: what about average path length?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### wtf?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It turns out that once we make the lattice directed, it is possible to remove  \n",
    "the constraint of having a uniform distribution of input weights.  \n",
    "\n",
    "This 12x12 square lattice with a global input, and global reservoir weight  \n",
    "magnitude, performs just as well as the standard Echo State Network.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = OrderedDict()\n",
    "params['hidden_nodes'] = 144\n",
    "def_esn = ESN(**params)\n",
    "\n",
    "dir_esn = pickle.load(open('models/dir_esn.pkl', 'rb'))\n",
    "dir_esn.w_in = torch.ones(dir_esn.hidden_nodes)\n",
    "dir_esn.w_in *= 0.1\n",
    "\n",
    "def_nrmse = evaluate_esn(ds.dataset, def_esn)\n",
    "dir_nrmse = evaluate_esn(ds.dataset, dir_esn)\n",
    "\n",
    "print('NRMSE:')\n",
    "print(' default:', def_nrmse)\n",
    "print(' lattice:', dir_nrmse)\n",
    "\n",
    "print()\n",
    "print('Lattice weights:')\n",
    "print(f' w_in:  unique-{np.unique(dir_esn.w_in)}')\n",
    "print(f' w_res: unique-{np.unique(dir_esn.w_res)}')\n",
    "\n",
    "plot_lattice(dir_esn.G.reverse())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### The most important nodes in the lattice above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We try to find \"important\" nodes by removing one by one (may be a bit too  \n",
    "simple, as the performance of several nodes may be very closely linked).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "import copy\n",
    "\n",
    "dir_esn = pickle.load(open('models/dir_esn.pkl', 'rb'))\n",
    "ds.dataset = pickle.load(open('dataset/ds_narma.pkl', 'rb'))\n",
    "\n",
    "dir_esn.w_in = torch.ones(dir_esn.hidden_nodes)\n",
    "dir_esn.w_in *= 0.1\n",
    "\n",
    "def_nrmse = evaluate_esn(ds.dataset, dir_esn)\n",
    "nrmse_diffs = []\n",
    "\n",
    "for i in range(len(dir_esn.G.nodes)):\n",
    "    esn = copy.deepcopy(dir_esn)\n",
    "    esn.remove_hidden_node(i)\n",
    "    nrmse_diffs.append(evaluate_esn(ds.dataset, esn) - def_nrmse)\n",
    "\n",
    "pickle.dump(nrmse_diffs, open('experiments/dir_diff_nrmses.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plot import plot_vector_hist\n",
    "\n",
    "nrmse_diffs = pickle.load(open('experiments/dir_diff_nrmses.pkl', 'rb'))\n",
    "nrmse_diffs = np.array(nrmse_diffs)\n",
    "\n",
    "def_nrmse = evaluate_esn(ds.dataset, dir_esn)\n",
    "max_clip = 1 - def_nrmse\n",
    "np.clip(nrmse_diffs, -1, max_clip, out=nrmse_diffs)\n",
    "\n",
    "title = 'Importance when removing single node (black = low importance)'\n",
    "plot_lattice(dir_esn.G, cols=nrmse_diffs, cmap_r=True, title=title)\n",
    "\n",
    "set_figsize(10, 6)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "plt.title('Distribution of difference in NRSME when removing nodes')\n",
    "plot_vector_hist(nrmse_diffs, n_bins=50, m=None, ax=ax1, show=False)\n",
    "plt.title('Distribution of difference in NRSME when removing nodes (rejected outliers)')\n",
    "plot_vector_hist(nrmse_diffs, n_bins=50, m=10.0, ax=ax2, show=True)\n",
    "set_figsize(default_w, default_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "So it seems that individual nodes can be removed quite freely, with the  \n",
    "exception of one single node in this case. This is important when considering  \n",
    "the robustness of the reservoir to single node failure.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Performance when removing single nodes one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "import copy\n",
    "import tqdm\n",
    "\n",
    "dir_esn = pickle.load(open('models/dir_esn.pkl', 'rb'))\n",
    "ds.dataset = pickle.load(open('dataset/ds_narma.pkl', 'rb'))\n",
    "removed_nodes = pickle.load(open('experiments/removed_nodes.pkl', 'rb'))\n",
    "\n",
    "dir_esn.w_in = torch.ones(dir_esn.hidden_nodes)\n",
    "dir_esn.w_in *= 0.1\n",
    "\n",
    "for node in removed_nodes:\n",
    "    dir_esn.remove_hidden_node(node)\n",
    "\n",
    "while len(dir_esn.G.nodes) > 1:\n",
    "    def_nrmse = evaluate_esn(ds.dataset, dir_esn)\n",
    "    nrmses = []\n",
    "    nrmse_diffs = []\n",
    "\n",
    "    for i in tqdm.tqdm(range(len(dir_esn.G.nodes))):\n",
    "        esn = copy.deepcopy(dir_esn)\n",
    "        esn.remove_hidden_node(i)\n",
    "        nrmse = evaluate_esn(ds.dataset, esn)\n",
    "        nrmses.append(nrmse)\n",
    "        nrmse_diffs.append(nrmse - def_nrmse)\n",
    "\n",
    "    best_node = np.argmin(nrmse_diffs)\n",
    "    dir_esn.remove_hidden_node(best_node)\n",
    "    removed_nodes.append(best_node)\n",
    "\n",
    "    print()\n",
    "    print(f'it: removed-{best_node}, nrmse-{nrmse}')\n",
    "\n",
    "    pickle.dump(removed_nodes, open('experiments/removed_nodes.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's compare the performances as we remove nodes to that of ESNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "dir_esn = pickle.load(open('models/dir_esn.pkl', 'rb'))\n",
    "ds.dataset = pickle.load(open('dataset/ds_narma.pkl', 'rb'))\n",
    "removed_nodes = pickle.load(open('experiments/removed_nodes.pkl', 'rb'))\n",
    "\n",
    "dir_esn.w_in = torch.ones(dir_esn.hidden_nodes)\n",
    "dir_esn.w_in *= 0.1\n",
    "\n",
    "def_params = OrderedDict()\n",
    "\n",
    "lattice_nrmses = []\n",
    "lattices = []\n",
    "def_nrmses = []\n",
    "for node in tqdm.tqdm(removed_nodes):\n",
    "    dir_esn.remove_hidden_node(node)\n",
    "    lattice_nrmses.append(evaluate_esn(ds.dataset, dir_esn))\n",
    "    lattices.append(dir_esn.G.copy())\n",
    "\n",
    "    def_params['hidden_nodes'] = len(dir_esn.G.nodes)\n",
    "    def_esn = ESN(**def_params)\n",
    "    def_nrmses.append(evaluate_esn(ds.dataset, def_esn))\n",
    "\n",
    "pickle.dump(lattice_nrmses, open('experiments/deg_lattice_nrmses.pkl', 'wb'))\n",
    "pickle.dump(lattices, open('experiments/deg_lattices.pkl', 'wb'))\n",
    "pickle.dump(def_nrmses, open('experiments/deg_def_nrmses', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lattice_nrmses = pickle.load(open('experiments/deg_lattice_nrmses.pkl', 'rb'))\n",
    "lattices = pickle.load(open('experiments/deg_lattices.pkl', 'rb'))\n",
    "def_nrmses = pickle.load(open('experiments/deg_def_nrmses', 'rb'))\n",
    "\n",
    "max_nodes = len(lattices)\n",
    "print(f'Lattice min NRMSE: {max_nodes - np.argmin(lattice_nrmses)} nodes with NRMSE {min(lattice_nrmses)}')\n",
    "print(f'Default min NRMSE: {max_nodes - np.argmin(def_nrmses)} nodes with NRMSE {min(def_nrmses)}')\n",
    "\n",
    "x = list(range(max_nodes, 0, -1))\n",
    "\n",
    "plt.plot(x, lattice_nrmses, label='Lattice')\n",
    "plt.plot(x, def_nrmses, label='ESN')\n",
    "\n",
    "plt.gca().invert_xaxis()\n",
    "plt.ylim((0.0, 1.0))\n",
    "\n",
    "plt.xlabel('Hidden nodes')\n",
    "plt.ylabel('NRMSE')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What do the reservoirs look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir_esn = pickle.load(open('models/dir_esn.pkl', 'rb'))\n",
    "lattice_nrmses = pickle.load(open('experiments/deg_lattice_nrmses.pkl', 'rb'))\n",
    "lattices = pickle.load(open('experiments/deg_lattices.pkl', 'rb'))\n",
    "\n",
    "max_nodes = len(lattices)\n",
    "\n",
    "for i in [130, 70, 35, 20]:\n",
    "    title = f'Lattice, {i} nodes, NRMSE {lattice_nrmses[max_nodes-i]:.3f}'\n",
    "    plot_lattice(dir_esn.G, alpha=0.5, show=False, ax=plt.gca())\n",
    "    plot_lattice(lattices[max_nodes - i], ax=plt.gca(), title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Removing nodes in Echo State Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Does this (removing nodes) work with standard Echo State Networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from ESN import find_esn\n",
    "from experiment import remove_nodes_incrementally\n",
    "\n",
    "try:\n",
    "    cur_esn = pickle.load(open('models/esn_model_removed_nodes.pkl', 'rb'))\n",
    "except FileNotFoundError:\n",
    "    params = { 'hidden_nodes': 144, 'w_res_density': 0.10 }\n",
    "    cur_esn = find_esn(dataset=ds.dataset, required_nrmse=0.28, **params)\n",
    "    pickle.dump(cur_esn, open('models/esn_model_removed_nodes.pkl', 'wb'))\n",
    "\n",
    "remove_nodes_incrementally(ds.dataset, cur_esn, 'experiments/esn_removed_nodes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "from experiment import evaluate_incremental_node_removal\n",
    "\n",
    "esn_file = 'models/esn_model_removed_nodes.pkl'\n",
    "removed_nodes_file = 'experiments/esn_removed_nodes.pkl'\n",
    "\n",
    "nrmses, esns = evaluate_incremental_node_removal(ds.dataset, esn_file, removed_nodes_file, esns=True)\n",
    "pickle.dump(nrmses, open('experiments/esn_removed_nodes_nrmses.pkl', 'wb'))\n",
    "pickle.dump(esns, open('experiments/esn_removed_nodes_esns.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "esn_nrmses = pickle.load(open('experiments/esn_removed_nodes_nrmses.pkl', 'rb'))\n",
    "lattice_nrmses = pickle.load(open('experiments/deg_lattice_nrmses.pkl', 'rb'))\n",
    "\n",
    "print(f'Default min NRMSE: {max_nodes - np.argmin(esn_nrmses)} nodes with NRMSE {min(esn_nrmses)}')\n",
    "print(f'Lattice min NRMSE: {max_nodes - np.argmin(lattice_nrmses)} nodes with NRMSE {min(lattice_nrmses)}')\n",
    "\n",
    "x = list(range(len(esn_nrmses), 0, -1))\n",
    "plt.plot(x, esn_nrmses, label='ESN')\n",
    "plt.plot(x, lattice_nrmses, label='Lattice')\n",
    "\n",
    "plt.gca().invert_xaxis()\n",
    "plt.ylim((0.0, 1.0))\n",
    "\n",
    "plt.xlabel('Hidden nodes')\n",
    "plt.ylabel('NRMSE')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "esns = pickle.load(open('experiments/esn_removed_nodes_esns.pkl', 'rb'))\n",
    "esn_nrmses = pickle.load(open('experiments/esn_removed_nodes_nrmses.pkl', 'rb'))\n",
    "\n",
    "linear_esn = esns[len(esns)-31]\n",
    "A = linear_esn.w_res.numpy()\n",
    "G = nx.from_numpy_matrix(A)\n",
    "nx.draw(G, pos=nx.spring_layout(G), with_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The other way around: there is only a set amount of points to attach to the\n",
    "lattice; can we grow the lattice?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "name": "esn.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
